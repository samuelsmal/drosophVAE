{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-15T14:25:22.517267Z",
     "start_time": "2019-03-15T14:25:22.496947Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:95% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def fix_layout(width:int=95):\n",
    "    from IPython.core.display import display, HTML\n",
    "    display(HTML(\"<style>.container { width:\" + str(width) + \"% !important; }</style>\"))\n",
    "    \n",
    "fix_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-15T14:28:38.731754Z",
     "start_time": "2019-03-15T14:28:38.711080Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import uuid\n",
    "import shutil\n",
    "from glob import glob\n",
    "from datetime import date\n",
    "\n",
    "import skimage\n",
    "from functools import reduce\n",
    "from skimage import io\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from tqdm import tqdm, trange\n",
    "import sacred\n",
    "from sacred.stflow import LogFileWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-15T14:20:22.080032Z",
     "start_time": "2019-03-15T14:20:22.075596Z"
    }
   },
   "outputs": [],
   "source": [
    "from som_vae.somvae_model import SOMVAE\n",
    "from som_vae.utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-15T14:23:42.450348Z",
     "start_time": "2019-03-15T14:23:42.419558Z"
    }
   },
   "outputs": [],
   "source": [
    "ex = sacred.Experiment(\"fly_tryouts\", interactive=True)\n",
    "ex.observers.append(sacred.observers.FileStorageObserver.create(\"./sacred_runs\"))\n",
    "ex.captured_out_filter = sacred.utils.apply_backspaces_and_linefeeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-15T14:23:52.934155Z",
     "start_time": "2019-03-15T14:23:52.913372Z"
    },
    "code_folding": [
     1
    ]
   },
   "outputs": [],
   "source": [
    "@ex.config\n",
    "def ex_config():\n",
    "    \"\"\"Sacred configuration for the experiment.\n",
    "    \n",
    "    Params:\n",
    "        num_epochs (int): Number of training epochs.\n",
    "        patience (int): Patience for the early stopping.\n",
    "        batch_size (int): Batch size for the training.\n",
    "        latent_dim (int): Dimensionality of the SOM-VAE's latent space.\n",
    "        som_dim (list): Dimensionality of the self-organizing map.\n",
    "        learning_rate (float): Learning rate for the optimization.\n",
    "        alpha (float): Weight for the commitment loss.\n",
    "        beta (float): Weight for the SOM loss.\n",
    "        gamma (float): Weight for the transition probability loss.\n",
    "        tau (float): Weight for the smoothness loss.\n",
    "        decay_factor (float): Factor for the learning rate decay.\n",
    "        name (string): Name of the experiment.\n",
    "        ex_name (string): Unique name of this particular run.\n",
    "        logdir (path): Directory for the experiment logs.\n",
    "        modelpath (path): Path for the model checkpoints.\n",
    "        interactive (bool): Indicator if there should be an interactive progress bar for the training.\n",
    "        data_set (string): Data set for the training.\n",
    "        save_model (bool): Indicator if the model checkpoints should be kept after training and evaluation.\n",
    "        time_series (bool): Indicator if the model should be trained on linearly interpolated\n",
    "            MNIST time series.\n",
    "        mnist (bool): Indicator if the model is trained on MNIST-like data.\n",
    "    \"\"\"\n",
    "    num_epochs = 2\n",
    "    patience = 100\n",
    "    batch_size = 32\n",
    "    latent_dim = 64\n",
    "    som_dim = [8,8]\n",
    "    learning_rate = 0.0005\n",
    "    alpha = 1.0\n",
    "    beta = 0.9\n",
    "    gamma = 1.8\n",
    "    tau = 1.4\n",
    "    decay_factor = 0.9\n",
    "    name = ex.get_experiment_info()[\"name\"]\n",
    "    ex_name = \"{}_{}_{}-{}_{}_{}\".format(name, latent_dim, som_dim[0], som_dim[1], str(date.today()), uuid.uuid4().hex[:5])\n",
    "    logdir = \"../logs/{}\".format(ex_name)\n",
    "    modelpath = \"../models/{}/{}.ckpt\".format(ex_name, ex_name)\n",
    "    interactive = True\n",
    "    data_set = \"MNIST_data\"\n",
    "    save_model = False\n",
    "    time_series = True\n",
    "    mnist = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-15T14:28:11.968477Z",
     "start_time": "2019-03-15T14:28:11.642663Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'utils'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-0723af64e516>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;31m# <codecell>\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjupyter\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfix_layout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0mfix_layout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m80\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'utils'"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# <nbformat>4</nbformat>\n",
    "\n",
    "# <markdowncell>\n",
    "\n",
    "# # Imports\n",
    "\n",
    "# <codecell>\n",
    "\n",
    "\n",
    "# <codecell>\n",
    "\n",
    "import sys\n",
    "sys.path.append('/home/samuel/')\n",
    "\n",
    "from drosophpose.GUI import skeleton\n",
    "\n",
    "\n",
    "LEGS = [0, 1, 2, 5, 6, 7]\n",
    "LEGS = [0, 1, 2] #, 5, 6, 7] # since we do not care about the other side\n",
    "CAMERA_OF_INTEREST = 1\n",
    "NB_OF_AXIS = 2\n",
    "NB_TRACKED_POINTS = 5 # per leg, igoring the rest for now\n",
    "NB_CAMERAS = 7\n",
    "NB_RECORDED_DIMESIONS = 2\n",
    "\n",
    "FRAMES_PER_SECOND = 100\n",
    "NYQUIST_FREQUENCY_OF_MEASUREMENTS = FRAMES_PER_SECOND / 2\n",
    "\n",
    "# <codecell>\n",
    "\n",
    "POSE_DATA_PATH = \"/ramdya-nas/SVB/181220_Rpr_R57C10_GC6s_tdTom/001_coronal/behData/images_renamed/pose_result__mnt_NAS_SVB_181220_Rpr_R57C10_GC6s_tdTom_001_coronal_behData_images_renamed.pkl\"\n",
    "\n",
    "# <markdowncell>\n",
    "\n",
    "# # Loading data\n",
    "\n",
    "# <markdowncell>\n",
    "\n",
    "# ## loading \n",
    "\n",
    "# <codecell>\n",
    "\n",
    "def load_data(path=POSE_DATA_PATH):\n",
    "    with open(POSE_DATA_PATH, 'rb') as f:\n",
    "        pose_data_raw = pickle.load(f)\n",
    "        return pose_data_raw['points2d']\n",
    "\n",
    "# <markdowncell>\n",
    "\n",
    "# ## simple checks\n",
    "\n",
    "# <codecell>\n",
    "\n",
    "def _check_shape_(joint_positions):\n",
    "    \"\"\" should be (7, <nb frames>, 38, 2)\n",
    "    7 for the images, some should be 0 because it didn't record the images for these points\n",
    "    1000 for the nb of frames\n",
    "    38 for the features (some for the legs ...) check skeleton.py in semigh's code\n",
    "    2 for the pose dimensions\n",
    "    \"\"\"\n",
    "    s = joint_positions.shape\n",
    "    \n",
    "    if s[0] != NB_CAMERAS or s[2] != len(skeleton.tracked_points) or s[3] != NB_RECORDED_DIMESIONS:\n",
    "        raise ValueError(f\"shape of pose data is wrong, it's {joint_positions.shape}\")\n",
    "        \n",
    "    return joint_positions \n",
    "\n",
    "def _crude_value_check_(joint_positions):\n",
    "    if np.sum(joint_positions == 0) == np.product(joint_positions.shape):\n",
    "        raise ValueError('not every value should be zero')\n",
    "        \n",
    "    return joint_positions\n",
    "\n",
    "def _simple_checks_(data):\n",
    "    return reduce(lambda acc, el: el(acc), [_check_shape_, _crude_value_check_], data)\n",
    "\n",
    "# <markdowncell>\n",
    "\n",
    "# ## extracting data of interest\n",
    "\n",
    "# <codecell>\n",
    "\n",
    "def _get_camera_of_interest_(joint_positions, camera_idx=CAMERA_OF_INTEREST):\n",
    "    return joint_positions[CAMERA_OF_INTEREST]\n",
    "\n",
    "def _get_visible_legs_(joint_positions, camera_idx=CAMERA_OF_INTEREST):\n",
    "    idx_visible_joints = [skeleton.camera_see_joint(CAMERA_OF_INTEREST, j) for j in range(len(skeleton.tracked_points))]\n",
    "    return joint_positions[:, idx_visible_joints, :]\n",
    "\n",
    "# <codecell>\n",
    "\n",
    "def get_data(path=POSE_DATA_PATH):\n",
    "    return reduce(lambda acc, el: el(acc), [_get_camera_of_interest_, _get_visible_legs_], _simple_checks_(load_data(path)))\n",
    "\n",
    "# <markdowncell>\n",
    "\n",
    "# ## adding third dimension\n",
    "\n",
    "# <codecell>\n",
    "\n",
    "def add_third_dimension(joint_positions):\n",
    "    # just add a z-axis\n",
    "    # assumes that the positional data is in the last axis\n",
    "    paddings = [[0, 0] for i in joint_positions.shape]\n",
    "    paddings[-1][1] = 1\n",
    "\n",
    "    return np.pad(joint_positions, paddings, mode='constant', constant_values=0)\n",
    "\n",
    "# <markdowncell>\n",
    "\n",
    "# ## executing\n",
    "\n",
    "# <codecell>\n",
    "\n",
    "joint_positions = add_third_dimension(get_data())\n",
    "\n",
    "NB_FRAMES = joint_positions.shape[1]\n",
    "\n",
    "# <codecell>\n",
    "\n",
    "for leg in LEGS:\n",
    "    print(\"{0:.3}% of the data for leg {1} is 0\".format((joint_positions[:, leg:leg+5, :2] == 0).mean(), leg))\n",
    "\n",
    "# <markdowncell>\n",
    "\n",
    "# # Checking data\n",
    "\n",
    "# <codecell>\n",
    "\n",
    "def _get_feature_name_(tracking_id):\n",
    "    return str(skeleton.tracked_points[tracking_id])[len('Tracked.'):]\n",
    "\n",
    "def _get_feature_id_(leg_id, tracking_point_id):\n",
    "    if leg_id < 3:\n",
    "        return leg_id * 5 + tracking_point_id\n",
    "    else:\n",
    "        return (leg_id - 5) * 5 + tracking_point_id + 19\n",
    "\n",
    "# <codecell>\n",
    "\n",
    "def ploting_frames(joint_positions):\n",
    "    for leg in LEGS:\n",
    "        fig, axs = plt.subplots(1, NB_OF_AXIS, sharex=True, figsize=(20, 10))\n",
    "        for tracked_point in range(NB_TRACKED_POINTS):\n",
    "            for axis in range(NB_OF_AXIS):\n",
    "                cur_ax = axs[axis]\n",
    "                cur_ax.plot(joint_positions[:, _get_feature_id_(leg, tracked_point),  axis], label = f\"{_get_feature_name_(tracked_point)}_{('x' if axis == 0 else 'y')}\")\n",
    "                if axis == 0:\n",
    "                    cur_ax.set_ylabel('x pos')\n",
    "                else:\n",
    "                    cur_ax.set_ylabel('y pos')\n",
    "                cur_ax.legend(loc='upper right')\n",
    "                cur_ax.set_xlabel('frame')\n",
    "\n",
    "        #plt.xlabel('frame')\n",
    "        #plt.legend(loc='lower right')\n",
    "        plt.suptitle('leg ' + str(leg))\n",
    "\n",
    "# <codecell>\n",
    "\n",
    "ploting_frames(joint_positions)\n",
    "\n",
    "# <markdowncell>\n",
    "\n",
    "# ## Normalization\n",
    "\n",
    "# <codecell>\n",
    "\n",
    "def normalize(joint_positions, using_median=True, to_probability_distr=False):\n",
    "    # alternatives could be to use only the median of the first joint -> data is then fixed to top (is that differnt to now?)\n",
    "    if using_median:\n",
    "        return joint_positions - np.median(joint_positions.reshape(-1, 3), axis=0)\n",
    "    elif to_probability_distr:\n",
    "        return\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "\n",
    "# <codecell>\n",
    "\n",
    "def normalize_ts(time_series, ax=0):\n",
    "    # for shape (frame,feat)\n",
    "    eps = 0.0001\n",
    "    print(\"shapes:\", np.shape(np.transpose(time_series)), np.shape(np.mean(np.transpose(time_series), axis=ax)))\n",
    "#     n_time_series = (np.transpose(time_series) - np.mean(np.transpose(time_series), axis=ax))/(np.std(np.transpose(time_series), axis=ax) + eps)\n",
    "    norm = np.sum(np.transpose(time_series), axis=ax); norm = np.transpose(norm) #shape = 1,frames\n",
    "    n_time_series = np.transpose(time_series) / np.sum(np.transpose(time_series), axis=ax)\n",
    "    n_time_series = np.transpose(n_time_series)\n",
    "#     n_time_series = np.zeros(shape=np.shape(time_series))\n",
    "#     for i in range(np.shape(time_series)[1]):\n",
    "#         n_time_series[:,i] = (time_series[:,i] - np.mean(time_series[:,i])) / (np.std(time_series[:,i]) + eps)\n",
    "    return n_time_series, norm\n",
    "\n",
    "\n",
    "def normalize_pose(points3d, median3d=False):\n",
    "    # normalize experiment\n",
    "    if median3d:\n",
    "        points3d -= np.median(points3d.reshape(-1, 3), axis=0)\n",
    "    else:\n",
    "        for i in range(np.shape(points3d)[1]): #frames\n",
    "            for j in range(np.shape(points3d)[2]): #xyz\n",
    "                points3d[:,i,j] = normalize_ts(points3d[:,i,j]) \n",
    "    return points3d\n",
    "\n",
    "# <codecell>\n",
    "\n",
    "joint_positions = normalize(joint_positions)\n",
    "\n",
    "# <codecell>\n",
    "\n",
    "ploting_frames(joint_positions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fly_data = np."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-15T14:18:58.471054Z",
     "start_time": "2019-03-15T14:18:57.627866Z"
    }
   },
   "outputs": [],
   "source": [
    "mnist = input_data.read_data_sets(f\"../data/{ex_config()['data_set']}\")\n",
    "\n",
    "data_train = np.reshape(mnist.train.images, [-1,28,28,1])\n",
    "labels_train = mnist.train.labels\n",
    "data_val = data_train[45000:]\n",
    "labels_val = labels_train[45000:]\n",
    "data_train = data_train[:45000]\n",
    "labels_train = data_train[:45000]\n",
    "\n",
    "\n",
    "@ex.capture\n",
    "def get_data_generator(time_series):\n",
    "    \"\"\"Creates a data generator for the training.\n",
    "    \n",
    "    Args:\n",
    "        time_series (bool): Indicates whether or not we want interpolated MNIST time series or just\n",
    "            normal MNIST batches.\n",
    "    \n",
    "    Returns:\n",
    "        generator: Data generator for the batches.\"\"\"\n",
    "\n",
    "    def batch_generator(mode=\"train\", batch_size=100):\n",
    "        \"\"\"Generator for the data batches.\n",
    "        \n",
    "        Args:\n",
    "            mode (str): Mode in ['train', 'val'] that decides which data set the generator\n",
    "                samples from (default: 'train').\n",
    "            batch_size (int): The size of the batches (default: 100).\n",
    "            \n",
    "        Yields:\n",
    "            np.array: Data batch.\n",
    "        \"\"\"\n",
    "        assert mode in [\"train\", \"val\"], \"The mode should be in {train, val}.\"\n",
    "        if mode==\"train\":\n",
    "            images = data_train.copy()\n",
    "            labels = labels_train.copy()\n",
    "        elif mode==\"val\":\n",
    "            images = data_val.copy()\n",
    "            labels = labels_val.copy()\n",
    "        \n",
    "        while True:\n",
    "            indices = np.random.permutation(np.arange(len(images)))\n",
    "            images = images[indices]\n",
    "            labels = labels[indices]\n",
    "\n",
    "            if time_series:\n",
    "                for i, image in enumerate(images):\n",
    "                    start_image = image\n",
    "                    end_image = images[np.random.choice(np.where(labels == (labels[i] + 1) % 10)[0])]\n",
    "                    interpolation = interpolate_arrays(start_image, end_image, batch_size)\n",
    "                    yield interpolation + np.random.normal(scale=0.01, size=interpolation.shape)\n",
    "            else:\n",
    "                for i in range(len(images)//batch_size):\n",
    "                    yield images[i*batch_size:(i+1)*batch_size]\n",
    "\n",
    "    return batch_generator\n",
    "\n",
    "\n",
    "@ex.capture\n",
    "def train_model(model, x, lr_val, num_epochs, patience, batch_size, logdir,\n",
    "        modelpath, learning_rate, interactive, generator):\n",
    "    \"\"\"Trains the SOM-VAE model.\n",
    "    \n",
    "    Args:\n",
    "        model (SOM-VAE): SOM-VAE model to train.\n",
    "        x (tf.Tensor): Input tensor or placeholder.\n",
    "        lr_val (tf.Tensor): Placeholder for the learning rate value.\n",
    "        num_epochs (int): Number of epochs to train.\n",
    "        patience (int): Patience parameter for the early stopping.\n",
    "        batch_size (int): Batch size for the training generator.\n",
    "        logdir (path): Directory for saving the logs.\n",
    "        modelpath (path): Path for saving the model checkpoints.\n",
    "        learning_rate (float): Learning rate for the optimization.\n",
    "        interactive (bool): Indicator if we want to have an interactive\n",
    "            progress bar for training.\n",
    "        generator (generator): Generator for the data batches.\n",
    "    \"\"\"\n",
    "    train_gen = generator(\"train\", batch_size)\n",
    "    val_gen = generator(\"val\", batch_size)\n",
    "\n",
    "    num_batches = len(data_train)//batch_size\n",
    "\n",
    "    saver = tf.train.Saver(keep_checkpoint_every_n_hours=2.)\n",
    "    summaries = tf.summary.merge_all()\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        patience_count = 0\n",
    "        test_losses = []\n",
    "        with LogFileWriter(ex):\n",
    "            train_writer = tf.summary.FileWriter(logdir+\"/train\", sess.graph)\n",
    "            test_writer = tf.summary.FileWriter(logdir+\"/test\", sess.graph)\n",
    "        print(\"Training...\")\n",
    "        train_step_SOMVAE, train_step_prob = model.optimize\n",
    "        try:\n",
    "            if interactive:\n",
    "                pbar = tqdm(total=num_epochs*(num_batches)) \n",
    "            for epoch in range(num_epochs):\n",
    "                batch_val = next(val_gen)\n",
    "                test_loss, summary = sess.run([model.loss, summaries], feed_dict={x: batch_val})\n",
    "                test_losses.append(test_loss)\n",
    "                test_writer.add_summary(summary, tf.train.global_step(sess, model.global_step))\n",
    "                if test_losses[-1] == min(test_losses):\n",
    "                    saver.save(sess, modelpath, global_step=epoch)\n",
    "                    patience_count = 0\n",
    "                else:\n",
    "                    patience_count += 1\n",
    "                if patience_count >= patience:\n",
    "                    break\n",
    "                for i in range(num_batches):\n",
    "                    batch_data = next(train_gen)\n",
    "                    if i%100 == 0:\n",
    "                        train_loss, summary = sess.run([model.loss, summaries], feed_dict={x: batch_data})\n",
    "                        train_writer.add_summary(summary, tf.train.global_step(sess, model.global_step))\n",
    "                    train_step_SOMVAE.run(feed_dict={x: batch_data, lr_val:learning_rate})\n",
    "                    train_step_prob.run(feed_dict={x: batch_data, lr_val:learning_rate*100})\n",
    "                    if interactive:\n",
    "                        pbar.set_postfix(epoch=epoch, train_loss=train_loss, test_loss=test_loss, refresh=False)\n",
    "                        pbar.update(1)\n",
    "\n",
    "        except KeyboardInterrupt:\n",
    "            pass\n",
    "        finally:\n",
    "            saver.save(sess, modelpath)\n",
    "            if interactive:\n",
    "                pbar.close()\n",
    "\n",
    "\n",
    "\n",
    "@ex.capture\n",
    "def evaluate_model(model, x, modelpath, batch_size):\n",
    "    \"\"\"Evaluates the performance of the trained model in terms of normalized\n",
    "    mutual information, purity and mean squared error.\n",
    "    \n",
    "    Args:\n",
    "        model (SOM-VAE): Trained SOM-VAE model to evaluate.\n",
    "        x (tf.Tensor): Input tensor or placeholder.\n",
    "        modelpath (path): Path from which to restore the model.\n",
    "        batch_size (int): Batch size for the evaluation.\n",
    "        \n",
    "    Returns:\n",
    "        dict: Dictionary of evaluation results (NMI, Purity, MSE).\n",
    "    \"\"\"\n",
    "    saver = tf.train.Saver(keep_checkpoint_every_n_hours=2.)\n",
    "\n",
    "    num_batches = len(data_val)//batch_size\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        saver.restore(sess, modelpath)\n",
    "\n",
    "        test_k_all = []\n",
    "        test_rec_all = []\n",
    "        test_mse_all = []\n",
    "        print(\"Evaluation...\")\n",
    "        for i in range(num_batches):\n",
    "            batch_data = data_val[i*batch_size:(i+1)*batch_size]\n",
    "            test_k_all.extend(sess.run(model.k, feed_dict={x: batch_data}))\n",
    "            test_rec = sess.run(model.reconstruction_q, feed_dict={x: batch_data})\n",
    "            test_rec_all.extend(test_rec)\n",
    "            test_mse_all.append(mean_squared_error(test_rec.flatten(), batch_data.flatten()))\n",
    "\n",
    "        test_nmi = compute_NMI(test_k_all, labels_val[:len(test_k_all)])\n",
    "        test_purity = compute_purity(test_k_all, labels_val[:len(test_k_all)])\n",
    "        test_mse = np.mean(test_mse_all)\n",
    "\n",
    "    results = {}\n",
    "    results[\"NMI\"] = test_nmi\n",
    "    results[\"Purity\"] = test_purity\n",
    "    results[\"MSE\"] = test_mse\n",
    "#    results[\"optimization_target\"] = 1 - test_nmi\n",
    "\n",
    "    return results\n",
    " \n",
    "\n",
    "@ex.automain\n",
    "def main(latent_dim, som_dim, learning_rate, decay_factor, alpha, beta, gamma, tau, modelpath, save_model, mnist):\n",
    "    \"\"\"Main method to build a model, train it and evaluate it.\n",
    "    \n",
    "    Args:\n",
    "        latent_dim (int): Dimensionality of the SOM-VAE's latent space.\n",
    "        som_dim (list): Dimensionality of the SOM.\n",
    "        learning_rate (float): Learning rate for the training.\n",
    "        decay_factor (float): Factor for the learning rate decay.\n",
    "        alpha (float): Weight for the commitment loss.\n",
    "        beta (float): Weight for the SOM loss.\n",
    "        gamma (float): Weight for the transition probability loss.\n",
    "        tau (float): Weight for the smoothness loss.\n",
    "        modelpath (path): Path for the model checkpoints.\n",
    "        save_model (bool): Indicates if the model should be saved after training and evaluation.\n",
    "        \n",
    "    Returns:\n",
    "        dict: Results of the evaluation (NMI, Purity, MSE).\n",
    "    \"\"\"\n",
    "    # Dimensions for MNIST-like data\n",
    "    input_length = 28\n",
    "    input_channels = 28\n",
    "    x = tf.placeholder(tf.float32, shape=[None, 28, 28, 1])\n",
    "\n",
    "    data_generator = get_data_generator()\n",
    "\n",
    "    lr_val = tf.placeholder_with_default(learning_rate, [])\n",
    "\n",
    "    model = SOMVAE(inputs=x, latent_dim=latent_dim, som_dim=som_dim, learning_rate=lr_val, decay_factor=decay_factor,\n",
    "            input_length=input_length, input_channels=input_channels, alpha=alpha, beta=beta, gamma=gamma,\n",
    "            tau=tau, mnist=mnist)\n",
    "\n",
    "    train_model(model, x, lr_val, generator=data_generator)\n",
    "\n",
    "    result = evaluate_model(model, x)\n",
    "\n",
    "    if not save_model:\n",
    "        shutil.rmtree(os.path.dirname(modelpath))\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
