{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sources: \n",
    "\n",
    "- https://github.com/tensorflow/probability/blob/v0.6.0/tensorflow_probability/examples/vae.py\n",
    "\n",
    "\n",
    "The VAE defines a generative model in which a latent code `Z` is sampled from a\n",
    "prior `p(Z)`, then used to generate an observation `X` by way of a decoder\n",
    "`p(X|Z)`. The full reconstruction follows\n",
    "\n",
    "```none\n",
    "   X ~ p(X)              # A random image from some dataset.\n",
    "   Z ~ q(Z | X)          # A random encoding of the original image (\"encoder\").\n",
    "Xhat ~ p(Xhat | Z)       # A random reconstruction of the original image\n",
    "                         #   (\"decoder\").\n",
    "```\n",
    "\n",
    "To fit the VAE, we assume an approximate representation of the posterior in the\n",
    "form of an encoder `q(Z|X)`. We minimize the KL divergence between `q(Z|X)` and\n",
    "the true posterior `p(Z|X)`: this is equivalent to maximizing the evidence lower\n",
    "bound (ELBO),\n",
    "\n",
    "```none\n",
    "-log p(x)\n",
    "= -log int dz p(x|z) p(z)\n",
    "= -log int dz q(z|x) p(x|z) p(z) / q(z|x)\n",
    "<= int dz q(z|x) (-log[ p(x|z) p(z) / q(z|x) ])   # Jensen's Inequality\n",
    "=: KL[q(Z|x) || p(x|Z)p(Z)]\n",
    "= -E_{Z~q(Z|x)}[log p(x|Z)] + KL[q(Z|x) || p(Z)]\n",
    "```\n",
    "\n",
    "-or-\n",
    "\n",
    "```none\n",
    "-log p(x)\n",
    "= KL[q(Z|x) || p(x|Z)p(Z)] - KL[q(Z|x) || p(Z|x)]\n",
    "<= KL[q(Z|x) || p(x|Z)p(Z)                        # Positivity of KL\n",
    "= -E_{Z~q(Z|x)}[log p(x|Z)] + KL[q(Z|x) || p(Z)]\n",
    "```\n",
    "\n",
    "The `-E_{Z~q(Z|x)}[log p(x|Z)]` term is an expected reconstruction loss and\n",
    "`KL[q(Z|x) || p(Z)]` is a kind of distributional regularizer. See\n",
    "[Kingma and Welling (2014)][1] for more details.\n",
    "\n",
    "This script supports both a (learned) mixture of Gaussians prior as well as a\n",
    "fixed standard normal prior. You can enable the fixed standard normal prior by\n",
    "setting `mixture_components` to 1. Note that fixing the parameters of the prior\n",
    "(as opposed to fitting them with the rest of the model) incurs no loss in\n",
    "generality when using only a single Gaussian. The reasoning for this is\n",
    "two-fold:\n",
    "\n",
    "  * On the generative side, the parameters from the prior can simply be absorbed\n",
    "    into the first linear layer of the generative net. If `z ~ N(mu, Sigma)` and\n",
    "    the first layer of the generative net is given by `x = Wz + b`, this can be\n",
    "    rewritten,\n",
    "\n",
    "      s ~ N(0, I)\n",
    "      x = Wz + b\n",
    "        = W (As + mu) + b\n",
    "        = (WA) s + (W mu + b)\n",
    "\n",
    "    where Sigma has been decomposed into A A^T = Sigma. In other words, the log\n",
    "    likelihood of the model (E_{Z~q(Z|x)}[log p(x|Z)]) is independent of whether\n",
    "    or not we learn mu and Sigma.\n",
    "\n",
    "  * On the inference side, we can adjust any posterior approximation\n",
    "    q(z | x) ~ N(mu[q], Sigma[q]), with\n",
    "\n",
    "    new_mu[p] := 0\n",
    "    new_Sigma[p] := eye(d)\n",
    "    new_mu[q] := inv(chol(Sigma[p])) @ (mu[p] - mu[q])\n",
    "    new_Sigma[q] := inv(Sigma[q]) @ Sigma[p]\n",
    "\n",
    "    A bit of algebra on the KL divergence term `KL[q(Z|x) || p(Z)]` reveals that\n",
    "    it is also invariant to the prior parameters as long as Sigma[p] and\n",
    "    Sigma[q] are invertible.\n",
    "\n",
    "This script also supports using the analytic KL (KL[q(Z|x) || p(Z)]) with the\n",
    "`analytic_kl` flag. Using the analytic KL is only supported when\n",
    "`mixture_components` is set to 1 since otherwise no analytic form is known.\n",
    "\n",
    "Here we also compute tighter bounds, the IWAE [Burda et. al. (2015)][2].\n",
    "\n",
    "These as well as image summaries can be seen in Tensorboard. For help using\n",
    "Tensorboard see\n",
    "https://www.tensorflow.org/guide/summaries_and_tensorboard\n",
    "which can be run with\n",
    "  `python -m tensorboard.main --logdir=MODEL_DIR`\n",
    "\n",
    "#### References\n",
    "\n",
    "[1]: Diederik Kingma and Max Welling. Auto-Encoding Variational Bayes. In\n",
    "     _International Conference on Learning Representations_, 2014.\n",
    "     https://arxiv.org/abs/1312.6114\n",
    "[2]: Yuri Burda, Roger Grosse, Ruslan Salakhutdinov. Importance Weighted\n",
    "     Autoencoders. In _International Conference on Learning Representations_,\n",
    "     2015.\n",
    "     https://arxiv.org/abs/1509.00519"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-02T20:53:48.212261Z",
     "start_time": "2019-05-02T20:53:48.208309Z"
    }
   },
   "outputs": [],
   "source": [
    "from datetime import date\n",
    "from datetime import timedelta\n",
    "\n",
    "#_NIGHTLY_VERSION_ = 20190312 # 20190430 # 20190502 # 20190312\n",
    "#_NIGHTLY_VERSION_ = str((date(2019, 3, 19) - timedelta(days=diff))).replace('-', '')\n",
    "#_NIGHTLY_VERSION_ = str((date.today() - timedelta(days=diff))).replace('-', '')\n",
    "#!pip -q install --upgrade tf-nightly==1.14.1-dev{_NIGHTLY_VERSION_} \\\n",
    "#                          tf-nightly-gpu==1.14.1-dev{_NIGHTLY_VERSION_} \\\n",
    "#                          tfp-nightly==0.7.0.dev20190312\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-03T08:47:19.985077Z",
     "start_time": "2019-05-03T08:47:18.716085Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_probability as tfp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-03T13:12:02.614008Z",
     "start_time": "2019-05-03T13:12:02.549193Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import tensorflow as tf\n",
      "\n",
      "\n",
      "_TF_DEFAULT_SESSION_CONFIG_ = tf.ConfigProto()\n",
      "_TF_DEFAULT_SESSION_CONFIG_.gpu_options.allow_growth = True \n",
      "_TF_DEFAULT_SESSION_CONFIG_.gpu_options.polling_inactive_delay_msecs = 10\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import TensorFlow >= 1.9 and enable eager execution\n",
    "import tensorflow as tf\n",
    "tfe = tf.contrib.eager\n",
    "tf.enable_eager_execution()\n",
    "\n",
    "\n",
    "#from tensorflow.python import tf2\n",
    "#if not tf2.enabled():\n",
    "#    import tensorflow.compat.v2 as tf\n",
    "#    #import tensorflow.compat.v1 as tf\n",
    "#    tf.enable_v2_behavior()\n",
    "#    assert tf2.enabled()\n",
    "\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "tfk = tf.keras\n",
    "tfkl = tf.keras.layers\n",
    "tfpl = tfp.layers\n",
    "tfd = tfp.distributions\n",
    "\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import PIL\n",
    "#import imageio\n",
    "from IPython import display\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "\n",
    "import inspect\n",
    "\n",
    "from drosoph_vae.helpers import tensorflow as _donotim\n",
    "print(inspect.getsource(_donotim))\n",
    "\n",
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "#device_lib.list_local_devices()\n",
    "\n",
    "#import tensorflow as tf\n",
    "\n",
    "_TF_DEFAULT_SESSION_CONFIG_ = tf.ConfigProto(device_count={'GPU': 1})\n",
    "_TF_DEFAULT_SESSION_CONFIG_.gpu_options.allow_growth = True \n",
    "_TF_DEFAULT_SESSION_CONFIG_.gpu_options.polling_inactive_delay_msecs = 10\n",
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, accuracy_score, roc_auc_score, roc_curve\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.manifold import TSNE\n",
    "import os\n",
    "\n",
    "%matplotlib inline\n",
    "from drosoph_vae.helpers.misc import extract_args, chunks, foldl\n",
    "from drosoph_vae.helpers.jupyter import fix_layout, display_video\n",
    "from drosoph_vae.settings import config, skeleton\n",
    "from drosoph_vae.helpers import video, plots, misc, jupyter\n",
    "from drosoph_vae import preprocessing\n",
    "from drosoph_vae.helpers.logging import enable_logging\n",
    "#from drosoph_vae.helpers.tensorflow import _TF_DEFAULT_SESSION_CONFIG_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-03T08:47:42.848223Z",
     "start_time": "2019-05-03T08:47:42.840949Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:95% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "jupyter.fix_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-03T08:47:44.976033Z",
     "start_time": "2019-05-03T08:47:44.964083Z"
    }
   },
   "outputs": [],
   "source": [
    "### Utility Functions\n",
    "## Plots\n",
    "# Plot Feature Projection [credit: https://www.kaggle.com/shivamb/semi-supervised-classification-using-autoencoders]\n",
    "def tsne_plot(x1, y1, name=None):\n",
    "    tsne = TSNE(n_components=2, random_state=0)\n",
    "    X_t = tsne.fit_transform(x1)\n",
    "#     plt.figure(figsize=(12, 8))\n",
    "    plt.scatter(X_t[np.where(y1 == 0), 0], X_t[np.where(y1 == 0), 1], marker='o', color='g', linewidth='1', alpha=0.8, label='Non Fraud', s=2)\n",
    "\n",
    "    plt.legend(loc='best');\n",
    "    #plt.savefig(name);\n",
    "    plt.title('tsne')\n",
    "    plt.show();\n",
    "    \n",
    "    \n",
    "# Plot Keras training history\n",
    "def plot_loss(hist):\n",
    "    plt.plot(hist.history['loss'])\n",
    "    plt.plot(hist.history['val_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-03T08:47:52.042097Z",
     "start_time": "2019-05-03T08:47:46.919717Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:this works only for the first legs!\n",
      "WARNING:root:this works only for the first legs!\n",
      "WARNING:root:this works only for the first legs!\n",
      "WARNING:root:this works only for the first legs!\n",
      "WARNING:root:this works only for the first legs!\n"
     ]
    }
   ],
   "source": [
    "from drosoph_vae import settings\n",
    "from drosoph_vae import preprocessing\n",
    "\n",
    "joint_positions, normalisation_factors = preprocessing.get_data_and_normalization(settings.data.EXPERIMENTS)\n",
    "\n",
    "frames_idx_with_labels = preprocessing.get_frames_with_idx_and_labels(settings.data.LABELLED_DATA)[:len(joint_positions)]\n",
    "\n",
    "#frames_of_interest = frames_idx_with_labels.label.isin([settings.data._BehaviorLabel_.GROOM_ANT, settings.data._BehaviorLabel_.WALK_FORW, settings.data._BehaviorLabel_.REST])\n",
    "frames_of_interest = ~frames_idx_with_labels.label.isin([settings.data._BehaviorLabel_.REST])\n",
    "\n",
    "joint_positions = joint_positions[frames_of_interest]\n",
    "frames_idx_with_labels = frames_idx_with_labels[frames_of_interest]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-03T08:47:59.465381Z",
     "start_time": "2019-05-03T08:47:59.453192Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of input data:(1538, 30)\n"
     ]
    }
   ],
   "source": [
    "# flatten the data\n",
    "reshaped_joint_position = joint_positions[:,:,: config.NB_DIMS].reshape(joint_positions.shape[0], -1).astype(np.float32)\n",
    "\n",
    "\n",
    "# scaling the data to be in [0, 1]\n",
    "# this is due to the sigmoid activation function in the reconstruction\n",
    "scaler = MinMaxScaler()\n",
    "#resh = scaler.fit_transform(resh)\n",
    "\n",
    "print(f\"total number of input data:{reshaped_joint_position.shape}\")\n",
    "\n",
    "\n",
    "#if drosoph_vae_config['time_series']:\n",
    "#    _time_series_idx_ = list(to_time_series(range(len(joint_positions))))\n",
    "#    _jp = np.concatenate([joint_positions[idx].reshape(1, -1, 30) for idx in _time_series_idx_], axis=0)\n",
    "#else:\n",
    "#    _jp = joint_positions\n",
    "#    \n",
    "#nb_of_data_points = (reshaped_joint_position.shape[0] // config['batch_size']) * config['batch_size']\n",
    "# train - test split\n",
    "nb_of_data_points = int(reshaped_joint_position.shape[0] * 0.7)\n",
    "#\n",
    "X_train = scaler.fit_transform(reshaped_joint_position[:nb_of_data_points])\n",
    "X_test = scaler.transform(reshaped_joint_position[nb_of_data_points:])\n",
    "# just generating some labels, no clue what they are for except validation?\n",
    "#labels = frames_idx_with_labels['label'].apply(lambda x: x.value).values\n",
    "\n",
    "#if drosoph_vae_config['time_series']:\n",
    "#    labels = np.concatenate([labels[idx].reshape(1, -1, 1) for idx in _time_series_idx_], axis=0)\n",
    "\n",
    "#data = {\n",
    "#  \"X_train\": data_train,\n",
    "#  \"X_val\": data_test,\n",
    "#  \"y_train\": labels[:nb_of_data_points],\n",
    "#  \"y_val\": labels[nb_of_data_points:]\n",
    "#}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#flags.DEFINE_float(\n",
    "#    \"learning_rate\", default=0.001, help=\"Initial learning rate.\")\n",
    "#flags.DEFINE_integer(\n",
    "#    \"max_steps\", default=5001, help=\"Number of training steps to run.\")\n",
    "#flags.DEFINE_integer(\n",
    "#    \"latent_size\",\n",
    "#    default=16,\n",
    "#    help=\"Number of dimensions in the latent code (z).\")\n",
    "#flags.DEFINE_integer(\"base_depth\", default=32, help=\"Base depth for layers.\")\n",
    "#flags.DEFINE_string(\n",
    "#    \"activation\",\n",
    "#    default=\"leaky_relu\",\n",
    "#    help=\"Activation function for all hidden layers.\")\n",
    "#flags.DEFINE_integer(\n",
    "#    \"batch_size\",\n",
    "#    default=32,\n",
    "#    help=\"Batch size.\")\n",
    "#flags.DEFINE_integer(\n",
    "#    \"n_samples\", default=16, help=\"Number of samples to use in encoding.\")\n",
    "#flags.DEFINE_integer(\n",
    "#    \"mixture_components\",\n",
    "#    default=100,\n",
    "#    help=\"Number of mixture components to use in the prior. Each component is \"\n",
    "#         \"a diagonal normal distribution. The parameters of the components are \"\n",
    "#         \"intialized randomly, and then learned along with the rest of the \"\n",
    "#         \"parameters. If `analytic_kl` is True, `mixture_components` must be \"\n",
    "#         \"set to `1`.\")\n",
    "#flags.DEFINE_bool(\n",
    "#    \"analytic_kl\",\n",
    "#    default=False,\n",
    "#    help=\"Whether or not to use the analytic version of the KL. When set to \"\n",
    "#         \"False the E_{Z~q(Z|X)}[log p(Z)p(X|Z) - log q(Z|X)] form of the ELBO \"\n",
    "#         \"will be used. Otherwise the -KL(q(Z|X) || p(Z)) + \"\n",
    "#         \"E_{Z~q(Z|X)}[log p(X|Z)] form will be used. If analytic_kl is True, \"\n",
    "#         \"then you must also specify `mixture_components=1`.\")\n",
    "#flags.DEFINE_string(\n",
    "#    \"data_dir\",\n",
    "#    default=os.path.join(os.getenv(\"TEST_TMPDIR\", \"/tmp\"), \"vae/data\"),\n",
    "#    help=\"Directory where data is stored (if using real data).\")\n",
    "#flags.DEFINE_string(\n",
    "#    \"model_dir\",\n",
    "#    default=os.path.join(os.getenv(\"TEST_TMPDIR\", \"/tmp\"), \"vae/\"),\n",
    "#    help=\"Directory to put the model's fit.\")\n",
    "#flags.DEFINE_integer(\n",
    "#    \"viz_steps\", default=500, help=\"Frequency at which to save visualizations.\")\n",
    "#flags.DEFINE_bool(\n",
    "#    \"fake_data\",\n",
    "#    default=False,\n",
    "#    help=\"If true, uses fake data instead of MNIST.\")\n",
    "#flags.DEFINE_bool(\n",
    "#    \"delete_existing\",\n",
    "#    default=False,\n",
    "#    help=\"If true, deletes existing `model_dir` directory.\")\n",
    "#\n",
    "#FLAGS = flags.FLAGS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-03T13:18:11.266452Z",
     "start_time": "2019-05-03T13:18:11.260903Z"
    }
   },
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"max_steps\": 5001,\n",
    "    \"latent_size\":  2,\n",
    "    \"base_depth\": 32,\n",
    "    \"activation\": \"leaky_relu\",\n",
    "    \"batch_size\": 32,\n",
    "    \"n_samples\": 16,\n",
    "    \"mixture_components\": 1,\n",
    "    \"analytic_kl\": False,\n",
    "    \"model_dir\": '../neural_clustering_data/models/',\n",
    "    \"viz_steps\": 500\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-03T13:18:22.497276Z",
     "start_time": "2019-05-03T13:18:22.473121Z"
    }
   },
   "outputs": [],
   "source": [
    "def _softplus_inverse(x):\n",
    "    \"\"\"Helper which computes the function inverse of `tf.nn.softplus`.\"\"\"\n",
    "    return tf.log(tf.math.expm1(x))\n",
    "\n",
    "\n",
    "def make_encoder(activation, latent_size, base_depth):\n",
    "    \"\"\"Creates the encoder function.\n",
    "  \n",
    "    Args:\n",
    "      activation: Activation function in hidden layers.\n",
    "      latent_size: The dimensionality of the encoding.\n",
    "      base_depth: The lowest depth for a layer.\n",
    "  \n",
    "    Returns:\n",
    "      encoder: A `callable` mapping a `Tensor` of images to a\n",
    "        `tfd.Distribution` instance over encodings.\n",
    "    \"\"\"\n",
    "    conv = functools.partial(\n",
    "        tf.keras.layers.Conv2D, padding=\"SAME\", activation=activation)\n",
    "  \n",
    "    encoder_net = tf.keras.Sequential([\n",
    "        conv(base_depth, 5, 1),\n",
    "        conv(base_depth, 5, 2),\n",
    "        conv(2 * base_depth, 5, 1),\n",
    "        conv(2 * base_depth, 5, 2),\n",
    "        conv(4 * latent_size, 7, padding=\"VALID\"),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(2 * latent_size, activation=None),\n",
    "    ])\n",
    "  \n",
    "    def encoder(images):\n",
    "        images = 2 * tf.cast(images, dtype=tf.float32) - 1\n",
    "        net = encoder_net(images)\n",
    "        return tfd.MultivariateNormalDiag(\n",
    "            loc=net[..., :latent_size],\n",
    "            scale_diag=tf.nn.softplus(net[..., latent_size:] +\n",
    "                                      _softplus_inverse(1.0)),\n",
    "            name=\"code\")\n",
    "  \n",
    "    return encoder\n",
    "\n",
    "\n",
    "def make_decoder(activation, latent_size, output_shape, base_depth):\n",
    "    \"\"\"Creates the decoder function.\n",
    "\n",
    "    Args:\n",
    "      activation: Activation function in hidden layers.\n",
    "      latent_size: Dimensionality of the encoding.\n",
    "      output_shape: The output image shape.\n",
    "      base_depth: Smallest depth for a layer.\n",
    "\n",
    "    Returns:\n",
    "      decoder: A `callable` mapping a `Tensor` of encodings to a\n",
    "        `tfd.Distribution` instance over images.\n",
    "    \"\"\"\n",
    "    deconv = functools.partial(\n",
    "        tf.keras.layers.Conv2DTranspose, padding=\"SAME\", activation=activation)\n",
    "    conv = functools.partial(\n",
    "        tf.keras.layers.Conv2D, padding=\"SAME\", activation=activation)\n",
    "\n",
    "    decoder_net = tf.keras.Sequential([\n",
    "        deconv(2 * base_depth, 7, padding=\"VALID\"),\n",
    "        deconv(2 * base_depth, 5),\n",
    "        deconv(2 * base_depth, 5, 2),\n",
    "        deconv(base_depth, 5),\n",
    "        deconv(base_depth, 5, 2),\n",
    "        deconv(base_depth, 5),\n",
    "        conv(output_shape[-1], 5, activation=None),\n",
    "    ])\n",
    "\n",
    "    def decoder(codes):\n",
    "        original_shape = tf.shape(codes)\n",
    "        # Collapse the sample and batch dimension and convert to rank-4 tensor for\n",
    "        # use with a convolutional decoder network.\n",
    "        codes = tf.reshape(codes, (-1, 1, 1, latent_size))\n",
    "        logits = decoder_net(codes)\n",
    "        logits = tf.reshape(logits, shape=tf.concat([original_shape[:-1], output_shape], axis=0))\n",
    "        return tfd.Independent(tfd.Bernoulli(logits=logits),\n",
    "                               reinterpreted_batch_ndims=len(output_shape),\n",
    "                               name=\"image\")\n",
    "\n",
    "    return decoder\n",
    "\n",
    "\n",
    "def make_mixture_prior(latent_size, mixture_components):\n",
    "    \"\"\"Creates the mixture of Gaussians prior distribution.\n",
    "  \n",
    "    Args:\n",
    "      latent_size: The dimensionality of the latent representation.\n",
    "      mixture_components: Number of elements of the mixture.\n",
    "  \n",
    "    Returns:\n",
    "      random_prior: A `tfd.Distribution` instance representing the distribution\n",
    "        over encodings in the absence of any evidence.\n",
    "    \"\"\"\n",
    "    if mixture_components == 1:\n",
    "       # See the module docstring for why we don't learn the parameters here.\n",
    "       return tfd.MultivariateNormalDiag(\n",
    "           loc=tf.zeros([latent_size]),\n",
    "           scale_identity_multiplier=1.0)\n",
    "  \n",
    "    loc = tf.get_variable(name=\"loc\", shape=[mixture_components, latent_size])\n",
    "    raw_scale_diag = tf.get_variable(name=\"raw_scale_diag\", shape=[mixture_components, latent_size])\n",
    "    mixture_logits = tf.get_variable(name=\"mixture_logits\", shape=[mixture_components])\n",
    "  \n",
    "    return tfd.MixtureSameFamily(\n",
    "        components_distribution=tfd.MultivariateNormalDiag(\n",
    "            loc=loc,\n",
    "            scale_diag=tf.nn.softplus(raw_scale_diag)),\n",
    "        mixture_distribution=tfd.Categorical(logits=mixture_logits),\n",
    "        name=\"prior\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-03T13:07:59.208329Z",
     "start_time": "2019-05-03T13:07:59.195996Z"
    }
   },
   "outputs": [],
   "source": [
    "def pack_images(images, rows, cols):\n",
    "    \"\"\"Helper utility to make a field of images.\"\"\"\n",
    "    shape = tf.shape(images)\n",
    "    width = shape[-3]\n",
    "    height = shape[-2]\n",
    "    depth = shape[-1]\n",
    "    images = tf.reshape(images, (-1, width, height, depth))\n",
    "    batch = tf.shape(images)[0]\n",
    "    rows = tf.minimum(rows, batch)\n",
    "    cols = tf.minimum(batch // rows, cols)\n",
    "    images = images[:rows * cols]\n",
    "    images = tf.reshape(images, (rows, cols, width, height, depth))\n",
    "    images = tf.transpose(images, [0, 2, 1, 3, 4])\n",
    "    images = tf.reshape(images, [1, rows * width, cols * height, depth])\n",
    "    return images\n",
    "\n",
    "\n",
    "def image_tile_summary(name, tensor, rows=8, cols=8):\n",
    "    tf.summary.image(name, pack_images(tensor, rows, cols), max_outputs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-03T13:18:27.429740Z",
     "start_time": "2019-05-03T13:18:27.407132Z"
    }
   },
   "outputs": [],
   "source": [
    "def model_fn(features, labels, mode, params, config):\n",
    "    \"\"\"Builds the model function for use in an estimator.\n",
    "  \n",
    "    Arguments:\n",
    "      features: The input features for the estimator.\n",
    "      labels: The labels, unused here.\n",
    "      mode: Signifies whether it is train or test or predict.\n",
    "      params: Some hyperparameters as a dictionary.\n",
    "      config: The RunConfig, unused here.\n",
    "  \n",
    "    Returns:\n",
    "      EstimatorSpec: A tf.estimator.EstimatorSpec instance.\n",
    "    \"\"\"\n",
    "    if params[\"analytic_kl\"] and params[\"mixture_components\"] != 1:\n",
    "        raise NotImplementedError(\n",
    "            \"Using `analytic_kl` is only supported when `mixture_components = 1` \"\n",
    "            \"since there's no closed form otherwise.\")\n",
    "  \n",
    "    encoder = make_encoder(params[\"activation\"],\n",
    "                           params[\"latent_size\"],\n",
    "                           params[\"base_depth\"])\n",
    "    decoder = make_decoder(params[\"activation\"],\n",
    "                           params[\"latent_size\"],\n",
    "                           IMAGE_SHAPE,\n",
    "                           params[\"base_depth\"])\n",
    "    latent_prior = make_mixture_prior(params[\"latent_size\"],\n",
    "                                      params[\"mixture_components\"])\n",
    "  \n",
    "    image_tile_summary(\"input\", tf.to_float(features), rows=1, cols=16)\n",
    "  \n",
    "    approx_posterior = encoder(features)\n",
    "    approx_posterior_sample = approx_posterior.sample(params[\"n_samples\"])\n",
    "    decoder_likelihood = decoder(approx_posterior_sample)\n",
    "    image_tile_summary(\n",
    "        \"recon/sample\",\n",
    "        tf.to_float(decoder_likelihood.sample()[:3, :16]),\n",
    "        rows=3,\n",
    "        cols=16)\n",
    "    image_tile_summary(\n",
    "        \"recon/mean\",\n",
    "        decoder_likelihood.mean()[:3, :16],\n",
    "        rows=3,\n",
    "        cols=16)\n",
    "  \n",
    "    # `distortion` is just the negative log likelihood.\n",
    "    distortion = -decoder_likelihood.log_prob(features)\n",
    "    avg_distortion = tf.reduce_mean(distortion)\n",
    "    tf.summary.scalar(\"distortion\", avg_distortion)\n",
    "  \n",
    "    if params[\"analytic_kl\"]:\n",
    "        rate = tfd.kl_divergence(approx_posterior, latent_prior)\n",
    "    else:\n",
    "        rate = (approx_posterior.log_prob(approx_posterior_sample)\n",
    "              - latent_prior.log_prob(approx_posterior_sample))\n",
    "    avg_rate = tf.reduce_mean(rate)\n",
    "    tf.summary.scalar(\"rate\", avg_rate)\n",
    "  \n",
    "    elbo_local = -(rate + distortion)\n",
    "  \n",
    "    elbo = tf.reduce_mean(elbo_local)\n",
    "    loss = -elbo\n",
    "    tf.summary.scalar(\"elbo\", elbo)\n",
    "  \n",
    "    importance_weighted_elbo = tf.reduce_mean(\n",
    "        tf.reduce_logsumexp(elbo_local, axis=0) -\n",
    "        tf.log(tf.to_float(params[\"n_samples\"])))\n",
    "    tf.summary.scalar(\"elbo/importance_weighted\", importance_weighted_elbo)\n",
    "  \n",
    "    # Decode samples from the prior for visualization.\n",
    "    random_image = decoder(latent_prior.sample(16))\n",
    "    image_tile_summary(\n",
    "        \"random/sample\", tf.to_float(random_image.sample()), rows=4, cols=4)\n",
    "    image_tile_summary(\"random/mean\", random_image.mean(), rows=4, cols=4)\n",
    "  \n",
    "    # Perform variational inference by minimizing the -ELBO.\n",
    "    global_step = tf.train.get_or_create_global_step()\n",
    "    learning_rate = tf.train.cosine_decay(params[\"learning_rate\"], global_step,\n",
    "                                          params[\"max_steps\"])\n",
    "    tf.summary.scalar(\"learning_rate\", learning_rate)\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "    train_op = optimizer.minimize(loss, global_step=global_step)\n",
    "  \n",
    "    return tf.estimator.EstimatorSpec(\n",
    "        mode=mode,\n",
    "        loss=loss,\n",
    "        train_op=train_op,\n",
    "        eval_metric_ops={\n",
    "            \"elbo\": tf.metrics.mean(elbo),\n",
    "            \"elbo/importance_weighted\": tf.metrics.mean(importance_weighted_elbo),\n",
    "            \"rate\": tf.metrics.mean(avg_rate),\n",
    "            \"distortion\": tf.metrics.mean(avg_distortion),\n",
    "        },\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-03T13:20:24.954448Z",
     "start_time": "2019-05-03T13:20:24.922321Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using config: {'_model_dir': '../neural_clustering_data/logs', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': 500, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fcf8b09cac8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using config: {'_model_dir': '../neural_clustering_data/logs', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': 500, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fcf8b09cac8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'train_input_fn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-f1c713762779>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'max_steps'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'viz_steps'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m     \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_input_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFLAGS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mviz_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m     \u001b[0meval_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meval_input_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Evaluation_results:\\n\\t%s\\n\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0meval_results\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_input_fn' is not defined"
     ]
    }
   ],
   "source": [
    "def build_input_fns(data_dir, batch_size):\n",
    "    \"\"\"Builds an Iterator switching between train and heldout data.\"\"\"\n",
    "  \n",
    "    # Build an iterator over training batches.\n",
    "    def train_input_fn():\n",
    "        dataset = static_mnist_dataset(data_dir, \"train\")\n",
    "        dataset = dataset.shuffle(50000).repeat().batch(batch_size)\n",
    "        return dataset.make_one_shot_iterator().get_next()\n",
    "  \n",
    "    # Build an iterator over the heldout set.\n",
    "    def eval_input_fn():\n",
    "        eval_dataset = static_mnist_dataset(data_dir, \"valid\")\n",
    "        eval_dataset = eval_dataset.batch(batch_size)\n",
    "        return eval_dataset.make_one_shot_iterator().get_next()\n",
    "  \n",
    "    return train_input_fn, eval_input_fn\n",
    "\n",
    "\n",
    "params['activation'] = getattr(tf.nn, params[\"activation\"])\n",
    "\n",
    "#train_input_fn, eval_input_fn = build_fake_input_fns(FLAGS.batch_size)\n",
    "\n",
    "estimator = tf.estimator.Estimator(\n",
    "  model_fn,\n",
    "  params=params,\n",
    "  config=tf.estimator.RunConfig(\n",
    "      model_dir= \"../neural_clustering_data/logs\",\n",
    "      save_checkpoints_steps=params['viz_steps'],\n",
    "  ),\n",
    ")\n",
    "\n",
    "for _ in range(params['max_steps'] // params['viz_steps']):\n",
    "    estimator.train(train_input_fn, steps=params['viz_steps'])\n",
    "    eval_results = estimator.evaluate(eval_input_fn)\n",
    "    print(\"Evaluation_results:\\n\\t%s\\n\" % eval_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-02T20:54:53.713149Z",
     "start_time": "2019-05-02T20:54:53.441931Z"
    }
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Tensor' object has no attribute 'log_prob'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-c33ed62f4916>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mtfkl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtfpl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMultivariateNormalTriL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlatent_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mtfpl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMultivariateNormalTriL\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlatent_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivity_regularizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtfpl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mKLDivergenceRegularizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprior\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m ], name='encoder')\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow/python/training/checkpointable/base.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    440\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 442\u001b[0;31m       \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    443\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    444\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprevious_value\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow/python/keras/engine/sequential.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, layers, name)\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow/python/training/checkpointable/base.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    440\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 442\u001b[0;31m       \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    443\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    444\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprevious_value\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow/python/keras/engine/sequential.py\u001b[0m in \u001b[0;36madd\u001b[0;34m(self, layer)\u001b[0m\n\u001b[1;32m    178\u001b[0m       \u001b[0;31m# If the model is being built continuously on top of an input layer:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m       \u001b[0;31m# refresh its output.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m       \u001b[0moutput_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m         raise TypeError('All layers in a Sequential model '\n",
      "\u001b[0;32m~/.conda/envs/tf_nightly/lib/python3.7/site-packages/tensorflow_probability/python/layers/distribution_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    162\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_enter_dunder_call\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m     distribution, _ = super(DistributionLambda, self).__call__(\n\u001b[0;32m--> 164\u001b[0;31m         inputs, *args, **kwargs)\n\u001b[0m\u001b[1;32m    165\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_enter_dunder_call\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdistribution\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    578\u001b[0m                              \u001b[0;34m'Tensor or a list of Tensors, not None '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m                              '(layer: ' + self.name + ').')\n\u001b[0;32m--> 580\u001b[0;31m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_activity_regularization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    581\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_mask_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprevious_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mbase_layer_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhave_all_keras_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m_handle_activity_regularization\u001b[0;34m(self, inputs, outputs)\u001b[0m\n\u001b[1;32m   1341\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ActivityRegularizer'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1342\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moutput_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1343\u001b[0;31m           \u001b[0mactivity_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1344\u001b[0m           batch_size = math_ops.cast(\n\u001b[1;32m   1345\u001b[0m               array_ops.shape(output)[0], activity_loss.dtype)\n",
      "\u001b[0;32m~/.conda/envs/tf_nightly/lib/python3.7/site-packages/tensorflow_probability/python/layers/distribution_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, distribution_a)\u001b[0m\n\u001b[1;32m    977\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    978\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdistribution_a\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 979\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_kl_divergence_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdistribution_a\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    980\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    981\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/tf_nightly/lib/python3.7/site-packages/tensorflow_probability/python/layers/distribution_layer.py\u001b[0m in \u001b[0;36m_fn\u001b[0;34m(distribution_a)\u001b[0m\n\u001b[1;32m   1098\u001b[0m       distribution_b_ = (distribution_b() if callable(distribution_b)\n\u001b[1;32m   1099\u001b[0m                          else distribution_b)\n\u001b[0;32m-> 1100\u001b[0;31m       \u001b[0mkl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkl_divergence_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdistribution_a\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdistribution_b_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1101\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mweight\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m         \u001b[0mkl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mkl\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/tf_nightly/lib/python3.7/site-packages/tensorflow_probability/python/layers/distribution_layer.py\u001b[0m in \u001b[0;36mkl_divergence_fn\u001b[0;34m(distribution_a, distribution_b)\u001b[0m\n\u001b[1;32m   1086\u001b[0m       \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_points_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdistribution_a\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1087\u001b[0m       return tf.reduce_mean(\n\u001b[0;32m-> 1088\u001b[0;31m           \u001b[0mdistribution_a\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mdistribution_b\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1089\u001b[0m           axis=test_points_reduce_axis)\n\u001b[1;32m   1090\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Tensor' object has no attribute 'log_prob'"
     ]
    }
   ],
   "source": [
    "def dense_layers(sizes):\n",
    "    return tfk.Sequential([tfkl.Dense(size, activation=tf.nn.leaky_relu) for size in sizes])\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "original_dim = data_train.shape[1]\n",
    "input_shape = data_train[0].shape\n",
    "dense_layer_dims = [20, 10, 8]\n",
    "latent_dim = 2\n",
    "batch_size = 128\n",
    "max_epochs = 1000\n",
    "\n",
    "# prior = tfd.Independent(tfd.Normal(loc=tf.zeros(latent_dim), scale=1),\n",
    "#                         reinterpreted_batch_ndims=1)\n",
    "\n",
    "prior = tfd.MultivariateNormalDiag(loc=tf.zeros([latent_dim]), \n",
    "                               scale_identity_multiplier=1.0)\n",
    "\n",
    "encoder = tfk.Sequential([\n",
    "    tfkl.InputLayer(input_shape=input_shape, name='encoder_input'),\n",
    "    dense_layers(dense_layer_dims),\n",
    "    tfkl.Dense(tfpl.MultivariateNormalTriL.params_size(latent_dim), activation=None),\n",
    "    tfpl.MultivariateNormalTriL(latent_dim, activity_regularizer=tfpl.KLDivergenceRegularizer(prior)),\n",
    "], name='encoder')\n",
    "\n",
    "encoder.summary()\n",
    "#plot_model(encoder, to_file='vae_mlp_encoder.png', show_shapes=True)\n",
    "\n",
    "decoder = tfk.Sequential([\n",
    "    tfkl.InputLayer(input_shape=[latent_dim]),\n",
    "    dense_layers(reversed(dense_layer_dims)),\n",
    "    tfkl.Dense(tfpl.IndependentNormal.params_size(original_dim), activation=None),\n",
    "    tfpl.IndependentNormal(original_dim),\n",
    "], name='decoder')\n",
    "\n",
    "decoder.summary()\n",
    "#plot_model(decoder, to_file='vae_mlp_decoder.png', show_shapes=True)\n",
    "\n",
    "vae = tfk.Model(inputs=encoder.inputs,\n",
    "                outputs=decoder(encoder.outputs[0]),\n",
    "                name='vae_mlp')\n",
    "\n",
    "negloglik = lambda x, rv_x: -rv_x.log_prob(x)\n",
    "\n",
    "vae.compile(optimizer=tf.keras.optimizers.Nadam(), \n",
    "            loss=negloglik)\n",
    "\n",
    "vae.summary()\n",
    "plot_model(vae,\n",
    "           to_file='vae_mlp.png',\n",
    "           show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-02T20:54:32.614436Z",
     "start_time": "2019-05-02T20:54:29.597Z"
    }
   },
   "outputs": [],
   "source": [
    "tf_train = tf.data.Dataset.from_tensor_slices((data_train, data_train)).batch(batch_size).prefetch(tf.data.experimental.AUTOTUNE).shuffle(int(10e4))\n",
    "tf_val = tf.data.Dataset.from_tensor_slices((data_test, data_test)).batch(batch_size).prefetch(tf.data.experimental.AUTOTUNE).shuffle(int(10e4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-05-02T20:19:45.037Z"
    }
   },
   "outputs": [],
   "source": [
    "checkpointer = ModelCheckpoint(filepath=f\"{config.__DATA_ROOT__}/experimental/model_checkpoints/vae_v2-0.1.0-mdl.h5\", verbose=0, save_best_only=True)\n",
    "earlystopper = EarlyStopping(monitor='val_loss', mode='min', min_delta=0.005, patience=20, verbose=0, restore_best_weights=True)\n",
    "\n",
    "hist = vae.fit(tf_train,\n",
    "               epochs=max_epochs,\n",
    "               shuffle=True,\n",
    "               verbose=0,\n",
    "               validation_data=tf_val,\n",
    "               callbacks=[checkpointer, earlystopper])\n",
    "\n",
    "\n",
    "plot_loss(hist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# reconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-05-02T20:19:50.146Z"
    }
   },
   "outputs": [],
   "source": [
    "reconstruct_samples_n = 100\n",
    "\n",
    "def reconstruction_log_prob(eval_samples, reconstruct_samples_n):\n",
    "    encoder_out = encoder(eval_samples)\n",
    "    encoder_samples = encoder_out.sample(reconstruct_samples_n)\n",
    "    return np.mean(decoder(encoder_samples).log_prob(eval_samples), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-02T20:18:57.098898Z",
     "start_time": "2019-05-02T20:18:57.064Z"
    }
   },
   "outputs": [],
   "source": [
    "def _reverse_to_original_shape_(pos_data, input_shape=None):\n",
    "    if input_shape is None:\n",
    "        if config.NB_DIMS == 2:\n",
    "            input_shape = (-1, config.NB_DIMS)\n",
    "        else:\n",
    "            input_shape = (-1,)\n",
    "        \n",
    "    return scaler.inverse_transform(pos_data).reshape(pos_data.shape[0], *(input_shape))\n",
    "\n",
    "reconstructed_train =  _reverse_to_original_shape_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-02T20:18:57.099336Z",
     "start_time": "2019-05-02T20:18:57.068Z"
    }
   },
   "outputs": [],
   "source": [
    "plots.plot_comparing_joint_position_with_reconstructed(joint_positions, \n",
    "                                                       np.vstack((reconstructed_from_encoding_train, reconstructed_from_encoding_val)), validation_cut_off=nb_of_data_points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# latent space plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-02T20:18:57.099782Z",
     "start_time": "2019-05-02T20:18:57.070Z"
    }
   },
   "outputs": [],
   "source": [
    "Y = frames_idx_with_labels.label.apply(lambda x: x.value)\n",
    "X = np.vstack((data_train, data_test))\n",
    "latent_x_mean = encoder(X).mean()\n",
    "latent_x_stddev  = encoder(X).stddev()\n",
    "\n",
    "plt.scatter(latent_x_mean[:, 0], latent_x_mean[:, 1], c=Y, cmap='RdYlGn_r', s=2)\n",
    "plt.title('latent means')\n",
    "plt.ylabel('mean[1]')\n",
    "plt.xlabel('mean[0]')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-02T20:18:57.100226Z",
     "start_time": "2019-05-02T20:18:57.076Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.scatter(latent_x_stddev[:, 0], latent_x_stddev[:, 1], c=Y, cmap='RdYlGn_r', s=2)\n",
    "plt.title('latent standard deviations')\n",
    "plt.ylabel('stddev[1]')\n",
    "plt.xlabel('stddev[0]')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-02T20:18:57.100673Z",
     "start_time": "2019-05-02T20:18:57.081Z"
    }
   },
   "outputs": [],
   "source": [
    "latent_x = encoder(X).sample()\n",
    "plt.scatter(latent_x[:, 0], latent_x[:, 1], c=Y, cmap='RdYlGn_r', s=2)\n",
    "plt.title('latent vector samples')\n",
    "plt.ylabel('z[1]')\n",
    "plt.xlabel('z[0]')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-02T20:18:57.101111Z",
     "start_time": "2019-05-02T20:18:57.084Z"
    }
   },
   "outputs": [],
   "source": [
    "x_log_prob = reconstruction_log_prob(X, reconstruct_samples_n)\n",
    "#ax = plt.hist(x_log_prob, 60)\n",
    "plt.hist([x_log_prob[frames_idx_with_labels['label'] == l] for l in seen_labels], 60)\n",
    "plt.title('reconstruction log probability')\n",
    "plt.ylabel('frequency')\n",
    "plt.xlabel(\"log p(x|x')\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
