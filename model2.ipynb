{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-26T15:47:52.294745Z",
     "start_time": "2019-04-26T15:47:50.158320Z"
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "from importlib import reload\n",
    "from collections import namedtuple\n",
    "import inspect\n",
    "from itertools import groupby\n",
    "from datetime import date\n",
    "from functional import seq\n",
    "from functools import reduce, partial\n",
    "from glob import glob\n",
    "import datetime\n",
    "import logging\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from hashlib import sha256\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import pathlib\n",
    "import pickle\n",
    "import re\n",
    "import seaborn as sns\n",
    "import shutil\n",
    "import cv2\n",
    "#import skimage\n",
    "#from skimage import io\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "from tqdm import tqdm, trange\n",
    "import uuid\n",
    "from IPython import display\n",
    "\n",
    "%matplotlib inline\n",
    "#plt.style.use(\"dark_background\")\n",
    "\n",
    "import sys\n",
    "#from drosophpose.GUI import skeleton\n",
    "\n",
    "from som_vae import somvae_model\n",
    "from som_vae.utils import *\n",
    "\n",
    "from som_vae.helpers.misc import extract_args, chunks, foldl\n",
    "from som_vae.helpers.jupyter import fix_layout, display_video\n",
    "from som_vae.settings import config, skeleton\n",
    "from som_vae.helpers import video, plots\n",
    "from som_vae import preprocessing\n",
    "from som_vae.helpers.logging import enable_logging\n",
    "from som_vae.helpers.tensorflow import _TF_DEFAULT_SESSION_CONFIG_\n",
    "    \n",
    "fix_layout()\n",
    "enable_logging()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-26T15:47:52.297451Z",
     "start_time": "2019-04-26T15:47:52.295947Z"
    }
   },
   "outputs": [],
   "source": [
    "#joint_positions = foldl(preprocessing.get_data(), \n",
    "#                        preprocessing.add_third_dimension,\n",
    "#                        preprocessing.get_only_first_legs)[:, :, :config.NB_DIMS]\n",
    "#\n",
    "#NB_FRAMES = joint_positions.shape[0]\n",
    "#__N_INPUT__ = len(config.LEGS) * config.NB_TRACKED_POINTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-26T15:47:52.382607Z",
     "start_time": "2019-04-26T15:47:52.298643Z"
    }
   },
   "outputs": [],
   "source": [
    "from som_vae import settings\n",
    "\n",
    "joint_positions, normalisation_factors = preprocessing.get_data_and_normalization(settings.data.EXPERIMENTS)\n",
    "\n",
    "frames_idx_with_labels = preprocessing.get_frames_with_idx_and_labels(settings.data.LABELLED_DATA)[:len(joint_positions)]\n",
    "\n",
    "#frames_of_interest = frames_idx_with_labels.label.isin([settings.data._BehaviorLabel_.GROOM_ANT, settings.data._BehaviorLabel_.WALK_FORW, settings.data._BehaviorLabel_.REST])\n",
    "frames_of_interest = ~frames_idx_with_labels.label.isin([settings.data._BehaviorLabel_.REST])\n",
    "\n",
    "joint_positions = joint_positions[frames_of_interest]\n",
    "frames_idx_with_labels = frames_idx_with_labels[frames_of_interest]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-26T12:20:24.221482Z",
     "start_time": "2019-04-26T12:20:24.219720Z"
    }
   },
   "source": [
    "## visual check of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-26T15:47:52.385710Z",
     "start_time": "2019-04-26T15:47:52.383903Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "if False:\n",
    "    plots.ploting_frames(joint_positions)\n",
    "    plots.ploting_frames(joint_positions - normalisation_factors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-24T17:20:50.474314Z",
     "start_time": "2019-03-24T17:20:50.470595Z"
    }
   },
   "source": [
    "# SOM-VAE model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-26T15:47:52.397866Z",
     "start_time": "2019-04-26T15:47:52.386684Z"
    }
   },
   "outputs": [],
   "source": [
    "reload(somvae_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-24T18:56:26.024960Z",
     "start_time": "2019-03-24T18:56:26.021674Z"
    }
   },
   "source": [
    "## functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-26T15:47:52.418954Z",
     "start_time": "2019-04-26T15:47:52.399042Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def get_data_generator(data_train, labels_train, data_val, labels_val, time_series):\n",
    "    \"\"\"Creates a data generator for the training.\n",
    "    \n",
    "    Args:\n",
    "        time_series (bool): Indicates whether or not we want interpolated MNIST time series or just\n",
    "            normal MNIST batches.\n",
    "    \n",
    "    Returns:\n",
    "        generator: Data generator for the batches.\"\"\"\n",
    "\n",
    "    def batch_generator(mode=\"train\", batch_size=100):\n",
    "        \"\"\"Generator for the data batches.\n",
    "        \n",
    "        Args:\n",
    "            mode (str): Mode in ['train', 'val'] that decides which data set the generator\n",
    "                samples from (default: 'train').\n",
    "            batch_size (int): The size of the batches (default: 100).\n",
    "            \n",
    "        Yields:\n",
    "            np.array: Data batch.\n",
    "        \"\"\"\n",
    "        assert mode in [\"train\", \"val\"], \"The mode should be in {train, val}.\"\n",
    "        if mode==\"train\":\n",
    "            images = data_train.copy()\n",
    "            labels = labels_train.copy()\n",
    "        elif mode==\"val\":\n",
    "            images = data_val.copy()\n",
    "            labels = labels_val.copy()\n",
    "        \n",
    "        while True:\n",
    "            # TODO does the perumation make sense? if there is no time relationship -> yes\n",
    "            #      but we argue that there is one, would perumutating it destroy this time relationship?\n",
    "            indices = np.random.permutation(np.arange(len(images)))\n",
    "\n",
    "            images = images[indices]\n",
    "            labels = labels[indices]\n",
    "            #if time_series:\n",
    "            #    \"\"\"It's a bit odd:\n",
    "            #    \n",
    "            #    1.) Take a random data point with the next label than the current one.\n",
    "            #    2.) Interpolate from the current data point to the select one in <batch_size> steps\n",
    "            #    3.) Add noise to it (normal)\n",
    "            #    \n",
    "            #    \n",
    "            #    WILL NOT WORK ON PRODUCTION: what to do if no labels are available?\n",
    "            #    \"\"\"\n",
    "            #    n_labels = len(np.unique(labels.reshape(-1)))\n",
    "            #    for i, image in enumerate(images):\n",
    "            #        start_image = image\n",
    "            #        end_image = images[np.random.choice(np.where(labels == (labels[i] + 1) % n_labels)[0])]\n",
    "            #        interpolation = interpolate_arrays(start_image, end_image, batch_size)\n",
    "            #        yield interpolation + np.random.normal(scale=0.01, size=interpolation.shape)\n",
    "            #else:\n",
    "            for i in range(len(images)//batch_size):\n",
    "                yield images[i*batch_size:(i+1)*batch_size]\n",
    "\n",
    "    return batch_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-26T15:47:52.436222Z",
     "start_time": "2019-04-26T15:47:52.421781Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def train_model(model, x, lr_val, num_epochs, patience, batch_size, logdir,\n",
    "        modelpath, learning_rate, interactive, generator):\n",
    "    \"\"\"Trains the SOM-VAE model.\n",
    "    \n",
    "    Args:\n",
    "        model (SOM-VAE): SOM-VAE model to train.\n",
    "        x (tf.Tensor): Input tensor or placeholder.\n",
    "        lr_val (tf.Tensor): Placeholder for the learning rate value.\n",
    "        num_epochs (int): Number of epochs to train.\n",
    "        patience (int): Patience parameter for the early stopping.\n",
    "        batch_size (int): Batch size for the training generator.\n",
    "        logdir (path): Directory for saving the logs.\n",
    "        modelpath (path): Path for saving the model checkpoints.\n",
    "        learning_rate (float): Learning rate for the optimization.\n",
    "        interactive (bool): Indicator if we want to have an interactive\n",
    "            progress bar for training.\n",
    "        generator (generator): Generator for the data batches.\n",
    "    \"\"\"\n",
    "    train_gen = generator(\"train\", batch_size)\n",
    "    val_gen = generator(\"val\", batch_size)\n",
    "\n",
    "    num_batches = len(data_train)//batch_size\n",
    "\n",
    "    saver = tf.train.Saver(keep_checkpoint_every_n_hours=2.)\n",
    "    summaries = tf.summary.merge_all()\n",
    "    \n",
    "    with tf.Session(config=_TF_DEFAULT_SESSION_CONFIG_) as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        patience_count = 0\n",
    "        train_losses = []\n",
    "        test_losses = []\n",
    "        test_losses_reconstrution = []\n",
    "        train_writer = tf.summary.FileWriter(logdir+\"/train\", sess.graph)\n",
    "        test_writer = tf.summary.FileWriter(logdir+\"/test\", sess.graph)\n",
    "        print(\"Training...\")\n",
    "        train_step_SOMVAE, train_step_prob = model.optimize\n",
    "        try:\n",
    "            if interactive:\n",
    "                pbar = tqdm(total=num_epochs*(num_batches)) \n",
    "            for epoch in range(num_epochs):\n",
    "                batch_val = next(val_gen)\n",
    "                test_loss, summary, test_loss_reconstruction = sess.run([model.loss, summaries, model.loss_reconstruction], feed_dict={x: batch_val})\n",
    "                test_losses.append(test_loss)\n",
    "                test_losses_reconstrution.append(test_loss_reconstruction)\n",
    "                test_writer.add_summary(summary, tf.train.global_step(sess, model.global_step))\n",
    "                if test_losses[-1] == min(test_losses):\n",
    "                    saver.save(sess, modelpath, global_step=epoch)\n",
    "                    patience_count = 0\n",
    "                else:\n",
    "                    patience_count += 1\n",
    "                if patience_count >= patience:\n",
    "                    break\n",
    "                for i in range(num_batches):\n",
    "                    batch_data = next(train_gen)\n",
    "                    \n",
    "                    if i%100 == 0:\n",
    "                        train_loss, summary = sess.run([model.loss, summaries], feed_dict={x: batch_data})\n",
    "                        train_writer.add_summary(summary, tf.train.global_step(sess, model.global_step))\n",
    "                        train_losses += [train_loss]\n",
    "                        \n",
    "                    train_step_SOMVAE.run(feed_dict={x: batch_data, lr_val:learning_rate})\n",
    "                    train_step_prob.run(feed_dict={x: batch_data, lr_val:learning_rate*100})\n",
    "                    \n",
    "                    if interactive:\n",
    "                        pbar.set_postfix(epoch=epoch, train_loss=train_loss, test_loss=test_loss, refresh=False)\n",
    "                        pbar.update(1)\n",
    "\n",
    "        except KeyboardInterrupt:\n",
    "            pass\n",
    "        finally:\n",
    "            saver.save(sess, modelpath)\n",
    "            if interactive:\n",
    "                pbar.close()\n",
    "                \n",
    "    return test_losses, train_losses, test_losses_reconstrution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-26T15:47:52.450902Z",
     "start_time": "2019-04-26T15:47:52.438036Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def evaluate_model(model, x, modelpath, batch_size, data, labels=None, tf_session_config=None):\n",
    "    \"\"\"Evaluates the performance of the trained model in terms of normalized\n",
    "    mutual information, purity and mean squared error.\n",
    "    \n",
    "    Args:\n",
    "        model (SOM-VAE): Trained SOM-VAE model to evaluate.\n",
    "        x (tf.Tensor): Input tensor or placeholder.\n",
    "        modelpath (path): Path from which to restore the model.\n",
    "        batch_size (int): Batch size for the evaluation.\n",
    "        \n",
    "    Returns:\n",
    "        dict: Dictionary of evaluation results (NMI, Purity, MSE).\n",
    "        x hat, reconstructed data\n",
    "        cluster assignments for each row\n",
    "        encoding of x\n",
    "    \"\"\"\n",
    "    if tf_session_config is None:\n",
    "        tf_session_config = _TF_DEFAULT_SESSION_CONFIG_\n",
    "    \n",
    "    saver = tf.train.Saver(keep_checkpoint_every_n_hours=2.)\n",
    "    \n",
    "    num_batches = len(data)//batch_size\n",
    "    \n",
    "    def _concat_(xs):\n",
    "        if len(xs[0].shape) == 1:\n",
    "            return np.hstack(xs)\n",
    "        else:\n",
    "            return np.vstack(xs)\n",
    "    \n",
    "    with tf.Session(config=tf_session_config) as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        saver.restore(sess, modelpath)\n",
    "\n",
    "        #everything = [sess.run([model.k,  model.x_hat_embedding, model.x_hat_encoding, model.z_e],  feed_dict={x: batch_data}) for batch_data in chunks(data, num_batches)]\n",
    "        cluster_assignments, x_hat_embedding, x_hat_encoding, x_hat_latent = [_concat_(_r) for _r in  \n",
    "                                                                              zip(*[sess.run([model.k,  \n",
    "                                                                                              model.x_hat_embedding,\n",
    "                                                                                              model.x_hat_encoding,\n",
    "                                                                                              model.z_e],  feed_dict={x: batch_data}) \n",
    "                                                                                    for batch_data in chunks(data, num_batches)])]\n",
    "\n",
    "        cluster_assignments = cluster_assignments.reshape(-1)\n",
    "        mse_encoding = mean_squared_error(x_hat_encoding.flatten(), data.flatten())\n",
    "        mse_embedding = mean_squared_error(x_hat_embedding.flatten(), data.flatten())\n",
    "        #if labels is not None:\n",
    "        #    nmi = compute_NMI(cluster_assignments.tolist(), labels[:len(cluster_assignments)])\n",
    "        #    purity = compute_purity(cluster_assignments.tolist(), labels[:len(cluster_assignments)])\n",
    "\n",
    "    results = {}\n",
    "    #results[\"NMI\"] = nmi \n",
    "    #results[\"Purity\"] = purity \n",
    "    results[\"MSE (encoding)\"] = mse_encoding \n",
    "    results[\"MSE (embedding)\"] = mse_embedding \n",
    "    results[\"nb of used clusters\"] = len(np.unique(cluster_assignments))\n",
    "#    results[\"optimization_target\"] = 1 - test_nmi\n",
    "\n",
    "    return results, x_hat_embedding, cluster_assignments, x_hat_encoding, x_hat_latent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-26T15:47:52.472046Z",
     "start_time": "2019-04-26T15:47:52.453557Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def model_main(X_train, X_val, y_train, y_val, latent_dim, som_dim, learning_rate, decay_factor, alpha, beta, gamma, tau, modelpath, save_model, image_like_input, time_series, config):\n",
    "    \"\"\"Main method to build a model, train it and evaluate it.\n",
    "    \n",
    "    Args:\n",
    "        latent_dim (int): Dimensionality of the SOM-VAE's latent space.\n",
    "        som_dim (list): Dimensionality of the SOM.\n",
    "        learning_rate (float): Learning rate for the training.\n",
    "        decay_factor (float): Factor for the learning rate decay.\n",
    "        alpha (float): Weight for the commitment loss.\n",
    "        beta (float): Weight for the SOM loss.\n",
    "        gamma (float): Weight for the transition probability loss.\n",
    "        tau (float): Weight for the smoothness loss.\n",
    "        modelpath (path): Path for the model checkpoints.\n",
    "        save_model (bool): Indicates if the model should be saved after training and evaluation.\n",
    "        \n",
    "        \n",
    "    Returns:\n",
    "        dict: Results of the evaluation (NMI, Purity, MSE).\n",
    "    \"\"\"\n",
    "    print(f\"running with config: {config}\")\n",
    "    ## TODO\n",
    "    #input_shape: e.g. (15, 1)  for flat data (flattened tabular)\n",
    "    #                  (28, 28, 3) for image like data\n",
    "   \n",
    "    if config['image_like_input']:\n",
    "        raise NotImplementedError\n",
    "        input_length = __NB_DIMS__\n",
    "        input_channels = __N_INPUT__\n",
    "        x = tf.placeholder(tf.float32, shape=[None, input_length, input_channels, 1]) # for image\n",
    "    else:\n",
    "        input_length = config['input_length']\n",
    "        #input_channels = __N_INPUT__ * __NB_DIMS__\n",
    "        input_channels = config['input_channels']\n",
    "        #x = tf.placeholder(tf.float32, shape=[None, input_length, input_channels]) \n",
    "        placeholder_shape = [None, *X_train.shape[1:]]\n",
    "        print(f\"placeholder_shape: {placeholder_shape}\")\n",
    "        x = tf.placeholder(tf.float32, shape=placeholder_shape) \n",
    "        \n",
    "    data_generator = get_data_generator(data_train=X_train, data_val=X_val, labels_train=y_train, labels_val=y_val,time_series=time_series)\n",
    "\n",
    "    lr_val = tf.placeholder_with_default(learning_rate, [])\n",
    "\n",
    "    # TODO pack all these variables in a config, at the end of the project extract them again\n",
    "    model = somvae_model.SOMVAE(inputs=x, latent_dim=latent_dim, som_dim=som_dim, learning_rate=lr_val, decay_factor=decay_factor,\n",
    "            input_length=input_length, input_channels=input_channels, alpha=alpha, beta=beta, gamma=gamma,\n",
    "            tau=tau, image_like_input=image_like_input, config=config)\n",
    "\n",
    "    test_losses, train_losses, test_losses_reconstruction = train_model(model, x, lr_val, generator=data_generator, **extract_args(config, train_model))\n",
    "\n",
    "    result =     evaluate_model(model, x, data=X_train, labels=y_train, **extract_args(config, evaluate_model))\n",
    "    result_val = evaluate_model(model, x, data=X_val,   labels=y_val,   **extract_args(config, evaluate_model))\n",
    "    \n",
    "\n",
    "    if not save_model:\n",
    "        shutil.rmtree(os.path.dirname(modelpath))\n",
    "        \n",
    "    print(f\"got (train): {result[0]}\")\n",
    "    print(f\"got (val: {result_val[0]}\")\n",
    "\n",
    "    return result, model, (train_losses, test_losses, test_losses_reconstruction), result_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-26T15:48:46.001215Z",
     "start_time": "2019-04-26T15:48:45.991586Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "## config\n",
    "\n",
    "_name_ = \"time_series_experiments\"\n",
    "_latent_dim_ = 16 \n",
    "_som_dim = [8,8]\n",
    "\n",
    "# TODO add hash of config to modelpath\n",
    "\n",
    "som_vae_config = {\n",
    "    \"num_epochs\": 400,\n",
    "    \"patience\": 100,\n",
    "    \"batch_size\": 10, # len(joint_positions), # if time_series then each batch should be a time series\n",
    "    \"latent_dim\": _latent_dim_,\n",
    "    \"som_dim\": _som_dim,\n",
    "    \"learning_rate\": 0.0005,\n",
    "    #\"alpha\": 0.0, #1.0,\n",
    "    #\"beta\": 0.0, #0.9,\n",
    "    #\"gamma\": 0.0, #1.8,\n",
    "    #\"tau\": 0.0, # 1.4,\n",
    "    \"alpha\": 0.0,                          # commit loss\n",
    "    \"beta\": 0.0,                           # loss som\n",
    "    \"gamma\": 0.0,                          # loss proba\n",
    "    \"tau\": 0.0,                            # loss z proba\n",
    "    \"loss_reconstruction_encoding\": 1.0,\n",
    "    \"loss_reconstruction_embedding\": 0.0,\n",
    "    \"decay_factor\": 0.9,\n",
    "    \"name\": _name_,\n",
    "    \"interactive\": True, # this is just for the progress bar\n",
    "    \"data_set\": \"MNIST_data\",\n",
    "    \"save_model\": False,\n",
    "    \"time_series\": True,\n",
    "    \"image_like_input\": False,\n",
    "    \"activation_fn\": \"relu\", # or relu -> with normalisation layer?\n",
    "    \"input_channels\": joint_positions.shape[1] * config.NB_DIMS,\n",
    "    \"input_length\": 1\n",
    "}\n",
    "\n",
    "# todo add git commit hash to config name\n",
    "# todo change _name_ to something meaningful\n",
    "\n",
    "_ex_name_ = \"{}_{}_{}-{}_{}_{}\".format(_name_, _latent_dim_, _som_dim[0], _som_dim[1], datetime.datetime.now().strftime('%Y-%m-%d_%H-%M-%S'), sha256(json.dumps(som_vae_config, sort_keys=True).encode()).hexdigest()[:5])\n",
    "som_vae_config['ex_name'] = _ex_name_\n",
    "\n",
    " \n",
    "som_vae_config[\"logdir\"] = \"../neural_clustering_data/logs/{}\".format(_ex_name_)\n",
    "som_vae_config[\"modelpath\"] = \"../neural_clustering_data/models/{0}/{0}.ckpt\".format(_ex_name_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-26T15:48:46.309976Z",
     "start_time": "2019-04-26T15:48:46.304725Z"
    }
   },
   "outputs": [],
   "source": [
    "# creating path to store model\n",
    "pathlib.Path(som_vae_config['modelpath']).parent.mkdir(parents=True, exist_ok=True)\n",
    "pathlib.Path(som_vae_config['logdir']).parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "_MODEL_CONFIG_PATH_ = pathlib.Path(som_vae_config['logdir']).parent.parent / 'model_configs'\n",
    "_MODEL_CONFIG_PATH_.mkdir(exist_ok=True)\n",
    "with open(f\"{_MODEL_CONFIG_PATH_}/{som_vae_config['ex_name']}.json\", 'w') as f:\n",
    "    json.dump(som_vae_config, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-26T15:48:46.653793Z",
     "start_time": "2019-04-26T15:48:46.649498Z"
    }
   },
   "outputs": [],
   "source": [
    "def to_time_series(data, sequence_length=som_vae_config['batch_size']):\n",
    "    for i in range(len(data)):\n",
    "        if i + sequence_length <= len(data):\n",
    "            yield data[i:i+sequence_length]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-26T15:48:46.885311Z",
     "start_time": "2019-04-26T15:48:46.846374Z"
    }
   },
   "outputs": [],
   "source": [
    "# flatten the data\n",
    "reshaped_joint_position = joint_positions[:,:,: config.NB_DIMS].reshape(joint_positions.shape[0], -1)\n",
    "print(f\"shape of input data:{reshaped_joint_position.shape}\")\n",
    "\n",
    "# TODO move scaler back down\n",
    "warnings.warn('scaling at this point is not good science, this is just a quick fix')\n",
    "scaler = MinMaxScaler()\n",
    "reshaped_joint_position = scaler.fit_transform(reshaped_joint_position)\n",
    "\n",
    "assert np.product(reshaped_joint_position.shape[1:]) > som_vae_config['latent_dim'],\\\n",
    "       'latent dimension should be strictly smaller than input dimensions, otherwise it\\'s not really a VAE...'\n",
    "\n",
    "if som_vae_config['time_series']:\n",
    "    # duplicating the elements, e.g.: [0, 1, 2], [1, 2, 3], [2, 3, 4], ...\n",
    "    # this has a sequence length of 3\n",
    "    _time_series_idx_ = list(to_time_series(range(len(joint_positions))))\n",
    "    _jp = np.concatenate([reshaped_joint_position[idx].reshape(1, -1, reshaped_joint_position.shape[1]) for idx in _time_series_idx_], axis=0)\\\n",
    "            .reshape(-1, som_vae_config['batch_size'] * reshaped_joint_position.shape[1])\n",
    "    som_vae_config['input_channels'] = som_vae_config['batch_size'] * reshaped_joint_position.shape[1]\n",
    "else:\n",
    "    _jp = reshaped_joint_position \n",
    "    \n",
    "#nb_of_data_points = (reshaped_joint_position.shape[0] // config['batch_size']) * config['batch_size']\n",
    "# train - test split\n",
    "nb_of_data_points = int(_jp.shape[0] * 0.7)\n",
    "\n",
    "# scaling the data to be in [0, 1]\n",
    "# this is due to the sigmoid activation function in the reconstruction\n",
    "#resh = scaler.fit_transform(resh)\n",
    "#data_train = scaler.fit_transform(_jp[:nb_of_data_points])\n",
    "#data_test = scaler.transform(_jp[nb_of_data_points:])\n",
    "\n",
    "data_train = _jp[:nb_of_data_points]\n",
    "data_test = _jp[nb_of_data_points:]\n",
    "\n",
    "# just generating some labels, no clue what they are for except validation?\n",
    "labels = frames_idx_with_labels['label'].apply(lambda x: x.value).values\n",
    "\n",
    "if som_vae_config['time_series']:\n",
    "    labels = np.concatenate([labels[idx].reshape(1, -1, 1) for idx in _time_series_idx_], axis=0)[:,-1]\n",
    "\n",
    "data = {\n",
    "  \"X_train\": data_train,\n",
    "  \"X_val\": data_test,\n",
    "  \"y_train\": labels[:nb_of_data_points],\n",
    "  \"y_val\": labels[nb_of_data_points:]\n",
    "}\n",
    "\n",
    "for k, v in data.items():\n",
    "    print(k, v.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## running fit & test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-26T15:50:30.156384Z",
     "start_time": "2019-04-26T15:48:47.891591Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "reload(somvae_model)\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "_args = inspect.getfullargspec(model_main).args\n",
    "res, mdl, losses, res_val = model_main(**{**{k:som_vae_config[k] for k in _args if k in som_vae_config}, **data, \"config\": som_vae_config})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-26T15:50:30.163958Z",
     "start_time": "2019-04-26T15:50:30.157780Z"
    }
   },
   "outputs": [],
   "source": [
    "def _reverse_to_original_shape_(pos_data, input_shape=None):\n",
    "    if input_shape is None:\n",
    "        input_shape = (-1, config.NB_DIMS)\n",
    "        \n",
    "    return scaler.inverse_transform(pos_data).reshape(pos_data.shape[0], *(input_shape))\n",
    "\n",
    "\n",
    "def _time_series_reverser_(xs, is_time_series=som_vae_config['time_series'], original_size=None):\n",
    "    if is_time_series:\n",
    "        return xs.reshape(-1, som_vae_config['batch_size'], 30)[:, -1]\n",
    "    else:\n",
    "        return xs\n",
    "\n",
    "reconstructed_from_embedding_train =  _reverse_to_original_shape_(_time_series_reverser_(res[1]))\n",
    "reconstructed_from_embedding_val   =  _reverse_to_original_shape_(_time_series_reverser_(res_val[1]))\n",
    "reconstructed_from_encoding_train  =  _reverse_to_original_shape_(_time_series_reverser_(res[3]))\n",
    "reconstructed_from_encoding_val    =  _reverse_to_original_shape_(_time_series_reverser_(res_val[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-26T15:50:30.667071Z",
     "start_time": "2019-04-26T15:50:30.165330Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "f =plots.plot_losses(losses)\n",
    "plots.plot_latent_frame_distribution(res[2], nb_bins=_latent_dim_)\n",
    "plots.plot_cluster_assignment_over_time(res[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-26T15:50:53.369038Z",
     "start_time": "2019-04-26T15:50:53.351551Z"
    }
   },
   "outputs": [],
   "source": [
    "_time_series_idx_ = list(to_time_series(range(len(joint_positions))))\n",
    "timed_jp = np.concatenate([joint_positions[idx,:,:2].reshape(1, -1, reshaped_joint_position.shape[1]) for idx in _time_series_idx_], axis=0)[:, -1].reshape(-1, 15, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-26T15:50:56.656037Z",
     "start_time": "2019-04-26T15:50:53.777537Z"
    }
   },
   "outputs": [],
   "source": [
    "plots.plot_comparing_joint_position_with_reconstructed(timed_jp, \n",
    "                                                 np.vstack((reconstructed_from_encoding_train, reconstructed_from_encoding_val)), validation_cut_off=nb_of_data_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-26T15:48:29.578483Z",
     "start_time": "2019-04-26T15:48:26.878896Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plots.plot_comparing_joint_position_with_reconstructed(joint_positions, \n",
    "                                                 np.vstack((reconstructed_from_encoding_train, reconstructed_from_encoding_val)), validation_cut_off=nb_of_data_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-26T15:48:32.132595Z",
     "start_time": "2019-04-26T15:48:29.579787Z"
    }
   },
   "outputs": [],
   "source": [
    "plots.plot_comparing_joint_position_with_reconstructed(joint_positions, \n",
    "                                                 np.vstack((reconstructed_from_embedding_train, reconstructed_from_embedding_val)), validation_cut_off=nb_of_data_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-26T15:51:30.736678Z",
     "start_time": "2019-04-26T15:51:30.723562Z"
    }
   },
   "outputs": [],
   "source": [
    "from numpy.lib.index_tricks import s_\n",
    "\n",
    "def mean_squared_difference(a, b):\n",
    "    return ((a - b) ** 2).mean()\n",
    "\n",
    "\n",
    "_n_train_ = len(res[3])\n",
    "_idx_ = (s_[:_n_train_,:,:config.NB_DIMS], s_[_n_train_:,:,:config.NB_DIMS]) * 2\n",
    "_order_split_ = ['train', 'test'] * 2\n",
    "_order_ = ['encoding'] * 2 + ['embedding'] * 2\n",
    "_loop_data_ = (reconstructed_from_encoding_train, reconstructed_from_encoding_val, reconstructed_from_embedding_train, reconstructed_from_embedding_val)\n",
    "\n",
    "for i, o, order_split, d in zip(_idx_, _order_, _order_split_, _loop_data_):\n",
    "    if som_vae_config['time_series']:\n",
    "        print(f\"MSE for {o}\\t{order_split}:\\t{mean_squared_difference(timed_jp[i], d)}\")\n",
    "    else:\n",
    "        print(f\"MSE for {o}\\t{order_split}:\\t{mean_squared_difference(joint_positions[i], d)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-25T16:06:39.862218Z",
     "start_time": "2019-03-25T16:06:39.855671Z"
    }
   },
   "source": [
    "## cool videos "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-26T15:51:31.652588Z",
     "start_time": "2019-04-26T15:51:31.650495Z"
    }
   },
   "outputs": [],
   "source": [
    "def reverse_pos_pipeline(x, normalisation_term=normalisation_factors):\n",
    "    \"\"\"TODO This is again pretty shitty... ultra hidden global variable\"\"\"\n",
    "    return x + normalisation_term[:x.shape[-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-26T15:51:31.796722Z",
     "start_time": "2019-04-26T15:51:31.793130Z"
    }
   },
   "outputs": [],
   "source": [
    "from som_vae.helpers.video import _float_to_int_color_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-26T15:51:31.921110Z",
     "start_time": "2019-04-26T15:51:31.917172Z"
    }
   },
   "outputs": [],
   "source": [
    "cluster_assignments = np.hstack((res[2], res_val[2]))\n",
    "\n",
    "cluster_ids = np.unique(cluster_assignments)\n",
    "cluster_colors = dict(zip(cluster_ids, _float_to_int_color_(sns.color_palette(palette='bright', n_colors=len(cluster_ids)))))\n",
    "\n",
    "joint_pos_embedding = np.vstack((reconstructed_from_embedding_train, reconstructed_from_embedding_val))\n",
    "joint_pos_encoding = np.vstack((reconstructed_from_encoding_train, reconstructed_from_encoding_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-26T15:51:32.225975Z",
     "start_time": "2019-04-26T15:51:32.223145Z"
    }
   },
   "outputs": [],
   "source": [
    "from som_vae.settings.data import EXPERIMENTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-26T15:48:32.216732Z",
     "start_time": "2019-04-26T15:48:32.204778Z"
    }
   },
   "outputs": [],
   "source": [
    "EXPERIMENTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-26T15:51:36.028784Z",
     "start_time": "2019-04-26T15:51:36.002002Z"
    }
   },
   "outputs": [],
   "source": [
    "images_paths_for_experiments = EXPERIMENTS.map(lambda x: (x, config.positional_data(x)))\\\n",
    "                                          .flat_map(lambda x: [(x[0], config.get_path_for_image(x[0], i)) for i in range(x[1].shape[1])])\\\n",
    "                                          .to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-26T15:51:36.378868Z",
     "start_time": "2019-04-26T15:51:36.365134Z"
    }
   },
   "outputs": [],
   "source": [
    "images_paths_for_experiments = np.array(images_paths_for_experiments)[frames_of_interest]\n",
    "\n",
    "if som_vae_config['time_series']:\n",
    "    images_paths_for_experiments = [images_paths_for_experiments[idx[-1]] for idx in _time_series_idx_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-26T15:51:36.628608Z",
     "start_time": "2019-04-26T15:51:36.621339Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "reload(video)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-26T15:51:50.781662Z",
     "start_time": "2019-04-26T15:51:36.913695Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# full video\n",
    "_p = video.comparision_video_of_reconstruction([reverse_pos_pipeline(p) for p in [joint_positions, joint_pos_encoding, joint_pos_embedding]],\n",
    "                                         images_paths_for_experiments=images_paths_for_experiments,\n",
    "                                         cluster_assignments=cluster_assignments,\n",
    "                                         cluster_colors=cluster_colors,\n",
    "                                         n_train=nb_of_data_points)\n",
    "\n",
    "print(_p)\n",
    "display_video(_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-26T15:48:39.816375Z",
     "start_time": "2019-04-26T15:47:50.283Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Creating videos for each cluster\n",
    "from som_vae.helpers import misc\n",
    "reload(video)\n",
    "from collections import OrderedDict\n",
    "\n",
    "__N_CLUSTER_TO_VIZ__ = 10\n",
    "\n",
    "_positional_data = [reverse_pos_pipeline(p) for p in [joint_positions, joint_pos_encoding, joint_pos_embedding]]\n",
    "\n",
    "_t = [(misc.flatten(sequences), cluster_id) for cluster_id, sequences in video.group_by_cluster(cluster_assignments).items()]\n",
    "_t = sorted(_t, key=lambda x: len(x[0]), reverse=True)\n",
    "\n",
    "cluster_vids = OrderedDict((p[1], video.comparision_video_of_reconstruction(_positional_data,\n",
    "                                                                      cluster_assignments=cluster_assignments,\n",
    "                                                                      images_paths_for_experiments=images_paths_for_experiments,\n",
    "                                                                      n_train=res[2].shape[0],\n",
    "                                                                      cluster_colors=cluster_colors,\n",
    "                                                                      cluster_id_to_visualize=p[1]))\n",
    "                    for p in _t[:__N_CLUSTER_TO_VIZ__])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-26T15:48:39.817423Z",
     "start_time": "2019-04-26T15:47:50.288Z"
    }
   },
   "outputs": [],
   "source": [
    "cluster_vids.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-26T15:48:39.818588Z",
     "start_time": "2019-04-26T15:47:50.294Z"
    }
   },
   "outputs": [],
   "source": [
    "# specific cluster id\n",
    "#cluster_id_of_interest = 57\n",
    "#display_video(cluster_vids[cluster_id_of_interest])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-26T15:48:39.819621Z",
     "start_time": "2019-04-26T15:47:50.298Z"
    }
   },
   "outputs": [],
   "source": [
    "# order by total size\n",
    "idx = 0\n",
    "display_video(list(cluster_vids.values())[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-26T15:48:39.820661Z",
     "start_time": "2019-04-26T15:47:50.303Z"
    }
   },
   "outputs": [],
   "source": [
    "idx += 1\n",
    "display_video(list(cluster_vids.values())[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-11T18:19:54.954651Z",
     "start_time": "2019-04-11T18:19:54.950591Z"
    }
   },
   "source": [
    "# on latent space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-26T15:51:50.785566Z",
     "start_time": "2019-04-26T15:51:50.783363Z"
    }
   },
   "outputs": [],
   "source": [
    "x_hat_latent_train = res[4]\n",
    "x_hat_latent_test  = res_val[4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## t-SNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-26T15:52:08.514006Z",
     "start_time": "2019-04-26T15:51:50.787245Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "X_embedded = TSNE(n_components=2, random_state=42).fit_transform(np.concatenate((x_hat_latent_train, x_hat_latent_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-26T15:52:08.526121Z",
     "start_time": "2019-04-26T15:52:08.515294Z"
    }
   },
   "outputs": [],
   "source": [
    "if som_vae_config['time_series']:\n",
    "    _time_series_last_frame_idx_ = [idx[-1] for idx in _time_series_idx_]\n",
    "\n",
    "    training_frames = frames_idx_with_labels[frames_of_interest].iloc[_time_series_last_frame_idx_][:x_hat_latent_train.shape[0]]\n",
    "    testing_frames = frames_idx_with_labels[frames_of_interest].iloc[_time_series_last_frame_idx_][x_hat_latent_train.shape[0]:]\n",
    "else:\n",
    "    training_frames = frames_idx_with_labels[frames_of_interest][:x_hat_latent_train.shape[0]]\n",
    "    testing_frames = frames_idx_with_labels[frames_of_interest][x_hat_latent_train.shape[0]:]\n",
    "    \n",
    "seen_labels = training_frames.label.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-26T15:52:08.793042Z",
     "start_time": "2019-04-26T15:52:08.527468Z"
    }
   },
   "outputs": [],
   "source": [
    "_cs = sns.color_palette(n_colors=len(seen_labels))\n",
    "\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "_all_frames_ = pd.concat((training_frames, testing_frames))\n",
    "\n",
    "behaviour_colours = dict(zip(seen_labels, _cs))\n",
    "\n",
    "for l, c in behaviour_colours.items():\n",
    "    _d = X_embedded[_all_frames_['label'] == l]\n",
    "    # c=[c] since matplotlib asks for it\n",
    "    plt.scatter(_d[:, 0], _d[:,1], c=[c], label=l.name, marker='.')\n",
    "    \n",
    "plt.legend()\n",
    "plt.title('simple t-SNE on latent space');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-26T15:53:44.000592Z",
     "start_time": "2019-04-26T15:52:08.794007Z"
    }
   },
   "outputs": [],
   "source": [
    "reload(video)\n",
    "if som_vae_config['time_series']:\n",
    "    original_jp = timed_jp\n",
    "else:\n",
    "    original_jp = joint_positions\n",
    "    \n",
    "_p = video.comparision_video_of_reconstruction([reverse_pos_pipeline(p) for p in [original_jp, joint_pos_encoding, joint_pos_embedding]],\n",
    "                                               images_paths_for_experiments=images_paths_for_experiments,\n",
    "                                               cluster_assignments=cluster_assignments,\n",
    "                                               cluster_colors=cluster_colors,\n",
    "                                               n_train=res[2].shape[0],\n",
    "                                               as_frames=True)\n",
    "\n",
    "\n",
    "_embedding_imgs = (video.plot_embedding_assignment(i, X_embedded, frames_idx_with_labels[frames_of_interest].iloc[_time_series_last_frame_idx_]) for i in range(len(X_embedded)))\n",
    "frames = (video.combine_images_h(fly_img, embedding_img) for fly_img, embedding_img in zip(_p, _embedding_imgs))\n",
    "\n",
    "embedding_with_recon_path = f\"../neural_clustering_data/videos/{som_vae_config['ex_name']}_embedding_with_recon.mp4\"\n",
    "video._save_frames_(embedding_with_recon_path, frames)\n",
    "display_video(embedding_with_recon_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-11T18:38:53.635350Z",
     "start_time": "2019-04-11T18:38:53.630494Z"
    }
   },
   "source": [
    "## linear model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-26T15:48:39.826578Z",
     "start_time": "2019-04-26T15:47:50.442Z"
    }
   },
   "outputs": [],
   "source": [
    "y_train = training_frames.label.apply(lambda x: x.value)\n",
    "y_test = testing_frames.label.apply(lambda x: x.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-26T15:48:39.827358Z",
     "start_time": "2019-04-26T15:47:50.445Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.utils.multiclass import unique_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-26T15:48:39.828019Z",
     "start_time": "2019-04-26T15:47:50.450Z"
    }
   },
   "outputs": [],
   "source": [
    "mdl = AdaBoostClassifier()\n",
    "mdl.fit(x_hat_latent_train, y_train)\n",
    "\n",
    "y_pred_train = mdl.predict(x_hat_latent_train)\n",
    "y_pred_test = mdl.predict(x_hat_latent_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-26T15:48:39.828780Z",
     "start_time": "2019-04-26T15:47:50.454Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(y_true, y_pred, classes,\n",
    "                          normalize=False,\n",
    "                          title=None,\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if not title:\n",
    "        if normalize:\n",
    "            title = 'Normalized confusion matrix'\n",
    "        else:\n",
    "            title = 'Confusion matrix, without normalization'\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    # Only use the labels that appear in the data\n",
    "    #classes = classes[unique_labels(y_true, y_pred)]\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    ax.figure.colorbar(im, ax=ax)\n",
    "    # We want to show all ticks...\n",
    "    ax.set(xticks=np.arange(cm.shape[1]),\n",
    "           yticks=np.arange(cm.shape[0]),\n",
    "           # ... and label them with the respective list entries\n",
    "           xticklabels=classes, yticklabels=classes,\n",
    "           title=title,\n",
    "           ylabel='True label',\n",
    "           xlabel='Predicted label')\n",
    "\n",
    "    # Rotate the tick labels and set their alignment.\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "             rotation_mode=\"anchor\")\n",
    "\n",
    "    # Loop over data dimensions and create text annotations.\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(j, i, format(cm[i, j], fmt),\n",
    "                    ha=\"center\", va=\"center\",\n",
    "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    fig.tight_layout()\n",
    "    return ax\n",
    "\n",
    "\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "plot_confusion_matrix(y_train , y_pred_train, classes=np.array([l.name for l in seen_labels]), \n",
    "                      title='train Confusion matrix, without normalization')\n",
    "\n",
    "plot_confusion_matrix(y_train , y_pred_train, classes=np.array([l.name for l in seen_labels]),  normalize=True,\n",
    "                      title='train Confusion matrix, without normalization')\n",
    "\n",
    "plot_confusion_matrix(y_test, y_pred_test, classes=np.array([l.name for l in seen_labels]), \n",
    "                      title='test Confusion matrix, without normalization')\n",
    "\n",
    "plot_confusion_matrix(y_test, y_pred_test, classes=np.array([l.name for l in seen_labels]),  normalize=True,\n",
    "                      title='test Confusion matrix, without normalization')"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {},
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "threshold": 4,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "1178.87px",
    "left": "0px",
    "right": "1238px",
    "top": "111.133px",
    "width": "212px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
