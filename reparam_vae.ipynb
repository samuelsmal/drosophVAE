{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-15T21:47:32.464024Z",
     "start_time": "2019-06-15T21:47:32.422506Z"
    }
   },
   "outputs": [],
   "source": [
    "from drosoph_vae.settings.config import SetupConfig\n",
    "# adapt according to your machine (0 should be fine, if you have a GPU)\n",
    "if SetupConfig.runs_on_lab_server():\n",
    "    %env CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
    "    %env CUDA_VISIBLE_DEVICES=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0TD5ZrvEMbhZ"
   },
   "source": [
    "# VAE using the reparametrization trick"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "e1_Y75QXJS6h"
   },
   "source": [
    "## Imports and enabling of eager execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-15T21:47:34.181587Z",
     "start_time": "2019-06-15T21:47:32.465137Z"
    },
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "YfIk2es3hJEd"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from collections import namedtuple\n",
    "from functools import partial\n",
    "import itertools\n",
    "from functional import seq\n",
    "from functools import reduce\n",
    "import warnings\n",
    "import os\n",
    "import traceback\n",
    "import time\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import gridspec\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import PIL\n",
    "import imageio\n",
    "from IPython import display\n",
    "from pathlib import Path\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from hdbscan import HDBSCAN\n",
    "\n",
    "from importlib import reload # for debugging and developing, optional\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib as tfc\n",
    "import tensorflow.contrib.eager as tfe\n",
    "import tensorflow.keras as tfk\n",
    "import tensorflow.keras.layers as tfkl\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "# for the KL-loss explosion problem\n",
    "tf.enable_eager_execution()\n",
    "# we currently handle them ourselves. but with this, it will throw an error before we can apply the fix\n",
    "tfe.seterr(inf_or_nan='raise')\n",
    "\n",
    "# otherwise TF will print soooo many warnings\n",
    "warnings.filterwarnings('ignore', '.*FutureWarning.*np.complexfloating.*')\n",
    "\n",
    "from drosoph_vae.helpers.tensorflow import _TF_DEFAULT_SESSION_CONFIG_\n",
    "import drosoph_vae.helpers.tensorflow as tf_helpers\n",
    "sess = tf.InteractiveSession(config=_TF_DEFAULT_SESSION_CONFIG_)\n",
    "tf.keras.backend.set_session(sess)\n",
    "\n",
    "from drosoph_vae import data_loading\n",
    "from drosoph_vae import settings\n",
    "from drosoph_vae import preprocessing\n",
    "from drosoph_vae.helpers import video, plots, misc, jupyter\n",
    "from drosoph_vae.helpers.misc import extract_args, chunks, foldl, if_last\n",
    "from drosoph_vae.helpers.jupyter import display_video\n",
    "from drosoph_vae.helpers.logging import enable_logging\n",
    "from drosoph_vae.helpers.tensorflow import to_tf_data\n",
    "from drosoph_vae.settings import config, skeleton\n",
    "from drosoph_vae.settings import data as SD\n",
    "from drosoph_vae.settings.config import RunConfig, SetupConfig\n",
    "from drosoph_vae.training import vae as vae_training\n",
    "from drosoph_vae.training import supervised as supervised_training\n",
    "from drosoph_vae.losses.normalized_mutual_information import normalized_mutual_information\n",
    "from drosoph_vae.losses.purity import purity\n",
    "from drosoph_vae.models.drosoph_vae_conv import DrosophVAEConv\n",
    "from drosoph_vae.models.drosoph_vae_skip_conv import DrosophVAESkipConv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-15T21:47:34.186258Z",
     "start_time": "2019-06-15T21:47:34.182943Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:95% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "jupyter.fix_layout()\n",
    "enable_logging()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-13T14:02:57.434212Z",
     "start_time": "2019-06-13T14:02:57.431753Z"
    }
   },
   "source": [
    "# Setup, loading of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-15T21:47:34.279799Z",
     "start_time": "2019-06-15T21:47:34.187463Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-15 23:47:34,202 - root - WARNING - preprocessing - this works only for the first legs!\n",
      "2019-06-15 23:47:34,208 - root - WARNING - preprocessing - this works only for the first legs!\n",
      "2019-06-15 23:47:34,211 - root - WARNING - preprocessing - this works only for the first legs!\n",
      "2019-06-15 23:47:34,214 - root - WARNING - preprocessing - this works only for the first legs!\n",
      "2019-06-15 23:47:34,217 - root - WARNING - preprocessing - this works only for the first legs!\n",
      "2019-06-15 23:47:34,221 - root - WARNING - preprocessing - this works only for the first legs!\n",
      "2019-06-15 23:47:34,224 - root - WARNING - preprocessing - this works only for the first legs!\n",
      "2019-06-15 23:47:34,228 - root - WARNING - preprocessing - this works only for the first legs!\n",
      "2019-06-15 23:47:34,232 - root - WARNING - preprocessing - this works only for the first legs!\n",
      "2019-06-15 23:47:34,235 - root - WARNING - preprocessing - this works only for the first legs!\n",
      "2019-06-15 23:47:34,239 - root - WARNING - preprocessing - this works only for the first legs!\n",
      "2019-06-15 23:47:34,242 - root - WARNING - preprocessing - this works only for the first legs!\n",
      "2019-06-15 23:47:34,245 - root - WARNING - preprocessing - this works only for the first legs!\n",
      "2019-06-15 23:47:34,248 - root - WARNING - preprocessing - this works only for the first legs!\n",
      "2019-06-15 23:47:34,251 - root - WARNING - preprocessing - this works only for the first legs!\n",
      "2019-06-15 23:47:34,254 - root - WARNING - preprocessing - this works only for the first legs!\n",
      "2019-06-15 23:47:34,257 - root - WARNING - preprocessing - this works only for the first legs!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huh?? something odd with Experiment(study_id='180921_aDN_CsCh', fly_id='Fly6', experiment_id='003_SG1'): /home/sam/proj/epfl/neural_clustering_data/experiments/180921_aDN_CsCh/Fly6/003_SG1/behData/images: [Errno 2] No such file or directory: '/home/sam/proj/epfl/neural_clustering_data/experiments/180921_aDN_CsCh/Fly6/003_SG1/behData/images'\n",
      "huh?? something odd with Experiment(study_id='180921_aDN_CsCh', fly_id='Fly3', experiment_id='001_SG1'): /home/sam/proj/epfl/neural_clustering_data/experiments/180921_aDN_CsCh/Fly3/001_SG1/behData/images: [Errno 2] No such file or directory: '/home/sam/proj/epfl/neural_clustering_data/experiments/180921_aDN_CsCh/Fly3/001_SG1/behData/images'\n",
      "huh?? something odd with Experiment(study_id='180921_aDN_CsCh', fly_id='Fly4', experiment_id='003_SG1'): /home/sam/proj/epfl/neural_clustering_data/experiments/180921_aDN_CsCh/Fly4/003_SG1/behData/images: [Errno 2] No such file or directory: '/home/sam/proj/epfl/neural_clustering_data/experiments/180921_aDN_CsCh/Fly4/003_SG1/behData/images'\n",
      "huh?? something odd with Experiment(study_id='180921_aDN_CsCh', fly_id='Fly6', experiment_id='001_SG1'): /home/sam/proj/epfl/neural_clustering_data/experiments/180921_aDN_CsCh/Fly6/001_SG1/behData/images: [Errno 2] No such file or directory: '/home/sam/proj/epfl/neural_clustering_data/experiments/180921_aDN_CsCh/Fly6/001_SG1/behData/images'\n"
     ]
    }
   ],
   "source": [
    "setup_cfg = SetupConfig()\n",
    "run_cfg = RunConfig()\n",
    "\n",
    "frame_data, frame_labels, normalisation_factors = data_loading.load_labelled_data(run_config=run_cfg, setup_config=setup_cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-15T21:47:34.283380Z",
     "start_time": "2019-06-15T21:47:34.281095Z"
    }
   },
   "outputs": [],
   "source": [
    "def to_int_value(frame_with_label):\n",
    "    return np.array([l.label.value for l in frame_with_label[:,1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-15T21:47:34.302302Z",
     "start_time": "2019-06-15T21:47:34.284948Z"
    }
   },
   "outputs": [],
   "source": [
    "if run_cfg['data_type'] == config.DataType.ANGLE_3D:\n",
    "    frame_data, frame_labels, selected_columns, normalisation_factors = preprocessing.preprocess_angle_3d_data(\n",
    "        frame_data, frame_labels, **run_cfg.preprocessing_parameters())\n",
    "if run_cfg['data_type'] == config.DataType.POS_2D:\n",
    "    selected_columns = None\n",
    "    # preprocessing for the pos_2d data happens inside the loading function, yeah... I know ugly\n",
    "    frame_data, frame_labels = preprocessing.preprocess_pos_2d_data(frame_data, frame_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-15T21:47:34.319082Z",
     "start_time": "2019-06-15T21:47:34.303764Z"
    }
   },
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "n_train_data_points = int(frame_data.shape[0] * run_cfg['train_test_ratio'])\n",
    "\n",
    "X_train = scaler.fit_transform(frame_data[:n_train_data_points])\n",
    "X_test = scaler.transform(frame_data[n_train_data_points:])\n",
    "y_train = to_int_value(frame_labels[:n_train_data_points])\n",
    "y_test = to_int_value(frame_labels[n_train_data_points:])\n",
    "frame_labels_train = frame_labels[:n_train_data_points]\n",
    "frame_labels_test = frame_labels[n_train_data_points:]\n",
    "\n",
    "if run_cfg['use_time_series']:\n",
    "    X_train, X_test, y_train, y_test, frame_labels_train, frame_labels_test = [misc.to_time_series_np(x, sequence_length=run_cfg['time_series_length']) \n",
    "                                        for x in (X_train, X_test, y_train, y_test, frame_labels_train, frame_labels_test)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-15T21:47:34.323045Z",
     "start_time": "2019-06-15T21:47:34.320571Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "##\n",
    "## debugging overwrite\n",
    "##\n",
    "#    \n",
    "#if run_config['debug']:\n",
    "#    if run_config['d_zero_data']:\n",
    "#        # resetting the scaler to make our life easier down below the pipeline\n",
    "#        _dummy_data_ = np.zeros_like(joint_positions)\n",
    "#    elif run_config['d_sinoid_data']:\n",
    "#        if run_config['data_type'] == _DATA_TYPE_2D_POS_:\n",
    "#            _dummy_data_ = np.zeros_like(joint_positions)\n",
    "#            for frame in range(_dummy_data_.shape[0]):\n",
    "#                for joint in range(_dummy_data_.shape[1]):\n",
    "#                    _dummy_data_[frame, joint, :] = np.sin(2 * np.pi * frame/_dummy_data_.shape[0] + joint / _dummy_data_.shape[1])\n",
    "#                \n",
    "#        else:\n",
    "#            _dummy_data_ = np.array([[np.sin(x) + (offset / joint_positions.shape[1]) \n",
    "#                                      for x in range(len(joint_positions))] \n",
    "#                                     for offset in range(joint_positions.shape[1])]).T.astype(joint_positions.dtype)\n",
    "#    elif run_config['d_sinoid_cluster_data']:\n",
    "#        if run_config['data_type'] == _DATA_TYPE_2D_POS_:\n",
    "#            raise NotImplementedError\n",
    "#        else:\n",
    "#            _dummy_data_ = np.zeros_like(joint_positions)\n",
    "#            _dummy_labels_ = np.zeros(joint_positions.shape[0])\n",
    "#            for c in range(_dummy_data_.shape[1]):\n",
    "#                _dummy_data_[:, c], _dummy_labels_ = dummy_data_complex_sine_like(_dummy_data_.shape[0])\n",
    "#            \n",
    "#    if run_config['data_type'] == _DATA_TYPE_2D_POS_:\n",
    "#        _dummy_data_ = misc.prep_2d_pos_data(_dummy_data_)\n",
    "#        \n",
    "#    if run_config['use_time_series']:\n",
    "#        reshaped_joint_position = scaler.fit_transform(_dummy_data_)\n",
    "#        reshaped_joint_position = misc.to_time_series_np(reshaped_joint_position, sequence_length=run_config['time_series_length'])\n",
    "#        labels = _dummy_labels_[run_config['time_series_length'] - 1:]\n",
    "#    else:\n",
    "#        reshaped_joint_position = _dummy_data_\n",
    "#        labels = _dummy_labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-15T21:47:34.337555Z",
     "start_time": "2019-06-15T21:47:34.324312Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'drosoph_vae.helpers.plots' from '/home/sam/proj/epfl/neural_clustering_vae/drosoph_vae/helpers/plots.py'>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reload(plots)\n",
    "#\n",
    "# Making sure that the train/test distributions are not too different from each other\n",
    "#\n",
    "    \n",
    "#if run_cfg['data_type'] == data_loading.DataType.ANGLE_3D:\n",
    "#    fig = plots.plot_3d_angle_data_distribution(X_train[_plt_data_idx_],\n",
    "#                                                X_test[_plt_data_idx_],\n",
    "#                                                selected_columns, \n",
    "#                                                exp_desc=run_cfg.description())\n",
    "#else:\n",
    "#    fig = plots.plot_2d_distribution(data_train[_plt_data_idx_], data_test[_plt_data_idx_], exp_desc=run_config.description())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "THY-sZMiQ4UV"
   },
   "source": [
    "# model def"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Sources:\n",
    "\n",
    "- https://blog.keras.io/building-autoencoders-in-keras.html (keras autoencoder implementation)\n",
    "- https://medium.com/the-artificial-impostor/notes-understanding-tensorflow-part-3-7f6633fcc7c7 (temporal block)\n",
    "- https://stackoverflow.com/questions/46503816/keras-conv1d-layer-parameters-filters-and-kernel-size (refresher on conv layers)\n",
    "- https://towardsdatascience.com/types-of-convolutions-in-deep-learning-717013397f4d (refresher on conv layers)\n",
    "- https://jeddy92.github.io/JEddy92.github.io/ts_seq2seq_conv/ (for a good overview over diluted causal convolutions)\n",
    "- https://blog.goodaudience.com/introduction-to-1d-convolutional-neural-networks-in-keras-for-time-sequences-3a7ff801a2cf?gi=c5cb3c007035 (general reference)\n",
    "- https://medium.com/tensorflow/variational-autoencoders-with-tensorflow-probability-layers-d06c658931b7 (VAE with tensorflow probability)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-10T13:12:53.469656Z",
     "start_time": "2019-05-10T13:12:53.444967Z"
    },
    "hidden": true
   },
   "source": [
    "### Generative Network\n",
    "This defines the generative model which takes a latent encoding as input, and outputs the parameters for a conditional distribution of the observation, i.e. $p(x|z)$. Additionally, we use a unit Gaussian prior $p(z)$ for the latent variable.\n",
    "\n",
    "### Inference Network\n",
    "This defines an approximate posterior distribution $q(z|x)$, which takes as input an observation and outputs a set of parameters for the conditional distribution of the latent representation. In this example, we simply model this distribution as a diagonal Gaussian. In this case, the inference network outputs the mean and log-variance parameters of a factorized Gaussian (log-variance instead of the variance directly is for numerical stability).\n",
    "\n",
    "### Reparameterization Trick\n",
    "During optimization, we can sample from $q(z|x)$ by first sampling from a unit Gaussian, and then multiplying by the standard deviation and adding the mean. This ensures the gradients could pass through the sample to the inference network parameters.\n",
    "\n",
    "### Network architecture\n",
    "For the inference network, we use two convolutional layers followed by a fully-connected layer. In the generative network, we mirror this architecture by using a fully-connected layer followed by three convolution transpose layers (a.k.a. deconvolutional layers in some contexts). Note, it's common practice to avoid using batch normalization when training VAEs, since the additional stochasticity due to using mini-batches may aggravate instability on top of the stochasticity from sampling.\n",
    "\n",
    "The dilated convolution between signal $f$ and kernel $k$ and dilution factor $l$ is defined as:\n",
    "\n",
    "$$\\left(k \\ast_{l} f\\right)_t = \\sum_{\\tau=-\\infty}^{\\infty} k_\\tau \\cdot f_{t - l\\tau}$$\n",
    "\n",
    "![](./figures/diluted_convolution.png)\n",
    "![](./figures/WaveNet_gif.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "VAEs train by maximizing the evidence lower bound (ELBO) on the marginal log-likelihood:\n",
    "\n",
    "$$\\log p(x) \\ge \\text{ELBO} = \\mathbb{E}_{q(z|x)}\\left[\\log \\frac{p(x, z)}{q(z|x)}\\right].$$\n",
    "\n",
    "In practice, we optimize the single sample Monte Carlo estimate of this expectation:\n",
    "\n",
    "$$\\log p(x| z) + \\log p(z) - \\log q(z|x),$$\n",
    "where $z$ is sampled from $q(z|x)$.\n",
    "\n",
    "**Note**: we could also analytically compute the KL term, but here we incorporate all three terms in the Monte Carlo estimator for simplicity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-15T21:47:34.462832Z",
     "start_time": "2019-06-15T21:47:34.338546Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f5e1c16a940>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY4AAAEKCAYAAAAFJbKyAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJzs3Xd8VGXWwPHfmfSEEEIvARKKICAgIkWKYgGsuK4Fe8e6ru/uurq6lrWsrnXVxYKKva+rAgqxIUiRqqCA9AChQyAJqVPO+8fcQICQTJKZTMr57md2Zp6597lnRr0n92lXVBVjjDEmUK5wB2CMMaZuscRhjDGmUixxGGOMqRRLHMYYYyrFEocxxphKscRhjDGmUixxGGOMqRRLHMYYYyrFEocxxphKiQx3AKHQvHlzTU1NDXcYxhhTpyxatGiXqraoaLt6mThSU1NZuHBhuMMwxpg6RUQ2BLKdNVUZY4ypFEscxhhjKsUShzHGmEqpl30cZXG73WRmZlJYWBjuUEwpsbGxpKSkEBUVFe5QjDEBajCJIzMzk8TERFJTUxGRcIdjAFVl9+7dZGZmkpaWFu5wjDEBajBNVYWFhTRr1sySRi0iIjRr1syuAo2pYxpM4gAsadRC9s/EmLqnQSUOY4ypzyavncynqz8N+XEscdSQjIwMevXqVaPHfOCBB3jyyScD2nbhwoXcdtttQT3+pZdeSrdu3ejVqxfXXHMNbrc7qPUbYw5QVV5e+jJfrPsi5McKWeIQkVgRmS8iS0RkmYj8wylPE5F5IrJaRD4UkWinPMZ5v8b5PLVUXX9zyleKyKhQxVxbeb3ekB+jf//+PPfcc0Gt89JLL+W3337jl19+oaCggFdffTWo9RtjDliRtYINORsYnTY65McK5RVHEXCyqvYB+gKjRWQQ8C/gGVXtCuwBrnW2vxbYo6pdgGec7RCRHsBYoCcwGnhBRCJCGHfIrVu3jmOPPZYFCxbg9Xq54447OP744+nduzcvv/wyAN9//z0jRozgkksu4ZhjjiEjI4Ojjz6a66+/np49ezJy5EgKCgoAWLt2LaNHj+a4445j2LBh/Pbbb+Ue/+OPP6ZXr1706dOH4cOH7z/eWWedBcAZZ5xB37596du3L0lJSbz55ptHjLM8Z5xxBiKCiDBgwAAyMzOr87MZY8oxLWMakRLJqR1ODfmxQjYcV1UV2Oe8jXIeCpwMXOKUvwk8ALwIjHFeA/wX+I/4e07HAB+oahGwXkTWAAOAuVWN7R+Tl7F8S05Vdy9Tj7aNuf/snhVut3LlSsaOHcvrr79O3759mTBhAklJSSxYsICioiKGDBnCyJEjAZg/fz6//voraWlpZGRksHr1at5//31eeeUVLrzwQj755BMuu+wyxo0bx0svvUTXrl2ZN28eN998M999990RY3jwwQdJT0+nXbt27N2797DPv/zySwAWLVrE1Vdfzbnnnstrr71WZpxpaWn07duXn3/++YjHc7vdvP322zz77LMV/j7GmMpTVdLXpzOo7SCaxDYJ+fFCOo/DuTJYBHQBxgNrgb2q6nE2yQTaOa/bAZsAVNUjItlAM6f8x1LVlt6nTtm5cydjxozhk08+oWdPf5L56quvWLp0Kf/9738ByM7OZvXq1URHRzNgwICD5jeUnKQBjjvuODIyMti3bx9z5szhggsu2L9dUVFRuXEMGTKEq666igsvvJDzzjuvzG127drF5ZdfzkcffURSUtIR40xLSys3aQDcfPPNDB8+nGHDhlXwCxljqmLprqVsydvCzX1vrpHjhTRxqKoX6CsiTYBPgaPL2sx5LmtcppZTfhARGQeMA+jQoUO5cQVyZRAKSUlJtG/fntmzZ+9PHKrK888/z6hRB3fdfP/99yQkJBxUFhMTs/91REQEBQUF+Hw+mjRpUuHJu7SXXnqJefPm8cUXX5R5teD1ehk7diz33Xff/g79I8VZkX/84x/s3LkzoKYtY0zVTFs/jShXFCd3OLlGjlcjo6pUdS/wPTAIaCIiJQkrBdjivM4E2gM4nycBWaXLy9in9DEmqGp/Ve3fokWFy8mHRXR0NJ999hlvvfUW7733HgCjRo3ixRdf3D/iaNWqVeTl5QVcZ+PGjUlLS+Pjjz8G/Cf4JUuWlLvP2rVrGThwIA8++CDNmzdn06ZNB31+11130bt3b8aOHbu/rCpxvvrqq6Snp/P+++/jctkAPmNCwac+vsr4iqHthpIYnVgjxwzlqKoWzpUGIhIHnAqsAKYD5zubXQl87rye5LzH+fw7p59kEjDWGXWVBnQF5ocq7lBLSEhgypQpPPPMM3z++edcd9119OjRg379+tGrVy9uuOEGPB5PxRWV8u677/Laa6/Rp08fevbsyeeff17u9nfccQfHHHMMvXr1Yvjw4fTp0+egz5988km++uqr/R3kkyZNKjfOkuazQ914441s376dwYMH07dvXx588MFKfS9jTMUWb1/MjoIdjE4N/WiqEuI/N4egYpHe+Du/I/AnqI9U9UER6QR8ADQFfgIuU9UiEYkF3gaOxX+lMVZV1zl13QNcA3iA21V1annH7t+/vx56I6cVK1Zw9NFltZSZcLN/NsZU3cM/Psznaz5nxkUziI+Kr1ZdIrJIVftXtF0oR1UtxZ8EDi1fh39U1KHlhcAFh5Y7nz0CPBLsGI0xpi7z+Dx8veFrhqcMr3bSqAxreDbGmDpq/rb5ZBVmcXra6TV6XEscxhhTR6VnpBMfGc/QdkNr9LiWOIwxpg5ye918s+EbTu5wMrGRsTV6bEscxhhTB83dOpec4pwaHU1VwhKHMcbUQdPWTyMxOpET2p5Q48e2xFFDGuKy6tdeey19+vShd+/enH/++ezbt6/inYwxFSryFvHdpu84tcOpREVE1fjxLXHUAXV1WfVnnnmGJUuWsHTpUjp06MB//vOfoNZvTEM1K3MWee68sDRTgSWOsGgoy6o3btwY8C+DUlBQYLeJNSZIpmZMJTkmmQFtDpsSVyNCushhrTX1Ltj2S3DrbH0MnP5YhZs1tGXVr776ar788kt69OjBU089FcgvaYwpR747n5mZMzm709lEusJzCm+YiSNMGuKy6q+//jper5c//OEPfPjhh1x99dUB/FLGmCOZmTmTAk9Bjdzp70gaZuII4MogFBrisuolsV500UU88cQTljiMqaap66fSIq4F/Vr2C1sM1sdRgxrSsuqqypo1a/a/njx5Mt27dw/4exljDpdbnMuszbMYmTqSCFf47qDdMK84wqhkWfXTTjuNhIQErrvuOjIyMujXrx+qSosWLfjss88qVee7777LTTfdxMMPP4zb7Wbs2LGHLZVe2h133MHq1atRVU455RT69OnDjBkz9n/+5JNP0rNnz/3NYg8++GC5cZZ11aKqXHnlleTk5KCq9OnThxdffLFS38sYc7Dpm6ZT7CsO22iqEiFbVj2cbFn1usX+2RgTmJu/uZk1e9eQ/vv0kIxSDHRZdWuqMsaYOiC7KJu5W+YyOnV02Ie2W+Iwxpg64JsN3+BRD6PSKj9AJdgscRhjTB0wLWMaHRI70KNpj3CHYonDGGNqu10Fu5i/bT6jUkeFvZkKLHEYY0yt982Gb/CpL6yT/kqzxGGMMbXc1PVT6ZzUma5NuoY7FMASR41piMuql/jDH/5Ao0aNQlK3MfXd9rzt/LTjJ0al1Y5mKrAJgHWC1+slIiK0s0T79+9P//4VDt+utIULF5a5kKIxJjBfbfgKRcM+6a80u+IIg4ayrHrJPo8//nh1fi5jGrRp66fRvWl30pLSKt64hoTsikNE2gNvAa0BHzBBVZ8VkQeA64GdzqZ3q+qXzj5/A64FvMBtqprulI8GngUigFdVtVqrFP5r/r/4Lav8k2tldW/anTsH3Fnhdg1pWfX//Oc/nHPOObRp0ybQn9EYU0pmbiZLdy3l9n63hzuUg4SyqcoD/FlVF4tIIrBIRL52PntGVQ9qfBeRHsBYoCfQFvhGRI5yPh4PnAZkAgtEZJKqLg9h7CHRkJZV37JlCx9//DHff/994D+QMeYg6RnpAIxKDf+kv9JCljhUdSuw1XmdKyIrgHbl7DIG+EBVi4D1IrIGKLm91RpVXQcgIh8421Y5cQRyZRAKDWlZ9Z9++ok1a9bQpUsXAPLz8+nSpcv+FXONMRVLz0jnmObHkJKYEu5QDlIjfRwikgocC8xzim4VkaUiMlFEkp2ydkDp9b0znbIjldc5DWlZ9TPPPJNt27aRkZFBRkYG8fHxljSMqYSM7AxWZK2oVZ3iJUI+qkpEGgGfALerao6IvAg8BKjz/BRwDVDWODOl7OR22JK+IjIOGAfQoUOH4AQfAg1lWXVjTPVMy5gGwMjUkWGO5HAhXVZdRKKAKUC6qj5dxuepwBRV7eV0jKOqjzqfpQMPOJs+oKqjnPKDtiuLLatet9g/G2MO97vPf0fj6Ma8efqbNXbMsC+rLv6ZKq8BK0onDREpPcTmd8CvzutJwFgRiRGRNKArMB9YAHQVkTQRicbfgT4pVHEbY0y4rd6zmjV719SaJUYOFcqmqiHA5cAvIlLSjnE3cLGI9MXf3JQB3ACgqstE5CP8nd4e4BZV9QKIyK1AOv7huBNVdVkI4zbGmLCaljENl7g4reNp4Q6lTKEcVTWLsvstvixnn0eAR8oo/7K8/SoRU62Zsm/86uMdKI2pDlVl2vppHN/6eJrHNQ93OGVqMDPHY2Nj2b17t52oahFVZffu3cTGxoY7FGNqjRVZK9iYu7FWjqYq0WDWqkpJSSEzM5OdO3dWvLGpMbGxsaSk1K4x6saE07T104iUSE7tcGq4QzmiBpM4oqKiDpqFbYwxtY2qkp6RzqC2g2gS2yTc4RxRg2mqMsaY2m7prqVsydvC6WmnhzuUclniMMaYWmLa+mlEuaIY0X5EuEMplyUOY4ypBbw+L+kZ6QxrN4zE6MRwh1MuSxzGGFMLLN6xmJ0FO2vtpL/SLHEYY0wtkJ6RTmxELCemnBjuUCpkicMYY8LM4/Pw9YavObH9icRHxYc7nApZ4jDGmDCbv20+WYVZtXrSX2mWOIwxJszSM9JJiEpgaLuh4Q4lIJY4jDEmjNxeN19v+JoR7UcQG1k3lt+pMHGISCsReU1Epjrve4jItaEPzRhj6r+5W+eSW5xb6yf9lRbIFccb+Jc0b+u8XwXcHqqAjDGmIZm6fiqNoxszuM3gcIcSsEASR3NV/QjwAaiqB/CGNCpjjGkAirxFTN80nVM6nEJURFS4wwlYIIkjT0Sa4dznW0QGAdkhjcoYYxqAWZmzyHPn1YlJf6UFsjrun/HfqrWziMwGWgAXhDQqY4xpAKZmTKVpbFMGtB4Q7lAqpcLEoaqLROREoBv+O/qtVFV3yCMzxph6LN+dz8zMmZzT+RwiXXXrDheBjKpaC1ynqstU9VdVdYvIlBqIzRhj6q0ZmTMo8BQwKnVUuEOptED6ONzACBF5XUSinbJ2IYzJGGPqvWnrp9EyriX9WvYLdyiVFkjiyFfVi4AVwA8i0hGno9wYY0zl5Rbn8sPmHxiZOpIIV0S4w6m0QBrWBEBVHxeRRfjndDQNaVTGGFOPTd80HbfPXSebqSCwxHFfyQtV/VZERgFXhi4kY4yp36atn0bbhLb0adEn3KFUyRGbqkSku/Nys4j0K3kAzYAKO8dFpL2ITBeRFSKyTET+6JQ3FZGvRWS185zslIuIPCcia0RkqXOskrqudLZfLSKWtIwxddbewr3M3TKXUamjEJHgVv7ji/DD06Ch7U0o74rjT8A44KkyPlPg5Arq9gB/VtXFIpIILBKRr4GrgG9V9TERuQu4C7gTOB3o6jwGAi8CA0WkKXA/0N857iIRmaSqewL8jsYYU2t8u/FbPOoJ/qS/gj0w/VFIHQrBTkiHOGLiUNVxznOV7pquqluBrc7rXBFZgX801hjgJGezN4Hv8SeOMcBbqqrAjyLSRETaONt+rapZAE7yGQ28X5W4jDEmnKZmTKVDYgeObnp0cCueOx6KsmHE3cGttwyBzOO4wLliQET+LiL/E5FjK3MQEUkFjgXmAa2cpFKSXFo6m7UDNpXaLdMpO1K5McbUKbsKdrFg2wJGp40ObjNV3m5/M1WPc6F1r+DVewSBDMe917liGAqMwn+V8FKgBxCRRsAnwO2qmlPepmWUaTnlhx5nnIgsFJGFO3fuDDQ8Y4ypMV9v+Bqf+oJ/p7/Z/4biPDjpb8Gt9wgCSRwlK+GeCbyoqp8D0eVsv5+IROFPGu+q6v+c4u1OExTO8w6nPBNoX2r3FGBLOeUHUdUJqtpfVfu3aNEikPCMMaZGTVs/jc5Jnema3DV4leZuh/mvwDEXQMvuFW8fBIEkjs0i8jJwIfCliMQEsp/4r8NeA1ao6tOlPprEgeG8VwKflyq/whldNQjIdpqy0oGRIpLsjMAa6ZQZY0ydsS1vGz/t+Cn4neKzngFvMZx0V3DrLUcg8zguxN8Z/aSq7nWuEu4IYL8hwOXALyLys1N2N/AY8JFzF8GNHFhp90vgDGANkA9cDaCqWSLyELDA2e7Bko5yY4ypK77K+ApFg9tMlb0ZFk6EvhdDs87Bq7cCgayOmw/8r9T7/aOlKthvFmX3TwCcUsb2CtxyhLomAhMrOqYxxtRW6RnpHN30aFKTUoNX6Q9Pgfpg+F+DV2cAAmmqMsYYUw2ZuZks3bU0uEuM7N0Ii9+CfpdDcsfg1RsASxzGGBNi6Rn+btmg9m/MeBzEBcP+Erw6A2SJwxhjQmxaxjR6N+9Nu0ZBmoK2ey38/B70vxqSan5aW3lrVeWKSM6RHjUZpDHG1FUZ2Rn8lvVbcJupZvwLIqJh6J+CV2cllLfkSMls8QeBbcDb+Du7LwUSayQ6Y4yp46ZlTEOQ4CWOnSth6Udwwq2Q2Co4dVZSIE1Vo1T1BVXNVdUcVX0R+H2oAzPGmPpg2vppHNvyWFolBOkk//2jEBUPQ24PTn1VENDMcRG5VEQiRMQlIpdyYDa5McaYI1i9ZzVrs9dyetrpwalw26+w7FMYdBMkNA9OnVUQSOK4BP8kwO3O4wKnzBhjTDmmrp+KS1yc2vHU4FT4/aMQk+RvpgqjQCYAZuBf8twYY0yAVJX0jHQGtB5A87ggXB1s+Ql+mwIn3Q1xydWvrxqOmDhE5HnKWIW2hKreFpKIjDGmHlietZyNuRu5ptc1wanwu0f8CWPQTcGprxrKu+JYWGNRGGNMPZO+Pp1IiQxOM9Wm+bDmazjlfohtXP36qqm84bhvln4vIgmqmhf6kIwxpm5TVaZlTGNw28EkxSRVv8LvHob45jBgXPXrCoJAlkcfLCLLgRXO+z4i8kLIIzPGmDpqyc4lbM3bGpwlRjJmwfoZMPT/IKZR9esLgkBGVf0b/53/dgOo6hJgeCiDMsaYuiw9I51oVzQj2o+oXkWq/r6NRq3h+GuDE1wQBLRWlapuOqTI5nEYY0wZvD4v6RnpDG03lMToai6ysW46bJwDw/8CUXHBCTAIArmR0yYROQFQEYkGbsNptjLGGHOwxTsWs7NgZ/Un/ZVcbTROgX5XBCe4IAnkiuNG/DdYaof//t99OcINl4wxpqFLz0gnLjKO4SnVbNFflQ6bF8KJd0BkTHCCC5JAJgDuwr+woTHGmHJ4fB6+3vA1w1OGEx8VX/WKVGH6I5CcCn1r3+m3vAmAf1XVx480EdAmABpjzMHmb5tPVmEWp6dWs5lqxWTYthTOfREiooITXBCVd8Wx3Hm2iYDGGBOAaeunkRCVwNCUoVWvxOfzr0nVrAscc2Hwggui8hLHRcAUoImqPltD8RhjTJ3k9rr5ZuM3nNz+ZGIiqtEnsex/sGM5/P41iAhk/FLNK69z/DgR6QhcIyLJItK09KOmAjTGmLpgzpY55BbnVm/Sn9cD3z8GLXtAz/OCF1yQlZfOXgKmAZ2ARfjv/ldCnXJjjDH47/TXOLoxg9sMrnolv3wMu1fDhW+DK6BpdmFxxMhU9TlVPRqYqKqdVDWt1MOShjHGOAo9hUzfNJ1TO55KVFU7s71umPEYtO4NR58d3ACDrMKUpqpVWsNXRCaKyA4R+bVU2QMisllEfnYeZ5T67G8iskZEVorIqFLlo52yNSJyV1ViMcaYUJq1eRZ57rzq3Vf853dhTwaMuAdEKtw8nEJ5LfQGUFZj3zOq2td5fAkgIj2AsUBPZ58XnFvVRgDjgdOBHsDFzrbGGFMrFHmLGP/zeFrFt2JA6wFVq8RTBDOegHb94ahqJJ8aErIue1WdKSKpAW4+BvhAVYuA9SKyBij5J7BGVdcBiMgHzrbLy67GGGNq1vifxrNm7xpeOOUFIl1VPKUufgtyMuGc52r91QYEeMUhIh1F5FTndZyIVGflrltFZKnTlFVy/8N2QOmFFDOdsiOVlxXjOBFZKCILd+7cWY3wjDEmMIu3L+aNZW9w/lHnMyxlWNUqcRfAzCehw2DofHJwAwyRQO7HcT3wX+BlpygF+KyKx3sR6Ix/vautwFMlhyljWy2n/PBC1Qmq2l9V+7do0aKK4RljTGDy3fncM+se2jZqy1/6/6XqFS2cCPu2wcl/rxNXGxBYU9Ut+JuN5gGo6moRaVmVg6nq9pLXIvIK/gmG4L+SaF9q0xRgi/P6SOXGGBM2Ty96ms37NjNx1EQSohKqVknRPpj1DKSdCKnVmG1ewwJpqipS1eKSNyISyRH+6q+IiLQp9fZ3QMmIq0nAWBGJEZE0oCswH1gAdBWRNGdJ97HOtsYYEzZzNs/hw5UfcnmPy+nfun/VK5o/AfJ2+q826pBArjhmiMjdQJyInAbcDEyuaCcReR84CWguIpnA/cBJItIXf+LJAG4AUNVlIvIR/k5vD3CLqnqdem4F0oEI/HNKllXqGxpjTBBlF2Vz75x76ZTUidv6VWOt18IcmPMcdDkN2ldxNFaYBJI47gKuBX7Bf6L/Eni1op1U9eIyil8rZ/tHgEfKKP/SOaYxxoTdY/MfY3fBbp4b8Vz11qT68UUo2AMj7g5ecDUkkMQxBnhLVV8JdTDGGFObfbPhG6asm8JNfW6iZ/OeVa+oYA/MHQ/dzoR2/YIXYA0JpI/jHGCViLwtImc6fRzGGNOg7CrYxYNzH+Topkdzfe/rq1fZnP9AUXadvNqAwJYcuRroAnwMXAKsFZEKm6qMMaa+UFUemvsQee48/jn0n0S5qnFzpbzdMO8l6HEutO4VvCBrUEBXD6rqFpGp+Du14/A3X10XysCMMaa2mLxuMt9t+o4/H/dnuiR3qV5ls/8N7vw6e7UBgU0AHC0ibwBrgPPxd4y3KXcnY4ypJ7blbePReY/Sr2U/Lu9xefUqy90O81+BYy6AFt2CE2AYBHLFcRXwAXCDs5aUMcY0CD71ce/se/Gql4eHPkyEK6J6Fc56GrzFcOKdwQkwTCpMHKo6tiYCMcaY2ubDlR/y49YfuXfQvbRPbF/xDuXJ3uxfXqTvxdCsc3ACDJMjJg4RmaWqQ0Ukl4Nnigugqto45NEZY0yYbMjZwNMLn2ZI2yFccNQF1a/whydBFYb/tfp1hdkRE4eqDnWeq7MSrjHG1Dlen5d7Zt1DVEQU/zjhH0h1Fx/cswEWvw39LofkjsEJMowC6Rx/O5AyY4ypL95Y9gZLdi7h7oF30yqhVfUrnPk4iAuGVWMV3VokkAmAB02PdCYAHheacIwxJrxW7VnF+J/Hc1rH0zgz7czqV7h7Lfz8PvS/BpLKvJ1QnXPExOHcAzwX6C0iOSKS67zfDnxeYxEaY0wNcXvd3P3D3SRGJ/L3QX+vfhMVwIx/QUQ0DP2/6tdVSxwxcajqo07/xhOq2lhVE51HM1X9Ww3GaIwxNeLFJS+ycs9KHhj8AE1jm1a/wh2/wdKPYMD1kBiEJq9aIpB5HHeLyHnAUPyjq35Q1areAdAYY2qlpTuX8tqvr3FO53MY0WFEcCr9/lGIToAhtwenvloikD6O8cCN+JdV/xW4UUTGhzQqY4ypQQWeAu6ZdQ8t41ty14C7glPptl9g+Wcw8EZIaBacOmuJQK44TgR6qaoCiMib+JOIMcbUC88tfo6MnAxeGfkKidFBmoEw/VGISYITbg1OfbVIIFccK4EOpd63B5aGJhxjjKlZ87fO550V73Bx94sZ1GZQcCrdvBhWfgGDb4G45ODUWYsEcsXRDFghIvOd98cDc0VkEoCqnhOq4IwxJpT2Fe/j3tn30rFxR/7vuCCOepr+T3/CGHRT8OqsRQJJHPeFPApjjAmDxxc8zrb8bbw5+k3iIuOCU+nGebDmazj1AYitnyszBbLI4QwR6Qh0VdVvRCQOiFTV3NCHZ4wxofH9pu/5dM2nXNvrWvq27Bu8iqc/DAktYMC44NVZywSy5Mj1wH+Bl52iFMCG4xpj6qw9hXt4YM4DHJV8FDf3vTl4Fa//AdbP9E/2i04IXr21TCCd47cAQ4AcAFVdDbQMZVDGGBNKj8x7hOzibP459J9ER0QHp1JVmP4IJLbxLy9SjwWSOIpUtbjkjbNWlZazvTHG1FpT108lPSOdm/vcTLemQbwL39rvYONcGPZniApSf0ktFUjimCEidwNxInIa8DEwuaKdRGSiiOwQkV9LlTUVka9FZLXznOyUi4g8JyJrRGSpiPQrtc+VzvarReTKyn9FY4zx25G/g4d/fJjeLXpzda+rg1dxydVG4xTod0Xw6q1SKKH/uz6QxHEXsBP/pL8bgC+Bvwew3xvA6DLq+lZVuwLfOu8BTge6Oo9xwIvgTzTA/cBAYABwf0myMcaYylBV7p9zP8XeYh4Z8giRrkAGlQZoxWTYvAhOvAMiY4JXbyWoKk99tZJ/TF4e8uQRSOKIAyaq6gWqej4w0Skrl6rOBLIOKR4DvOm8fhM4t1T5W+r3I9BERNoAo4CvVTVLVfcAX3N4MjLGmAp9svoTZm2exe3H3U5qUmrwKl73PfxvHLTsCX0vDV69laCq/GvaSp7/bg35xR5CfdERSOL4loMTRRzwTRWP10pVtwI4zyWd7O2ATaW2y3TKjlRujDEBy8zN5IkFTzCg9QAu7n5x8Cpe/Q28dxE0TYMrPoOIqODVHSBV5eEvVvDSjLVcOrADj53XG5fIyjCnAAAgAElEQVQrCMvBlyOQxBGrqvtK3jiv44McR1nfUsspP7wCkXEislBEFu7cuTOowRlj6i6f+vj7bP+9NR4a8hAuCeS0F4CV0+CDi6F5V7hyCjSq+cGmPp9y/6RlvDZrPVedkMrD5/YKedKAwBJH3iGd1ccBBVU83nanCQrneYdTnol/DawSKcCWcsoPo6oTVLW/qvZv0aJFFcMzxtQ37yx/h0XbF3Hn8XfStlHb4FS6YjJ8eBm06glXTArL6rc+n3LPZ7/w1twNXD8sjfvP7hGcG08FIJDEcTvwsYj8ICI/AB8CVV3ucRJQMjLqSg7cSXAScIUzumoQkO00ZaUDI0Uk2ekUH+mUGWNMhdbtXcezi5/lpJSTOLfLuRXvEIhf/wcfXQlt+8IVn0N8EG74VElen/LXT5by/vxN3DKiM3efcXSNJQ0IbMmRBSLSHeiGv+noN1V1V7SfiLwPnAQ0F5FM/KOjHgM+EpFrgY3ABc7mXwJnAGuAfOBq59hZIvIQsMDZ7kFVPbTD3RhjDuP2ubl71t3ER8Vz/wn3B+fEuuRD+OxGaD8QLv0YYoK0BHsleLw+/vzxEj7/eQu3n9qVP57StUaTBgSQOEQkHvgT0FFVrxeRriLSTVWnlLefqh6pB+qUMrZV/DPUy6pnIv6RXMYYE7BXf3mVZbuX8eSJT9I8rnn1K/zpXfj8FkgdCpd8GJYlRdxeH7d/8DNf/LKVO0Z145YRXWo8Bgisqep1oBgY7LzPBB4OWUTGGFNNy3YvY8KSCZyedjqjUkdVv8KFr8PnN0Onk+CSj8KSNIo9Pm55dzFf/LKVe844OmxJAwJLHJ1V9XHADaCqBZQ92skYY8KuyFvEPT/cQ3JsMvcMvKf6Fc6bAFNuh64j4eIPIDrYg0orVuj2cuM7i/hq+XYeOLsH1w/vVOMxlBbI1MliZyn1klvHdgaKQhqVMcZU0fifxrM2ey0vnPICSTFJ1ats7nhIvxu6nQkXvB6WWeGFbi/j3l7EzFU7efjcXlw2qGONx3CoQBLH/cA0oL2IvIt/pdyrQhmUMcZUxeLti3lj2Rucf9T5DEsZVr3KZj0D3zwAPcbA718Ly+S+/GIP1725kLnrdvP473tz4fHtK96pBpSbOMTfVf8bcB4wCH8T1R9VdVcNxGaMMQHLd+dzz6x7aNuoLX/p/5fqVTbjcf+ihb3Oh9+9DBFBXNcqQPuKPFzz+gIWbsjiqQv6cF6/lBqP4UjK/TVUVUXkM1U9DviihmIyxphKe2rhU2zet5mJoyaSEFXFzuuSVW5nPgF9LoYx48EVEdxAA5BT6OaqifNZkpnNv8ceyzl9gjRxMUgC6Rz/UUSOD3kkxhhTRbM3z+ajVR9xeY/L6d+6f9UqUYVv7vcnjX5XwJgXwpI0svPdXP7afJZmZvOfi2tf0oDA+jhGADeIyAYgD39zlapq75BGZowxAcguyua+2ffRKakTt/W7rWqVqPo7wX98AfpfC2c8Ca4grWlVCXvyirnstXms3r6PFy87jtN6tKrxGAIRSOI4PeRRGGNMFT02/zF2F+7muZOfIyaiCqOefD6YegcseBUG3gSjH4UanokNsGtfEZe9Oo91u/J4+YrjGNGt9t6hO5AlRzbURCDGGFMZXp+Xl5e+zJR1U7ipz030bN6z8pX4fDDlj7D4LTjhNjjtwbAkjR25hVz6yjw27cln4pXHM7RrEGa6h1DNDxUwxphq2pa3jb/98DcWbl/ImZ3O5Pre11e+Ep8XPr8VlrwHw++AEfeEJWlsyy7kkld+ZFtOIa9fNYDBnWt+pd3KssRhjKlTZmyawd9n/50ibxEPDXmIMZ3HVH6RP6/Hv1jhLx/7E8aJfw1NsBXYvLeAS175kd37innrmgH0T635lXarwhKHMaZOKPYW88yiZ3hnxTt0b9qdx4c/TlpSWuUr8rrhk2th+edwyv0w7E/BDzYAm7LyufiVH8kucPPWtQPo1yE5LHFUhSUOY0ytl5GdwV9n/pUVWSu4pPsl/Kn/n6rWEe4pgo+vhpVfwMhH4ISq3lqoejJ25XHJKz+SV+zl3esG0julSVjiqCpLHMaYWm3S2kk8/OPDREdE8+yIZzm5w8lVq8hdCB9dAavT4fQnYOC44AYaoLU793HJKz/i9irvXT+Qnm2ruZ5WGFjiMMbUSnnuPB7+8WGmrJvCca2O47Fhj9E6oXXVKivOhw8vhbXfwVn/hv5XBzfYAK3ansslr8wDlPevH0S31jV/I6hgsMRhjKl1lu1exl9n/JXMfZnc3OdmxvUeR0RVZ3EX58F7F0HGLP8SIsdeFtxgA7R8Sw6XvTaPSJfw3vWD6dKyUVjiCAZLHMaYWkNVeWfFOzy96Gmaxjbl1ZGvcnzraqx4VJQL714Im370L1bY56LgBVsJv27O5rLX5hEXFcF71w8irXnN3wgqmCxxGGNqhazCLO6dfS8zM2dyUspJPDTkIZrEVqPTuDAb3jkfNi/yL4ve67zgBVsJP2/ayxWvzSMxNooPxg2ifdOavxFUsFniMMaE3fyt87nrh7vYW7SXuwbcxSXdL6n83IzSCvbA2+fBtl/gwjfh6LODF2wlLNqQxZUTF9A0IZr3rh9ISnLdTxpgicMYE0Yen4eXlrzEhKUT6Ni4Iy+c+gLdm3avXqV5u+HtMbBzJVz0DnQbHZxgK2neut1c/cYCWjWO5b3rB9ImKS4scYSCJQ5jTFhs3beVu364i8U7FjOm8xjuHng38VHV/It83054awxkrYWL34cupwYn2EqavWYX1765gJTkeN67biAtG8eGJY5QscRhjKlx3274lvvm3IfH5+HRYY9yVqezql9p7jZ48xzYuxEu+RA6nVT9OqtgxqqdjHtrIWnNE3jnuoE0b1Tz9ykPtbAkDhHJAHIBL+BR1f4i0hT4EEgFMoALVXWPc/vaZ4EzgHzgKlVdHI64jTHVU+Qt4okFT/Dhyg/p0awHTwx/gg6NO1S/4uzN8ObZ/uRx2X8hdWj166yCb1ds56Z3FtOlZSPeuW4gTROiwxJHqNX8nUoOGKGqfVW15HZddwHfqmpX4FvnPfjvB9LVeYwDXqzxSI0x1bZu7zou+eISPlz5IVf0uIJ3Tn8nOElj70Z44wzYtwMu/1/Yksa0X7dx4zuL6N4mkfeur79JA2pXU9UY4CTn9ZvA98CdTvlbqqr4b2PbRETaqOrWsERpjKkUVeXTNZ/y2PzHiI2IZfwp4xmeMjw4lW+YA/+7wT/09orPIeW44NRbCXlFHt6au4Env1pJn5Qk3rhmAI1jo2o8jpoUrsShwFciosDLqjoBaFWSDFR1q4iU3P6qHbCp1L6ZTpklDmNqudziXB6a+xBTM6YysPVA/jnsn7SMr+ad7VRh/QyY8QRsmAWNWsGVk6Bt3+AEHaCcQjdvzcngtVnr2ZPv5pTuLXn24mNpFFOb/h4PjXB9wyGqusVJDl+LyG/lbFvWYG49bCORcfibsujQIQiXv8aYavll5y/cMfMOtuVt47Zjb+OaXtdUfdkQ8CeMNd/AjMchcz4ktoHRj0G/KyG65uZH7M0vZuLsDN6YvZ6cQg8jurXg1pO7clzHurMsenWFJXGo6hbneYeIfAoMALaXNEGJSBtgh7N5JtC+1O4pwJYy6pwATADo37//YYnFGFMzfOrjzWVv8tzi52gR34I3Rr9B35bVuBrw+WDVVJj5BGz5CZLaw5lPQd/LIKrmhrnu3lfEq7PW8/bcDewr8jCyRyv+cHJXjkmpe6vbVleNJw4RSQBcqprrvB4JPAhMAq4EHnOeP3d2mQTcKiIfAAOBbOvfMKZ22lWwi7/P+juzt8zmtI6ncf/g+0mKqeKJ1eeDFZ/DzCdh+6+QnArnPA+9x0JkzXU878gpZMLMdbw7byOFHi9nHNOGW0d04eg2jWsshtomHFccrYBPneUEIoH3VHWaiCwAPhKRa4GNwAXO9l/iH4q7Bv9w3PCsh2yMKdecLXO4+4e7yS3O5d5B93LBURdUbdkQrwd+/QR+eAp2rYRmXf0LFPY6HyJq7pS1ZW8BL89Yy/sLNuH1KWP6tOXmEV3q9Kq2wVLjiUNV1wF9yijfDZxSRrkCt9RAaMaYKnD73Iz/aTwTf51IWlIaE0ZO4KjkoypfkdcNSz6AWU9D1jpo2QPOnwg9zoXq9I1U0qasfF74fi3/XbQJVfh9vxRuHtGZjs3q9oq2wVT/u/+NMSGTmZvJnT/cydKdS/l9199z54A7iYus5JpMniL46R2Y9W/I3giteztrTJ0JrpqbarZ+Vx7jp6/h0582EyHCRce358YTO9ebhQmDyRKHMabSVJX0jHQenPsgivLEiU8wOrWSiwm6C2DRmzD7WcjdAu36w5lPQteRUJ2VcStp9fZc/jN9DZOXbCEqwsUVgztyw/DOtE6qX+tLBZMlDmNMwDblbmLK2ilMXjeZTbmb6N28N/8a/i9SElMCr6RoHyycCHOeh7wd0OEEOHc8dBpRowlj+ZYc/jN9NVN/3UZcVATXD+vEdcM60SKx/q0tFWyWOIwx5copziE9I50pa6eweMdiBGFA6wHc0PsGzuh0BlGuAGdJF+bA/AkwdzwUZEHaiXDi6zW+RMiSTXt5/rs1fLNiO4kxkdxyUheuGZpWr5cICTZLHMaYw7h9buZsnsOktZP4ftP3FPuK6ZTUiT/2+yNndTqL1gmtA6+sYA/8+BLMe9G/NEjXkTD8Dmg/IHRfoAwLM7J47rs1zFy1k6S4KP7v1KO4akgqSXH1e3mQULDEYYwB/P0Wy3cvZ/K6yUxdP5WswiySY5K5oNsFnN3pbHo061G54bV5u/xXF/NfgeJc6H4WDP8LtD02dF/iEKrK3HW7ef7bNcxdt5tmCdHcObo7lw3qQGI9X08qlCxxGNPAbcvbxpR1U5i8djLrstcR5YpiRPsRnN35bIa0GxJ4U1SJ3G3+/ouFE/0d4D3PhWF/gda9QvMFyqCqzFy9i+e/Xc3CDXtokRjD3888mksGdiA+2k571WW/oDENUJ47j282fMPktZOZv20+itKvZT/uG3wfIzuOrNps7+zNMPvf/pFSPjcccwEM+zO06Bb8L3AEqsq3K3bw/PQ1LNm0l7ZJsTw4picX9m9PbFTNzQWp7yxxGNNAeH1e5m2dx+R1k/l247cUeApon9iem/rcxFmdzqJ94/YVV1KWPRkw6xn46V1Aoc9YGPonaNY5mOGXy+dT0pdt4/nv1rB8aw7tm8bx6HnH8Pt+KURHhvO2Q/WTJQ5j6rlVe1YxZe0Uvlj3BTsKdpAYnchZnc7inM7n0KdFn6otCwKwe61/WZAlH/hndve7HIbcDskdg/sFylFQ7OWr5dsYP30Nq7bvo1PzBJ68oA9j+rYlKsISRqhY4jCmHtpVsIsv133J5HWT+S3rNyIlkqEpQ7mr810MTxlOTEQV5ipkb4YNs/2PjNmwezVExsKA62HIH6Fx2+B/kUPkFrpZtGEP89ZnMX99Fksz9+L2Kke1asSzY/tyVu+2RLhqbi5IQ2WJw5h6otBTyPRN05m0dhJzt8zFq156NevF3wb8jdFpo2ka2zTwylT9TVAbZvvvspcxC/Zu8H8W0xg6DPZfYfQeC4mtQvJ9wH/vi/lOkpi3PotlW7LxKUS6hN4pSVw7tBMndG7G0C7NcVnCqDGWOIypw3zqY9H2RUxeO5mvN3zNPvc+Wie05upeV3N2p7Pp1KRTYBWpwq5VpRLFbP8yIABxTaHjCTDwRkgdAq16hWzRwZ25RU6i2M289Vn8ti0XgOhIF8e2b8KtJ3dlYFpTju3QxEZHhZH98sbUQRnZGUxeN5kpa6ewJW8L8ZHxnNbxNM7pfA79W/fHJRW07/t8sGPZgauJDXMgf5f/s0atoOMQf5LoOASadwvZYoNbswuYt85/NTFv/W7W7cwDID46guM6JnNW7zYMSGtGn/ZJxETaqKjawhKHMbVcTnEOK7NWsmrPKlbtWcXy3cv5Les3XOJicJvB3NbvNk7ucHL5q9J6PbBtyYGriY1z/LO4AZI6QJdTDySKpp1CsmaUqrIxK39//8S89bvZlFUAQGJsJMenNuWi/u0ZkNaUXu2SrHO7FrPEYUwt4VMfm3I3sTJrJSv3rGRV1ipW7lnJ1rwDN7xMjknmqKZH8efj/syZnc6kRXyLsivzFMOWxQc6sjfNg+J9/s+adoajz/GvEdXxBGjSISTfR1VZu3Of/2pinT9ZbMsp9IeQEM2A1KZcfUIaAzs1pXvrxtapXYdY4jAmDPLceazas+qgJLF672oKPP6/wCMkgtTGqfRt2ZcLky+kW3I3ujXtRou4FmUPn3UXQOaCA01PmQvA4z9J0+Jo6H2R/4qiwwnQuE1IvpPPp/y2LXd//8T89VnszisGoGViDAM7NWNAWlMGpTWlS8tGVR8GbMLOEocxIeRTH5v3bd5/9VDS5JS5L3P/NonRiXRL7sZ5Xc+jW3I3jmp6FF2adCl/yGxRrv8qoqTpafMi/2xtBFofA8dd7SSKwZDQPCTfzeP1sWxLzv5mp/nrs8gp9ACQkhzHid1aMCjNnyw6Nou3RFGPWOIwJkjy3fms3rv6oP6IVXtWkef2d/gKQsfGHenRrAe/6/q7/VcRreJbHX5SVYX8LNi7EbI3wd5NB17v2QA7loN6QSL8iwYOusnf9NR+IMQ1Ccr38fmUnfuKyNxTQOaefDbvLWDznoL9z5v25FPo9gHQqXkCZxzThoGdmjIgrRntmlTyLoCmTrHEYUwlqSpb87Ye3NS0ZxUbczaiKACNohpxVPJRnN3pbLo17Ua35G50Se5yoAPb5/PfxChrE6yf7U8KezcdSBLZmw70SZSISoAm7SGpPRw1yn9FkTIAYhpV6Xu4vT62ZReSWSoZbN7rTxCZewrYureQYq/voH2axEfRrkkcnVokMPyoFhzboQkDUpvSsrHdLa8hscRhTCmqSr4nn10Fu9hdsNv/XLh7/+uMnAxW7VlFbnHu/n3aJ7anW3I3zux05v6riLaxLZF9Ww8kga1TD756yM4Eb9HBB49t4k8MTTtBp5MOJIkm7f0jn+KbVmq0U6HbyxYnCZS+Wsjck8/mPQVsyynEpwfv0yIxhpTkOI5pl8ToXq1JaRJHSnI87ZLjaNckjoQYO2UYSxymgch35/tP/oX+hFD6denksLtgN4XewsP2d4mL5JhkUhJTOD31dLoldeKo6CZ09bpI2LfDnxDW/gzZk/0JImeLvymptISW/iTQpjd0P9M/mml/YmgPsY0r9Z1yC92HNR9l7ikg03m9a9/BiSnCJbRuHEu75DgGdWq2PxmUJIY2SbG2gqwJiCUOU2cVeArKPPEfepWwu3D3/tFKpQlCcmwyzWKa0CyqMR0SU2nWpDvNJZpmRNBMhWZeL809bpoUFRJRlA2bt8Ly+bBv+yGVuSCxrT8JdDzh4ITQpCMktYOow9v9izxecgs95OS6yd25l5xCNzkFHnIL3Ye8dp4LPGQXuNmWU0h2gfuguqIjXPuTwSndW9IuOY4U53275DhaN44l0uZGmCCoM4lDREYDzwIRwKuq+liYQzLV4PF5KPAUkO/O9z978g97ne/J379NTnEOWYVZBzUh5Xvyy6w7OTKeZhFxNHPF0JtImkW2pLlLaeZx09xdTLPifJoX7KNJQQ6RxRsqDja6EcQm+ddoatQCup4GSR3QpBQKE9qRG9uGPREtyHVz8Mk+x0POdjc5hQXkFK7wJ4gCf0IoeV3k8ZV7aJdAYmwUibGRNI6NonFcJB2axXN8WjLtmsT7E0NyHClN4mjeKMbWazI1ok4kDhGJAMYDpwGZwAIRmaSqy8MbWf3nU1+ZJ/jSJ/VDX+cX51Hg3ke+O48Cdx4F7nzyPQX+h7eQAm8RRT53xQd3uIBEImiGi+Y+6On10sxdTLPiAn8i8Hr9VwZeH8leL6XvV6cSgTe6MZ6oRDzRiRRHJlIc2Zy8pEbsTk6gICKRAlcCeeJ/7JMEcjSeXOLJ0XiyfXEUeKHI46PI4yMv258Mcgs95BZ68PrygDXO43AxkS4SnRN+YmwUjWMjaZccR2MnESTGRtI4Luqg1weSRBQJ0RE2jNXUOnUicQADgDWqug5ARD4AxgC1KnH41Lf/4VXvgWefD6/Pg8/ndZ49eNXtf/Z50JJy9eD1eZ1yL25vIR5vMW5vEW5vMW5PEcXeIoq9xbg9xRR7i/F4iyn2FeP2uSn2unF73bh9bjw+D26fG7d6cPv8D4969z971It7/7MPDyXPPjzqw40PD4oHrfiLO0SVeFXifEq8+oh3nhN9SitV4nw+4lT3lx+8nf/zeOfzKCKI0kgifZGIRFNMDPskgVwSyCWObI0nW+PZ4IvjZ18ce7z+shxN2H/SzyWefGKgILATr0sgJjKCmCgXMZEuoiM9xETmExNZ8t5F68axHNUqkcaxkYckhIOTQ8lVgvUZmPqoriSOdsCmUu8zgYHBPsjGrau4ccp5+AR8gML+175Sr72ly0vKasFfhZGqRKkSqRCFOu8hSpUo/J+VvI/D2c7ZJ0KFCPX/C+F/LQfKfBFEaQQuXwQRvkgifJGILxLRKMQXjToPr8bgJooijaKIKIqIpogoijWSIqLZJ1G4XdF4JBqvKxqvRONxxeBxReF1xeJzReGNiEFdUbgiIoiMECJcLiJdQqRLiImKOOgkHhPpvI9y0THywGcxJZ9FuYiOcDmJIGJ/eXTJNqXKoyP9x7G/7o2pWF1JHGX913zQn8IiMg4YB9ChQ9XW3omOiqO1JiDqwqX+zlNBcDkP//+Dq9S7/f8vLkT9J54Dn5f67LDXzkMOPJf8r6QswhWFi0giXdG4JAqXK4oIiSZCoohwxeCKiCLCFU2EKxqXKxpxRYMrEnFFgCsCdUU67/3P6orY/15ckWhEBOKKwuWKwCX4YxdwieBylbz3n7QjXKWfXf7niIPLIw79/JD97KRsTP1QVxJHJlD6hsgpwJbSG6jqBGACQP/+/QNvXymldfP2TBw3r6oxGmNMg1BXxuYtALqKSJqIRANjgUlhjskYYxqkOnHFoaoeEbkVSMc/HHeiqi4Lc1jGGNMg1YnEAaCqXwJfhjsOY4xp6OpKU5UxxphawhKHMcaYSrHEYYwxplIscRhjjKkUSxzGGGMqRVSrNFeuVhORnUAAy54eUXNgV5DCqevstziY/R4Hs9/jgPrwW3RU1RYVbVQvE0d1ichCVe0f7jhqA/stDma/x8Hs9zigIf0W1lRljDGmUixxGGOMqRRLHGWbEO4AahH7LQ5mv8fB7Pc4oMH8FtbHYYwxplLsisMYY0ylWOIoRURGi8hKEVkjIneFO55wEpH2IjJdRFaIyDIR+WO4Ywo3EYkQkZ9EZEq4Ywk3EWkiIv8Vkd+cf0cGhzumcBKR/3P+O/lVRN4XkdhwxxRKljgcIhIBjAdOB3oAF4tIj/BGFVYe4M+qejQwCLilgf8eAH8EVoQ7iFriWWCaqnYH+tCAfxcRaQfcBvRX1V74b/0wNrxRhZYljgMGAGtUdZ2qFgMfAGPCHFPYqOpWVV3svM7Ff2JoF96owkdEUoAzgVfDHUu4iUhjYDjwGoCqFqvq3vBGFXaRQJyIRALxHHKH0vrGEscB7YBNpd5n0oBPlKWJSCpwLNCQ76v7b+CvgC/cgdQCnYCdwOtO092rIpIQ7qDCRVU3A08CG4GtQLaqfhXeqELLEscBUkZZgx9yJiKNgE+A21U1J9zxhIOInAXsUNVF4Y6llogE+gEvquqxQB7QYPsERSQZf+tEGtAWSBCRy8IbVWhZ4jggE2hf6n0K9fxysyIiEoU/abyrqv8LdzxhNAQ4R0Qy8Ddhniwi74Q3pLDKBDJVteQK9L/4E0lDdSqwXlV3qqob+B9wQphjCilLHAcsALqKSJqIROPv3JoU5pjCRkQEfxv2ClV9OtzxhJOq/k1VU1Q1Ff+/F9+par3+i7I8qroN2CQi3ZyiU4DlYQwp3DYCg0Qk3vnv5hTq+WCBOnPP8VBTVY+I3Aqk4x8VMVFVl4U5rHAaAlwO/CIiPztldzv3fjfmD8C7zh9Z64CrwxxP2KjqPBH5L7AY/2jEn6jns8ht5rgxxphKsaYqY4wxlWKJwxhjTKVY4jDGGFMpljiMMcZUiiUOY4wxlWKJw5gKiMj3IhLye0mLyG3OSrPvHlJ+kq3Ia2oTm8dhTAiJSKSqegLc/GbgdFVdH8qYDlXJGI2xKw5TP4hIqvPX+ivOfRG+EpE457P9Vwwi0txZOgQRuUpEPhORySKyXkRuFZE/OQv3/SgiTUsd4jIRmePcb2GAs3+CiEwUkQXOPmNK1fuxiEwGDlvszjnGr87jdqfsJfyLB04Skf8r53sOcOL4yXnu5pT/ICJ9S203W0R6BxqjiLQRkZki8rMT17Cq/9Mw9Z0lDlOfdAXGq2pPYC/w+wD26QVcgn9Z/UeAfGfhvrnAFaW2S1DVE/BfFUx0yu7Bv/zI8cAI4IlSq8QOBq5U1ZNLH0xEjsM/y3og/vucXC8ix6rqjfjXRhuhqs+UE+9vwHAnxvuAfzrlrwJXOcc4CohR1aWViPESIF1V++K/v0bJagHGHMaaqkx9sl5VS054i4DUAPaZ7txvJFdEsoHJTvkvQO9S270PoKozRaSxiDQBRuJf/PAvzjaxQAfn9deqmlXG8YYCn6pqHoCI/A8Yhn+ZikAkAW/+f3t3zBJXEIVh+P2KVBY2VilMJVhYGNJa+AeEiGL6WKWwTBVSWQTERqtAJC4INjb2NmGDQiJiCPkBaVKHBAwIusdiRpzdJO4dwWb3e2Dh3oE7e3ZZ9tyZC+dImiBVb36Qx/eA15JeAs+BVh5vGuMx8D4Xttwvvkezv3jFYYPkvDi+5ObG6IKb33pvS8/ymk5x3qH7xqq3Nk+QSvEvRMR0fo1HxHVxu7P/xPiv8v01VknJbgqYI3+eiPgDHJDKexaqEBQAAAD+SURBVC8Bu8X79Y0xItqk5kw/gB1J5WrLrIsThw2D78CTfLx4xzmeAUiaITXq+UUqiLmSK6Ii6XGDedrA01xJdQSYBz5WxDFK+nOHvDVV2AI2geNiJdEoRkmPSD1H3pGqIg9zmXTrw4nDhsE68ELSETB2xzl+5uvfAst5bJW0VfRV0rd8fqvcjrcFfCZ1VNyKiKbbVABrwBtJh6QqzuXcJ8BvYLsYbhrjLPBF0inp2dBGRUw2ZFwd12xASHoIfAAmI8Itbu3eeMVhNgDyM4lPwCsnDbtvXnGYmVkVrzjMzKyKE4eZmVVx4jAzsypOHGZmVsWJw8zMqjhxmJlZlSsVJrfPfIVb9wAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def _receptive_field_size_temporal_conv_net_(kernel_size, n_layers):\n",
    "    return 1 + 2 * (kernel_size - 1) * (2 ** n_layers - 1)\n",
    "\n",
    "for k in range(2, 5):\n",
    "    plt.plot([_receptive_field_size_temporal_conv_net_(kernel_size=k, n_layers=n) for n in range(10)], label=f\"kernel size: {k}\")\n",
    "plt.xlabel('number of layers')\n",
    "plt.ylabel('receptive field size')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-15T21:47:34.469690Z",
     "start_time": "2019-06-15T21:47:34.463956Z"
    }
   },
   "outputs": [],
   "source": [
    "LatentSpaceEncoding = namedtuple('LatentSpaceEncoding', 'mean var')\n",
    "\n",
    "def get_latent_space(model, X):\n",
    "    def _encode_(x):\n",
    "        if hasattr(model, 'encode'):\n",
    "            # normal model\n",
    "            return model.encode(x)\n",
    "        else:\n",
    "            # only encoder/inference net\n",
    "            return model(x)\n",
    "        \n",
    "    if model.__class__ in [DrosophVAEConv, DrosophVAESkipConv]:\n",
    "        return LatentSpaceEncoding(*map(lambda x: x.numpy(), _encode_(X)))\n",
    "    else:\n",
    "        return LatentSpaceEncoding(*map(lambda x: x.numpy()[back_to_single_time], _encode_(X)))\n",
    "    \n",
    "    \n",
    "def _reshape_and_rescale_(X, scaler=scaler, data_type=run_cfg['data_type']):\n",
    "    \"\"\"To be defined in this notebook / function. Basically a larger lambda function\n",
    "    \"\"\"\n",
    "    rescaled = scaler.inverse_transform(X)\n",
    "    if data_type ==  config.DataType.POS_2D:\n",
    "        return rescaled.reshape(-1, 15, 2)\n",
    "    elif data_type ==  config.DataType.ANGLE_3D:\n",
    "        return rescaled\n",
    "    else:\n",
    "        raise ValueError(f\"uh, got something odd: {data_type}\")\n",
    "        \n",
    "def same_experiment_same_fly(exp_0, exp_1):\n",
    "    keys_0 = experiment_key(obj=exp_0).split('-')\n",
    "    keys_1 = experiment_key(obj=exp_1).split('-')\n",
    "    return keys_0[0] == keys_1[0] and keys_0[2] == keys_1[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-15T21:47:34.483733Z",
     "start_time": "2019-06-15T21:47:34.470680Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'drosoph_vae.training.supervised' from '/home/sam/proj/epfl/neural_clustering_vae/drosoph_vae/training/supervised.py'>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reload(vae_training)\n",
    "reload(supervised_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-15T21:47:34.497708Z",
     "start_time": "2019-06-15T21:47:34.484762Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import adjusted_mutual_info_score, homogeneity_score, silhouette_score\n",
    "from drosoph_vae.settings.data import Experiment, experiment_key\n",
    "\n",
    "def eval_model(training_results, X, X_eval, y, y_frames, run_config, supervised=False):\n",
    "    #\n",
    "    # Unsupervised part\n",
    "    #\n",
    "    \n",
    "    model = training_results['model']\n",
    "    exp_desc = run_config.description(short=False)\n",
    "    exp_desc_short = run_config.description()\n",
    "    X_hat_eval = _reshape_and_rescale_(model(X).numpy()[back_to_single_time], data_type=run_config['data_type'])\n",
    "    epochs = len(training_results['train_reports'])\n",
    "    \n",
    "    if supervised:\n",
    "        exp_desc_short = 'supervised' + exp_desc_short\n",
    "\n",
    "    \n",
    "    #\n",
    "    # Reconstruction plots\n",
    "    #\n",
    "    \n",
    "    if run_config['data_type'] == config.DataType.ANGLE_3D:\n",
    "        plot_recon_path = plots.plot_reconstruction_comparision_angle_3d(X_eval, X_hat_eval, \n",
    "                                                                         epochs=epochs, \n",
    "                                                                         selected_columns=selected_columns,\n",
    "                                                                         run_desc=exp_desc_short)\n",
    "    else:\n",
    "        plot_recon_path = plots.plot_reconstruction_comparision_pos_2d(X_eval, X_hat_eval, \n",
    "                                                                 epochs=epochs, \n",
    "                                                                 run_desc=exp_desc_short)\n",
    "        \n",
    "    #\n",
    "    # Latent plot\n",
    "    #\n",
    "\n",
    "    X_latent = get_latent_space(training_results['model'], X)\n",
    "    X_latent_mean_tsne_proj = TSNE(n_components=2, random_state=42).fit_transform(np.hstack((X_latent.mean, X_latent.var)))\n",
    "\n",
    "    #cluster_assignments = HDBSCAN(min_cluster_size=8).fit_predict(np.hstack((X_latent.mean, X_latent.var)))\n",
    "    # average because of the triplet loss (maybe? kinda makes sense... not?)\n",
    "    cluster_assignments = AgglomerativeClustering(n_clusters=2 * len(list(config.Behavior)), linkage='average')\\\n",
    "        .fit_predict(np.hstack((X_latent.mean, X_latent.var)))\n",
    "                                                                                      \n",
    "    plot_latent_path = plots.plot_latent_space(X_latent,\n",
    "                                         X_latent_mean_tsne_proj,\n",
    "                                         np.array([y.label.name for _, y in y_frames[back_to_single_time]]),\n",
    "                                         cluster_assignments,\n",
    "                                         exp_desc_short,\n",
    "                                         epochs=len(training_results['train_reports']))\n",
    "    \n",
    "    #\n",
    "    # Videos\n",
    "    #\n",
    "    group_videos = list(video.group_video_of_clusters(cluster_assignments,\n",
    "                                                      y_frames[back_to_single_time],\n",
    "                                                      exp_desc_short, \n",
    "                                                      epochs=epochs))\n",
    "    \n",
    "    #nmi = normalized_mutual_information(cluster_assignments, y)\n",
    "    #pur = purity(cluster_assignments, y)\n",
    "    silhouette = silhouette_score(np.hstack((X_latent.mean, X_latent.var)), y[:, -1])\n",
    "    adjusted_mutual_info = adjusted_mutual_info_score(y[:, -1], cluster_assignments)\n",
    "    homogeneity = homogeneity_score(y[:, -1], cluster_assignments)\n",
    "    \n",
    "    #\n",
    "    # Single video of Hubert, the special fly\n",
    "    # NOTE that the data is altered here\n",
    "    #\n",
    "\n",
    "    hubert = Experiment(**SetupConfig.value('hubert'))\n",
    "    hubert_idx = np.array([same_experiment_same_fly(l, hubert) for l in y_frames[back_to_single_time][:, 1]])\n",
    "\n",
    "    exp_descs = np.array([experiment_key(obj=l) for l in y_frames[back_to_single_time][:, 1]])\n",
    "\n",
    "    X_hat_eval = X_hat_eval[hubert_idx, :]\n",
    "    cluster_assignments = cluster_assignments[hubert_idx]\n",
    "    image_id_with_exp = y_frames[back_to_single_time][hubert_idx]\n",
    "    paths = [video._path_for_image_(image_id, label) for image_id, label in image_id_with_exp]\n",
    "\n",
    "    labels = [l.label.name for l in y_frames[back_to_single_time][hubert_idx, 1]]\n",
    "    \n",
    "    if run_config['data_type'] == config.DataType.POS_2D:\n",
    "        mean_, std_ = normalisation_factors[experiment_key(obj=hubert)]\n",
    "        X_hat_eval = (X_hat_eval *std_) + mean_\n",
    "\n",
    "        X_raw_input = (frame_data.reshape(-1, 15, 2) * std_) + mean_\n",
    "        X_raw_input = X_raw_input[y_frames[back_to_single_time][:, 0].astype(np.int)]\n",
    "        X_hat_eval = np.clip(X_hat_eval, np.min(X_raw_input), np.max(X_raw_input)) # some odd errors otherwise\n",
    "\n",
    "        full_video_path = video.comparision_video_of_reconstruction((X_raw_input, X_hat_eval), \n",
    "                                                                    cluster_assignments,\n",
    "                                                                    image_id_with_exp,\n",
    "                                                                    labels,\n",
    "                                                                    n_train_data_points,\n",
    "                                                                    paths,\n",
    "                                                                    epochs=epochs,\n",
    "                                                                    run_desc=exp_desc_short)\n",
    "    else:\n",
    "        full_video_path = video.comparision_video_of_reconstruction([],\n",
    "                                                                    cluster_assignments,\n",
    "                                                                    image_id_with_exp,\n",
    "                                                                    labels,\n",
    "                                                                    n_train_data_points,\n",
    "                                                                    paths,\n",
    "                                                                    epochs=epochs,\n",
    "                                                                    run_desc=exp_desc_short)\n",
    "        \n",
    "    \n",
    "\n",
    "    return {'latent_projection': X_latent_mean_tsne_proj, \n",
    "            'cluster_assignments': cluster_assignments,\n",
    "            'plot_paths': {'reconstruction': plot_recon_path, 'latent': plot_latent_path},\n",
    "            'video_paths': {'groups': group_videos, 'hubert': full_video_path},\n",
    "            'scores': {\n",
    "                'silhouette': silhouette,\n",
    "                'adjusted_mutual_info': adjusted_mutual_info,\n",
    "                'homogeneity': homogeneity\n",
    "            }\n",
    "           }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-15T21:47:34.522336Z",
     "start_time": "2019-06-15T21:47:34.498727Z"
    }
   },
   "outputs": [],
   "source": [
    "X = np.vstack((X_train, X_test))\n",
    "y = np.vstack((y_train, y_test))\n",
    "y_frames = np.vstack((frame_labels_train, frame_labels_test))\n",
    "\n",
    "train_dataset = to_tf_data(X_train, y_train, batch_size=run_cfg['batch_size'])\n",
    "test_dataset = to_tf_data(X_test, y_test, batch_size=run_cfg['batch_size']) \n",
    "\n",
    "if run_cfg['use_time_series']:\n",
    "    back_to_single_time = np.s_[:, -1, :]\n",
    "else:\n",
    "    back_to_single_time = np.s_[:]\n",
    "    \n",
    "X_eval = _reshape_and_rescale_(X[back_to_single_time])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-15T21:47:34.534705Z",
     "start_time": "2019-06-15T21:47:34.523557Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "\n",
    "def grid_search(grid_search_params):\n",
    "    parameters = product(*grid_search_params.values())\n",
    "    # it's important that it is a generator, tensorflow might complain overwise \n",
    "    # too many writers and such, depends heavily on the computer\n",
    "    cfgs = ((p, config.RunConfig(**dict(zip(grid_search_params.keys(), p)))) for p in parameters)\n",
    "    \n",
    "    vae_n_epochs = SetupConfig.value('training', 'vae', 'n_epochs')\n",
    "    vae_n_epochs_eval = SetupConfig.value('training', 'vae', 'n_epochs_eval')\n",
    "    supervised_n_epochs = SetupConfig.value('training', 'supervised', 'n_epochs')\n",
    "    supervised_n_epochs_eval = SetupConfig.value('training', 'supervised', 'n_epochs_eval')\n",
    "        \n",
    "    for p, cfg in cfgs:\n",
    "        #\n",
    "        # Unsupervised part\n",
    "        #\n",
    "        \n",
    "        # not the best code, but it needs to run... some results are better than none\n",
    "        try:\n",
    "            # this allows continuous training with a fixed number of epochs. uuuh yeah.\n",
    "            # there is however a side-effect problem here. I am running this on a GPU, `init` and `train` need to be called in order.\n",
    "            # it needs to be init->train, init->train, ... init resets the graph, and I guess this will free up memory\n",
    "            vae_training_args = vae_training.init(input_shape=X_train.shape[1:], run_config=cfg)\n",
    "            # model, losses, ...\n",
    "            vae_training_results = {}\n",
    "            # paths\n",
    "            vae_eval_results = []\n",
    "            for u in range(np.int(vae_n_epochs/ vae_n_epochs_eval)):\n",
    "                vae_training_results = vae_training.train(**{**vae_training_args, **vae_training_results},\n",
    "                                                          train_dataset=train_dataset, \n",
    "                                                          test_dataset=test_dataset,\n",
    "                                                          early_stopping=False,\n",
    "                                                          n_epochs=vae_n_epochs_eval)\n",
    "\n",
    "                vae_eval_results += [eval_model(vae_training_results, X, X_eval, y, y_frames, cfg)]\n",
    "                #for n, p in vae_eval_results[-1]['plot_paths'].items():\n",
    "                #    tf_helpers.tf_write_image(vae_training_args['test_summary_writer'], n, p, vae_training_results['train_reports'].shape[0])\n",
    "\n",
    "        except Exception:\n",
    "            print(f\"problem with unsupervised {vae_training_args}: {traceback.format_exc()}\")\n",
    "            continue\n",
    "            \n",
    "        try:\n",
    "            base_mdl = vae_training_results['model'].__class__(**vae_training_args['model_config'])\n",
    "            base_mdl.load_weights(vae_training_args['model_checkpoints_path'])\n",
    "\n",
    "            vae_training_results['model'] = base_mdl\n",
    "            vae_best = eval_model(vae_training_results, X, X_eval, y, y_frames, cfg)\n",
    "        except Exception:\n",
    "            print(f\"problem with loading the model: {traceback.format_exc()}\")\n",
    "            continue\n",
    "        #\n",
    "        # Supervised part\n",
    "        # \n",
    "        \n",
    "        try:\n",
    "            # the training process saves the model with the min loss.\n",
    "            \n",
    "            supervised_training_args = supervised_training.init(model=base_mdl.inference_net, run_config=cfg)\n",
    "            supervised_training_results = {}\n",
    "            supervised_eval_results = []\n",
    "            \n",
    "            for u in range(np.int(supervised_n_epochs/ supervised_n_epochs_eval)):\n",
    "                supervised_training_results = supervised_training.train(**{**supervised_training_args, **supervised_training_results},\n",
    "                                                          train_dataset=train_dataset, \n",
    "                                                          test_dataset=test_dataset,\n",
    "                                                          early_stopping=False,\n",
    "                                                          n_epochs=supervised_n_epochs_eval)\n",
    "\n",
    "                base_mdl.inference_net = supervised_training_results['model']\n",
    "                supervised_training_results['model'] = base_mdl \n",
    "                supervised_eval_results += [eval_model(supervised_training_results, X, X_eval, y, y_frames, cfg, supervised=True)]\n",
    "                supervised_training_results['model'] = base_mdl.inference_net\n",
    "\n",
    "            \n",
    "            # it always saves the full model\n",
    "            base_mdl.load_weights(vae_training_args['model_checkpoints_path'])\n",
    "            base_mdl.inference_net.load_weights(supervised_training_args['model_checkpoints_path'])\n",
    "            supervised_training_results['model'] = base_mdl \n",
    "            supervised_best = eval_model(supervised_training_results, X, X_eval, y, y_frames, cfg, supervised=True)\n",
    "        except Exception:\n",
    "            print(f\"problem with supervised {vae_training_args}\\n\\t{supervised_training_args}\\n\\t{traceback.format_exc()}\")\n",
    "            continue\n",
    "        \n",
    "        # too many figures overwise (duh)\n",
    "        plt.close('all')\n",
    "        \n",
    "        res = {'parameters': p,\n",
    "               'vae': {'train_reports': vae_training_results['train_reports'], \n",
    "                       'test_reports':  vae_training_results['test_reports'],\n",
    "                       'model_checkpoints_path': vae_training_args['model_checkpoints_path'],\n",
    "                       'best_model_eval_results': vae_best,\n",
    "                       'eval_results': vae_eval_results},\n",
    "               'supervised': {'train_reports': supervised_training_results['train_reports'],\n",
    "                              'test_reports':  supervised_training_results['test_reports'],\n",
    "                              'model_checkpoints_path': supervised_training_args['model_checkpoints_path'],\n",
    "                              'best_model_eval_results': supervised_best,\n",
    "                              'eval_results': supervised_eval_results}}\n",
    "        \n",
    "        yield res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-15T21:47:34.549555Z",
     "start_time": "2019-06-15T21:47:34.536067Z"
    }
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "class NoParsingFilter(logging.Filter):\n",
    "    def filter(self, record):\n",
    "        return not ('input image is not divisible' in record.getMessage())\n",
    "\n",
    "# such a pain in the ass\n",
    "logger= logging.getLogger('imageio_ffmpeg')\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.addFilter(NoParsingFilter())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-15T21:49:44.130308Z",
     "start_time": "2019-06-15T21:47:34.550651Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model: ModelType.SKIP_PADD_CONV\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-15 23:47:38,917 - tensorflow - WARNING - deprecation_wrapper - From /home/sam/proj/epfl/neural_clustering_vae/drosoph_vae/models/drosoph_vae.py:115: The name tf.random_normal is deprecated. Please use tf.random.normal instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0000, train/test loss: 1.923\t 2.733 took 5.833 sec\n",
      "Epoch: 0000, train/test loss: 1.000\t 1.000 took 1.947 sec\n",
      "=.........\r"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "# Note that the data will be reused -> Don't adapt the data_type here. \n",
    "# Either include the data loading into the grid-search or make two runs, one for each DataType\n",
    "\n",
    "grid_search_params = {\n",
    "    'model_impl': list(config.ModelType),\n",
    "    'latent_dim': [2, 4, ],\n",
    "    'vae_learning_rate': [1e-4, 1e-6],\n",
    "    'supervised_learning_rate': [1e-5, ],\n",
    "    'time_series_length': [16, 42],\n",
    "}\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "    started_at = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    if SetupConfig.runs_on_lab_server():\n",
    "        grid_search_results = list(grid_search(grid_search_params))\n",
    "        misc.dump_results(grid_search_results, f\"grid_search_only_vae_{started_at}\")\n",
    "    else:\n",
    "        grid_search_params = {\n",
    "            'model_impl': [config.ModelType.SKIP_PADD_CONV], # config.ModelType.values(),\n",
    "            'latent_dim': [2, ]\n",
    "        }\n",
    "        grid_search_results = list(grid_search(grid_search_params))\n",
    "        misc.dump_results(grid_search_results, f\"grid_search_only_vae_{started_at}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-15T21:50:15.746082Z",
     "start_time": "2019-06-15T21:50:15.735547Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'parameters': (<ModelType.SKIP_PADD_CONV: 2>, 2),\n",
       "  'vae': {'train_reports': array([[1.92296663, 1.92296663, 0.17722391],\n",
       "          [1.90406808, 1.90406808, 0.18091286],\n",
       "          [1.87522392, 1.87522392, 0.18521823],\n",
       "          [1.86022982, 1.86022982, 0.19059084]]),\n",
       "   'test_reports': array([[2.73343933, 2.73343933, 0.17722384],\n",
       "          [2.68942308, 2.68942308, 0.18091247],\n",
       "          [2.67720544, 2.67720544, 0.1852181 ],\n",
       "          [2.68128562, 2.68128562, 0.19058998]]),\n",
       "   'model_checkpoints_path': '/home/sam/proj/epfl/neural_clustering_data/models/SKIP_PADD_CONV-POS_2D-t-16-ld-8-mf-T_20190615-234738/checkpoint',\n",
       "   'best_model_eval_results': {'latent_projection': array([[  3.309956 , -37.229736 ],\n",
       "           [ -2.206424 ,  51.12841  ],\n",
       "           [ -5.510165 , -24.075493 ],\n",
       "           ...,\n",
       "           [  7.8207755,  46.002445 ],\n",
       "           [  3.5806134, -45.49971  ],\n",
       "           [ 23.580116 , -23.049122 ]], dtype=float32),\n",
       "    'cluster_assignments': array([ 9,  1,  9, ...,  1, 12,  9]),\n",
       "    'plot_paths': {'reconstruction': '/home/sam/proj/epfl/neural_clustering_data/figures/SKIP_PADD_CONV-POS_2D-t-16-ld-8_20190615-234738_e-4_input_gen_recon_comparision.png',\n",
       "     'latent': '/home/sam/proj/epfl/neural_clustering_data/figures/SKIP_PADD_CONV-POS_2D-t-16-ld-8_20190615-234738_e-4_latent_space_tsne.png'},\n",
       "    'video_paths': {'groups': [(9,\n",
       "       '/home/sam/proj/epfl/neural_clustering_data/videos/group_of_cluster-9-SKIP_PADD_CONV-POS_2D-t-16-ld-8_20190615-234738-e-4.mp4'),\n",
       "      (1,\n",
       "       '/home/sam/proj/epfl/neural_clustering_data/videos/group_of_cluster-1-SKIP_PADD_CONV-POS_2D-t-16-ld-8_20190615-234738-e-4.mp4'),\n",
       "      (0,\n",
       "       '/home/sam/proj/epfl/neural_clustering_data/videos/group_of_cluster-0-SKIP_PADD_CONV-POS_2D-t-16-ld-8_20190615-234738-e-4.mp4'),\n",
       "      (6,\n",
       "       '/home/sam/proj/epfl/neural_clustering_data/videos/group_of_cluster-6-SKIP_PADD_CONV-POS_2D-t-16-ld-8_20190615-234738-e-4.mp4'),\n",
       "      (2,\n",
       "       '/home/sam/proj/epfl/neural_clustering_data/videos/group_of_cluster-2-SKIP_PADD_CONV-POS_2D-t-16-ld-8_20190615-234738-e-4.mp4'),\n",
       "      (3,\n",
       "       '/home/sam/proj/epfl/neural_clustering_data/videos/group_of_cluster-3-SKIP_PADD_CONV-POS_2D-t-16-ld-8_20190615-234738-e-4.mp4'),\n",
       "      (4,\n",
       "       '/home/sam/proj/epfl/neural_clustering_data/videos/group_of_cluster-4-SKIP_PADD_CONV-POS_2D-t-16-ld-8_20190615-234738-e-4.mp4'),\n",
       "      (5,\n",
       "       '/home/sam/proj/epfl/neural_clustering_data/videos/group_of_cluster-5-SKIP_PADD_CONV-POS_2D-t-16-ld-8_20190615-234738-e-4.mp4'),\n",
       "      (7,\n",
       "       '/home/sam/proj/epfl/neural_clustering_data/videos/group_of_cluster-7-SKIP_PADD_CONV-POS_2D-t-16-ld-8_20190615-234738-e-4.mp4'),\n",
       "      (8,\n",
       "       '/home/sam/proj/epfl/neural_clustering_data/videos/group_of_cluster-8-SKIP_PADD_CONV-POS_2D-t-16-ld-8_20190615-234738-e-4.mp4')],\n",
       "     'hubert': '/home/sam/proj/epfl/neural_clustering_data/videos/SKIP_PADD_CONV-POS_2D-t-16-ld-8_20190615-234738_e-4_hubert_full.mp4'},\n",
       "    'scores': {'silhouette': -0.043524086,\n",
       "     'adjusted_mutual_info': 0.014169202401003281,\n",
       "     'homogeneity': 0.026082948190837594}},\n",
       "   'eval_results': [{'latent_projection': array([[-26.603584 , -22.046602 ],\n",
       "            [  7.9341545, -38.606518 ],\n",
       "            [-17.342089 , -49.191536 ],\n",
       "            ...,\n",
       "            [ 21.013165 ,   8.604893 ],\n",
       "            [  7.752834 , -10.284549 ],\n",
       "            [ 12.829642 , -36.311615 ]], dtype=float32),\n",
       "     'cluster_assignments': array([1, 0, 5, ..., 0, 0, 0]),\n",
       "     'plot_paths': {'reconstruction': '/home/sam/proj/epfl/neural_clustering_data/figures/SKIP_PADD_CONV-POS_2D-t-16-ld-8_20190615-234738_e-4_input_gen_recon_comparision.png',\n",
       "      'latent': '/home/sam/proj/epfl/neural_clustering_data/figures/SKIP_PADD_CONV-POS_2D-t-16-ld-8_20190615-234738_e-4_latent_space_tsne.png'},\n",
       "     'video_paths': {'groups': [(0,\n",
       "        '/home/sam/proj/epfl/neural_clustering_data/videos/group_of_cluster-0-SKIP_PADD_CONV-POS_2D-t-16-ld-8_20190615-234738-e-4.mp4'),\n",
       "       (2,\n",
       "        '/home/sam/proj/epfl/neural_clustering_data/videos/group_of_cluster-2-SKIP_PADD_CONV-POS_2D-t-16-ld-8_20190615-234738-e-4.mp4'),\n",
       "       (5,\n",
       "        '/home/sam/proj/epfl/neural_clustering_data/videos/group_of_cluster-5-SKIP_PADD_CONV-POS_2D-t-16-ld-8_20190615-234738-e-4.mp4'),\n",
       "       (1,\n",
       "        '/home/sam/proj/epfl/neural_clustering_data/videos/group_of_cluster-1-SKIP_PADD_CONV-POS_2D-t-16-ld-8_20190615-234738-e-4.mp4'),\n",
       "       (8,\n",
       "        '/home/sam/proj/epfl/neural_clustering_data/videos/group_of_cluster-8-SKIP_PADD_CONV-POS_2D-t-16-ld-8_20190615-234738-e-4.mp4'),\n",
       "       (9,\n",
       "        '/home/sam/proj/epfl/neural_clustering_data/videos/group_of_cluster-9-SKIP_PADD_CONV-POS_2D-t-16-ld-8_20190615-234738-e-4.mp4'),\n",
       "       (3,\n",
       "        '/home/sam/proj/epfl/neural_clustering_data/videos/group_of_cluster-3-SKIP_PADD_CONV-POS_2D-t-16-ld-8_20190615-234738-e-4.mp4'),\n",
       "       (4,\n",
       "        '/home/sam/proj/epfl/neural_clustering_data/videos/group_of_cluster-4-SKIP_PADD_CONV-POS_2D-t-16-ld-8_20190615-234738-e-4.mp4'),\n",
       "       (6,\n",
       "        '/home/sam/proj/epfl/neural_clustering_data/videos/group_of_cluster-6-SKIP_PADD_CONV-POS_2D-t-16-ld-8_20190615-234738-e-4.mp4'),\n",
       "       (7,\n",
       "        '/home/sam/proj/epfl/neural_clustering_data/videos/group_of_cluster-7-SKIP_PADD_CONV-POS_2D-t-16-ld-8_20190615-234738-e-4.mp4')],\n",
       "      'hubert': '/home/sam/proj/epfl/neural_clustering_data/videos/SKIP_PADD_CONV-POS_2D-t-16-ld-8_20190615-234738_e-4_hubert_full.mp4'},\n",
       "     'scores': {'silhouette': -0.058291934,\n",
       "      'adjusted_mutual_info': 0.020128755298625768,\n",
       "      'homogeneity': 0.029901366465149463}}]},\n",
       "  'supervised': {'train_reports': array([[1.],\n",
       "          [1.]]), 'test_reports': array([[1.],\n",
       "          [1.]]), 'model_checkpoints_path': '/home/sam/proj/epfl/neural_clustering_data/models/SKIP_PADD_CONV-POS_2D-t-16-ld-8_20190615-234738_supervised/checkpoint', 'best_model_eval_results': {'latent_projection': array([[-20.670465 ,  28.580317 ],\n",
       "           [ 26.398445 , -38.177895 ],\n",
       "           [-19.152752 ,  13.921611 ],\n",
       "           ...,\n",
       "           [ 29.745504 , -31.428114 ],\n",
       "           [-25.706347 ,  34.54971  ],\n",
       "           [  2.9557638,  31.19522  ]], dtype=float32),\n",
       "    'cluster_assignments': array([ 0, 10,  0, ...,  0,  0,  0]),\n",
       "    'plot_paths': {'reconstruction': '/home/sam/proj/epfl/neural_clustering_data/figures/supervisedSKIP_PADD_CONV-POS_2D-t-16-ld-8_20190615-234738_e-2_input_gen_recon_comparision.png',\n",
       "     'latent': '/home/sam/proj/epfl/neural_clustering_data/figures/supervisedSKIP_PADD_CONV-POS_2D-t-16-ld-8_20190615-234738_e-2_latent_space_tsne.png'},\n",
       "    'video_paths': {'groups': [(0,\n",
       "       '/home/sam/proj/epfl/neural_clustering_data/videos/group_of_cluster-0-supervisedSKIP_PADD_CONV-POS_2D-t-16-ld-8_20190615-234738-e-2.mp4'),\n",
       "      (1,\n",
       "       '/home/sam/proj/epfl/neural_clustering_data/videos/group_of_cluster-1-supervisedSKIP_PADD_CONV-POS_2D-t-16-ld-8_20190615-234738-e-2.mp4'),\n",
       "      (3,\n",
       "       '/home/sam/proj/epfl/neural_clustering_data/videos/group_of_cluster-3-supervisedSKIP_PADD_CONV-POS_2D-t-16-ld-8_20190615-234738-e-2.mp4'),\n",
       "      (4,\n",
       "       '/home/sam/proj/epfl/neural_clustering_data/videos/group_of_cluster-4-supervisedSKIP_PADD_CONV-POS_2D-t-16-ld-8_20190615-234738-e-2.mp4'),\n",
       "      (2,\n",
       "       '/home/sam/proj/epfl/neural_clustering_data/videos/group_of_cluster-2-supervisedSKIP_PADD_CONV-POS_2D-t-16-ld-8_20190615-234738-e-2.mp4'),\n",
       "      (5,\n",
       "       '/home/sam/proj/epfl/neural_clustering_data/videos/group_of_cluster-5-supervisedSKIP_PADD_CONV-POS_2D-t-16-ld-8_20190615-234738-e-2.mp4'),\n",
       "      (6,\n",
       "       '/home/sam/proj/epfl/neural_clustering_data/videos/group_of_cluster-6-supervisedSKIP_PADD_CONV-POS_2D-t-16-ld-8_20190615-234738-e-2.mp4'),\n",
       "      (7,\n",
       "       '/home/sam/proj/epfl/neural_clustering_data/videos/group_of_cluster-7-supervisedSKIP_PADD_CONV-POS_2D-t-16-ld-8_20190615-234738-e-2.mp4'),\n",
       "      (8,\n",
       "       '/home/sam/proj/epfl/neural_clustering_data/videos/group_of_cluster-8-supervisedSKIP_PADD_CONV-POS_2D-t-16-ld-8_20190615-234738-e-2.mp4'),\n",
       "      (9,\n",
       "       '/home/sam/proj/epfl/neural_clustering_data/videos/group_of_cluster-9-supervisedSKIP_PADD_CONV-POS_2D-t-16-ld-8_20190615-234738-e-2.mp4')],\n",
       "     'hubert': '/home/sam/proj/epfl/neural_clustering_data/videos/supervisedSKIP_PADD_CONV-POS_2D-t-16-ld-8_20190615-234738_e-2_hubert_full.mp4'},\n",
       "    'scores': {'silhouette': -0.050295085,\n",
       "     'adjusted_mutual_info': 0.015384491578431556,\n",
       "     'homogeneity': 0.024656703805343485}}, 'eval_results': [{'latent_projection': array([[-11.430339,  33.46207 ],\n",
       "            [ 17.04167 , -46.920837],\n",
       "            [-14.35123 ,  18.4672  ],\n",
       "            ...,\n",
       "            [ 21.694876, -41.366997],\n",
       "            [-14.601775,  40.934315],\n",
       "            [ 12.440418,  28.960943]], dtype=float32),\n",
       "     'cluster_assignments': array([3, 2, 3, ..., 2, 3, 3]),\n",
       "     'plot_paths': {'reconstruction': '/home/sam/proj/epfl/neural_clustering_data/figures/supervisedSKIP_PADD_CONV-POS_2D-t-16-ld-8_20190615-234738_e-2_input_gen_recon_comparision.png',\n",
       "      'latent': '/home/sam/proj/epfl/neural_clustering_data/figures/supervisedSKIP_PADD_CONV-POS_2D-t-16-ld-8_20190615-234738_e-2_latent_space_tsne.png'},\n",
       "     'video_paths': {'groups': [(3,\n",
       "        '/home/sam/proj/epfl/neural_clustering_data/videos/group_of_cluster-3-supervisedSKIP_PADD_CONV-POS_2D-t-16-ld-8_20190615-234738-e-2.mp4'),\n",
       "       (2,\n",
       "        '/home/sam/proj/epfl/neural_clustering_data/videos/group_of_cluster-2-supervisedSKIP_PADD_CONV-POS_2D-t-16-ld-8_20190615-234738-e-2.mp4'),\n",
       "       (4,\n",
       "        '/home/sam/proj/epfl/neural_clustering_data/videos/group_of_cluster-4-supervisedSKIP_PADD_CONV-POS_2D-t-16-ld-8_20190615-234738-e-2.mp4'),\n",
       "       (9,\n",
       "        '/home/sam/proj/epfl/neural_clustering_data/videos/group_of_cluster-9-supervisedSKIP_PADD_CONV-POS_2D-t-16-ld-8_20190615-234738-e-2.mp4'),\n",
       "       (0,\n",
       "        '/home/sam/proj/epfl/neural_clustering_data/videos/group_of_cluster-0-supervisedSKIP_PADD_CONV-POS_2D-t-16-ld-8_20190615-234738-e-2.mp4'),\n",
       "       (1,\n",
       "        '/home/sam/proj/epfl/neural_clustering_data/videos/group_of_cluster-1-supervisedSKIP_PADD_CONV-POS_2D-t-16-ld-8_20190615-234738-e-2.mp4'),\n",
       "       (5,\n",
       "        '/home/sam/proj/epfl/neural_clustering_data/videos/group_of_cluster-5-supervisedSKIP_PADD_CONV-POS_2D-t-16-ld-8_20190615-234738-e-2.mp4'),\n",
       "       (6,\n",
       "        '/home/sam/proj/epfl/neural_clustering_data/videos/group_of_cluster-6-supervisedSKIP_PADD_CONV-POS_2D-t-16-ld-8_20190615-234738-e-2.mp4'),\n",
       "       (7,\n",
       "        '/home/sam/proj/epfl/neural_clustering_data/videos/group_of_cluster-7-supervisedSKIP_PADD_CONV-POS_2D-t-16-ld-8_20190615-234738-e-2.mp4'),\n",
       "       (8,\n",
       "        '/home/sam/proj/epfl/neural_clustering_data/videos/group_of_cluster-8-supervisedSKIP_PADD_CONV-POS_2D-t-16-ld-8_20190615-234738-e-2.mp4')],\n",
       "      'hubert': '/home/sam/proj/epfl/neural_clustering_data/videos/supervisedSKIP_PADD_CONV-POS_2D-t-16-ld-8_20190615-234738_e-2_hubert_full.mp4'},\n",
       "     'scores': {'silhouette': -0.0508931,\n",
       "      'adjusted_mutual_info': 0.011243415819880921,\n",
       "      'homogeneity': 0.02152432225453273}}]}}]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-15T21:49:44.392502Z",
     "start_time": "2019-06-15T21:49:44.131649Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'stop' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-4f76a9dad686>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mstop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'stop' is not defined"
     ]
    }
   ],
   "source": [
    "stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-15T21:49:44.397009Z",
     "start_time": "2019-06-15T21:47:32.493Z"
    }
   },
   "outputs": [],
   "source": [
    "reload(tf_helpers)\n",
    "reload(vae_training)\n",
    "reload(video)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-15T21:49:44.398049Z",
     "start_time": "2019-06-15T21:47:32.496Z"
    }
   },
   "outputs": [],
   "source": [
    "#if not SetupConfig.runs_on_lab_server():\n",
    "#    reload(vae_training)\n",
    "#    epochs = 14\n",
    "#    eval_steps = 7\n",
    "#    run_cfg['latent_dim'] = 6\n",
    "#    vae_training_args = vae_training.init(input_shape=X_train.shape[1:], run_config=run_cfg)\n",
    "#    vae_training_results = {}\n",
    "#    eval_results = []\n",
    "#    for u in range(np.int(epochs / eval_steps)):\n",
    "#        vae_training_results = vae_training.train(**{**vae_training_args, **vae_training_results},\n",
    "#                                                  train_dataset=train_dataset, \n",
    "#                                                  test_dataset=test_dataset,\n",
    "#                                                  early_stopping=False,\n",
    "#                                                  n_epochs=eval_steps)\n",
    "#\n",
    "#        eval_results += [eval_model(vae_training_results, X, X_eval, y, y_frames, run_cfg)]\n",
    "#\n",
    "#    eval_results += [eval_model(vae_training_results, X, X_eval, y, y_frames, run_cfg)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-15T21:49:44.399077Z",
     "start_time": "2019-06-15T21:47:32.500Z"
    }
   },
   "outputs": [],
   "source": [
    "reload(supervised_training)\n",
    "\n",
    "cfg = RunConfig(model_impl=config.ModelType.SKIP_PADD_CONV, latent_dim=1)\n",
    "vae_training_args = vae_training.init(input_shape=X_train.shape[1:], run_config=cfg)\n",
    "vae_training_results = {}\n",
    "vae_eval_results = []\n",
    "epochs = 14\n",
    "eval_steps = 7\n",
    "\n",
    "try:\n",
    "    for u in range(np.int(epochs / eval_steps)):\n",
    "            vae_training_results = vae_training.train(**{**vae_training_args, **vae_training_results},\n",
    "                                                      train_dataset=train_dataset, \n",
    "                                                      test_dataset=test_dataset,\n",
    "                                                      early_stopping=False,\n",
    "                                                      n_epochs=eval_steps)\n",
    "\n",
    "            vae_eval_results += [eval_model(vae_training_results, X, X_eval, y, y_frames, cfg)]\n",
    "        #for n, p in vae_eval_results[-1]['plot_paths'].items():\n",
    "        #    tf_helpers.tf_write_image(vae_training_args['test_summary_writer'], n, p, vae_training_results['train_reports'].shape[0])\n",
    "\n",
    "    vae_eval_results += [eval_model(vae_training_results, X, X_eval, y, y_frames, cfg)]\n",
    "except Exception:\n",
    "    print(f\"problem with {vae_training_args}: {traceback.format_exc()}\")\n",
    "\n",
    "\n",
    "# the training process saves the model with the min loss.\n",
    "base_mdl = vae_training_results['model'].__class__(**vae_training_args['model_config'])\n",
    "base_mdl.load_weights(vae_training_args['model_checkpoints_path'])\n",
    "\n",
    "supervised_training_args = supervised_training.init(model=base_mdl.inference_net, run_config=cfg)\n",
    "supervised_training_results = {}\n",
    "supervised_eval_results = []\n",
    "\n",
    "for u in range(np.int(epochs / eval_steps)):\n",
    "    supervised_training_results = supervised_training.train(**{**supervised_training_args, **supervised_training_results},\n",
    "                                              train_dataset=train_dataset, \n",
    "                                              test_dataset=test_dataset,\n",
    "                                              early_stopping=False,\n",
    "                                              n_epochs=eval_steps)\n",
    "\n",
    "    base_mdl.inference_net = supervised_training_results['model']\n",
    "    supervised_training_results['model'] = base_mdl \n",
    "    supervised_eval_results += [eval_model(supervised_training_results, X, X_eval, y, y_frames, cfg)]\n",
    "    supervised_training_results['model'] = base_mdl.inference_net\n",
    "\n",
    "base_mdl.inference_net = supervised_training_results['model']\n",
    "supervised_training_results['model'] = base_mdl \n",
    "supervised_eval_results += [eval_model(supervised_training_results, X, X_eval, y, y_frames, cfg)]\n",
    "supervised_training_results['model'] = base_mdl.inference_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-15T21:49:44.400028Z",
     "start_time": "2019-06-15T21:47:32.503Z"
    }
   },
   "outputs": [],
   "source": [
    "base_mdl.inference_net.layers[1].summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-15T21:49:44.400966Z",
     "start_time": "2019-06-15T21:47:32.507Z"
    }
   },
   "outputs": [],
   "source": [
    "X[back_to_single_time].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-15T21:49:44.401969Z",
     "start_time": "2019-06-15T21:47:32.511Z"
    }
   },
   "outputs": [],
   "source": [
    "supervised_training_args['model'](X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-15T21:49:44.402883Z",
     "start_time": "2019-06-15T21:47:32.515Z"
    }
   },
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-15T21:49:44.403872Z",
     "start_time": "2019-06-15T21:47:32.519Z"
    }
   },
   "outputs": [],
   "source": [
    "get_latent_space(supervised_training_results['model'], X)\n",
    "#     17         return LatentSpaceEncoding(*map(lambda x: x.numpy(), model.encode(X)))\n",
    "#     18     else:\n",
    "#---> 19         return LatentSpaceEncoding(*map(lambda x: x.numpy()[back_to_single_time], model.encode(X)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-15T21:49:44.404750Z",
     "start_time": "2019-06-15T21:47:32.522Z"
    }
   },
   "outputs": [],
   "source": [
    "a = tf.zeros((128, 16, 4))\n",
    "b = tf.zeros((128, 16, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-15T21:49:44.405718Z",
     "start_time": "2019-06-15T21:47:32.525Z"
    }
   },
   "outputs": [],
   "source": [
    "tf.concat((a, b), axis=-1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-15T21:49:44.406569Z",
     "start_time": "2019-06-15T21:47:32.529Z"
    }
   },
   "outputs": [],
   "source": [
    "eval_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-15T21:49:44.407407Z",
     "start_time": "2019-06-15T21:47:32.532Z"
    }
   },
   "outputs": [],
   "source": [
    "vae_training_results['test_reports'][:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-15T21:49:44.408274Z",
     "start_time": "2019-06-15T21:47:32.535Z"
    }
   },
   "outputs": [],
   "source": [
    "vae_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-15T21:49:44.409161Z",
     "start_time": "2019-06-15T21:47:32.539Z"
    }
   },
   "outputs": [],
   "source": [
    "eval_model(vae_training_results, X, X_eval, y, y_frames, run_cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-15T21:49:44.410011Z",
     "start_time": "2019-06-15T21:47:32.543Z"
    }
   },
   "outputs": [],
   "source": [
    "from drosoph_vae.settings.data import Experiment, experiment_key\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-14T11:41:23.099387Z",
     "start_time": "2019-06-14T11:40:21.576Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-15T21:49:44.411036Z",
     "start_time": "2019-06-15T21:47:32.548Z"
    }
   },
   "outputs": [],
   "source": [
    "base_mdl = vae_training_results['model'].__class__(latent_dim=run_cfg['latent_dim'], \n",
    "                                                   input_shape=X_train.shape[1:],\n",
    "                                                   batch_size=run_cfg['batch_size'])\n",
    "base_mdl.load_weights(vae_training_args['model_checkpoints_path'])\n",
    "\n",
    "X_hat = base_mdl(X).numpy()[back_to_single_time]\n",
    "X_hat = _reshape_and_rescale_(X_hat, data_type=run_cfg['data_type'])\n",
    "\n",
    "cluster_assignments = AgglomerativeClustering(n_clusters=2 * len(list(config.Behavior)), linkage='average')\\\n",
    "        .fit_predict(X_encoded)\n",
    "\n",
    "hubert = Experiment(**SetupConfig.value('hubert'))\n",
    "hubert_idx = np.array([same_experiment_same_fly(l, hubert) for l in y_frames[back_to_single_time][:, 1]])\n",
    "\n",
    "exp_descs = np.array([experiment_key(obj=l) for l in y_frames[back_to_single_time][:, 1]])\n",
    "\n",
    "X_hat = X_hat[hubert_idx, :]\n",
    "cluster_assignments = cluster_assignments[hubert_idx]\n",
    "image_id_with_exp = y_frames[back_to_single_time][hubert_idx]\n",
    "paths = [video._path_for_image_(image_id, label) for image_id, label in image_id_with_exp]\n",
    "\n",
    "labels = [l.label.name for l in y_frames[back_to_single_time][hubert_idx, 1]]\n",
    "mean_, std_ = normalisation_factors[experiment_key(obj=hubert)]\n",
    "X_hat = (X_hat *std_) + mean_\n",
    "\n",
    "_t = frame_data.reshape(-1, 15, 2)\n",
    "X_raw_input = np.vstack((_t[run_cfg['time_series_length'] - 1:n_train_data_points], _t[n_train_data_points + run_cfg['time_series_length'] -1:]))\n",
    "\n",
    "X_raw_input = (X_raw_input * std_) + mean_\n",
    "X_raw_input = X_raw_input[y_frames[back_to_single_time][:, 0].astype(np.int)][hubert_idx]\n",
    "X_hat = np.clip(X_hat, np.min(X_raw_input), np.max(X_raw_input)) # some odd errors otherwise\n",
    "\n",
    "comparision_video_of_reconstruction((X_raw_input, X_hat), cluster_assignments, image_id_with_exp, labels, n_train_data_points, paths, run_desc=run_cfg.description())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-15T21:49:44.411965Z",
     "start_time": "2019-06-15T21:47:32.552Z"
    }
   },
   "outputs": [],
   "source": [
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-15T21:49:44.412925Z",
     "start_time": "2019-06-15T21:47:32.557Z"
    }
   },
   "outputs": [],
   "source": [
    "display_video('./tryout.mp4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-15T21:49:44.414120Z",
     "start_time": "2019-06-15T21:47:32.562Z"
    }
   },
   "outputs": [],
   "source": [
    "X_encoded = np.hstack([t.numpy() for t in base_mdl.encode(X)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-15T21:49:44.415140Z",
     "start_time": "2019-06-15T21:47:32.567Z"
    }
   },
   "outputs": [],
   "source": [
    "SetupConfig.value('fly_image_template')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-15T21:49:44.416122Z",
     "start_time": "2019-06-15T21:47:32.578Z"
    }
   },
   "outputs": [],
   "source": [
    "X_eval.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-13T22:13:35.775510Z",
     "start_time": "2019-06-13T22:13:35.766999Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-15T21:49:44.416876Z",
     "start_time": "2019-06-15T21:47:32.582Z"
    }
   },
   "outputs": [],
   "source": [
    "normalisation_factors.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-15T21:49:44.417539Z",
     "start_time": "2019-06-15T21:47:32.591Z"
    }
   },
   "outputs": [],
   "source": [
    "stop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-21T16:37:31.785780Z",
     "start_time": "2019-05-21T16:37:31.782033Z"
    }
   },
   "source": [
    "# evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-15T21:49:44.418127Z",
     "start_time": "2019-06-15T21:47:32.595Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "#if run_cfg['data_type'] == config.DataType.POS_2D:\n",
    "#    fig = plots.plot_comparing_joint_position_with_reconstructed(X_eval,\n",
    "#                                                                 X_hat_eval,\n",
    "#                                                                 X_gen_eval,\n",
    "#                                                                 validation_cut_off=n_train_data_points,\n",
    "#                                                                 exp_desc=exp_desc_short);\n",
    "#else:\n",
    "#    # ncols is an ugly hack... it works on the basis that we have three working angles for each leg\n",
    "#    if run_cfg['use_single_fly']:\n",
    "#        start = 0\n",
    "#        end = len(X_eval)\n",
    "#    else:\n",
    "#        start = 100\n",
    "#        end = 1000\n",
    "#    xticks = np.arange(start, end) / SetupConfig.value('frames_per_second') / 60.\n",
    "#    if run_cfg['debug']:\n",
    "#        _input_data = X[:, :, 0]\n",
    "#        _recon = model(X, apply_sigmoid=False).numpy()[:, :, 0]\n",
    "#        fig, axs = plt.subplots(nrows=_input_data.shape[-1], ncols=1, figsize=(20, 30), sharex=True, sharey=True)\n",
    "#        for i in range(_input_data.shape[-1]):\n",
    "#            _idx_ = np.s_[start:end, i]\n",
    "#            axs[i].plot(xticks, _input_data[_idx_], label='input')\n",
    "#            axs[i].plot(xticks, _recon[_idx_], label='reconstructed')\n",
    "#    else:\n",
    "#        fig, axs = plt.subplots(nrows=X_eval.shape[1], ncols=1, figsize=(20, 30), sharex=True, sharey=True)\n",
    "#        for i, cn in enumerate(SD.get_3d_columns_names(selected_cols)):\n",
    "#            _idx_ = np.s_[start:end, i]\n",
    "#            axs[i].plot(xticks, X_eval[_idx_], label='input')\n",
    "#            axs[i].plot(xticks, reconstructed_data[_idx_], label='reconstructed')\n",
    "#\n",
    "#            axs[i].set_title(cn)\n",
    "#\n",
    "#    axs[-1].set_xlabel('time [min]')\n",
    "#    axs[0].legend(loc='upper left')\n",
    "#    \n",
    "#    #plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "#    plt.suptitle(f\"Comparision of selection of data\\n({exp_desc})\")\n",
    "#    \n",
    "#    plt.tight_layout()\n",
    "#    plt.subplots_adjust(top=0.94)\n",
    "#    plt.savefig(f\"./figures/{exp_desc_short}_input_gen_recon_comparision.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Latent space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-15T21:49:44.418839Z",
     "start_time": "2019-06-15T21:47:32.600Z"
    }
   },
   "outputs": [],
   "source": [
    "#plot_latent_space(X_latent, X_latent_mean_tsne_proj, y, run_cfg, epochs=len(vae_training_results['train_reports']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-15T21:49:44.419596Z",
     "start_time": "2019-06-15T21:47:32.610Z"
    }
   },
   "outputs": [],
   "source": [
    "#from matplotlib.collections import LineCollection\n",
    "#\n",
    "#def plot_debug(input_data, cluster_assignments, cluster_colors=None):\n",
    "#    _clusters = np.unique(cluster_assignments)\n",
    "#    _colors = sns.color_palette(n_colors=len(_clusters))\n",
    "#    if cluster_colors is None:\n",
    "#        cluster_colors = dict(zip(_clusters, _colors))\n",
    "#        \n",
    "#    lines, colors = zip(*[([(x, input_data[x, 0]) for x in segment], cluster_colors[cluster_id])\n",
    "#                           for cluster_id, segments in video.group_by_cluster(cluster_assignments).items() \n",
    "#                           for segment in segments])\n",
    "#\n",
    "#\n",
    "#    \n",
    "#    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "#    coll = LineCollection(lines, colors=colors)\n",
    "#    #coll.set_array(np.random.random(xy.shape[0]))\n",
    "#\n",
    "#    ax.add_collection(coll)\n",
    "#    ax.autoscale_view()\n",
    "#\n",
    "#    plt.title('Input data and cluster assigment using debug data');\n",
    "#    \n",
    "#if run_cfg['debug']:\n",
    "#    plot_debug(input_data, cluster_assignments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-15T21:49:44.420300Z",
     "start_time": "2019-06-15T21:47:32.615Z"
    }
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "# use this to add a different shape to the scatter plot\n",
    "# frames_idx_with_labels[:len(frames_of_interest)][frames_of_interest][run_config['time_series_length'] - 1:]['label'].apply(lambda x: x.value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-15T21:49:44.420993Z",
     "start_time": "2019-06-15T21:47:32.620Z"
    }
   },
   "outputs": [],
   "source": [
    "#cluster_assignments = eval_results[-1]['cluster_assignments']\n",
    "#\n",
    "#group_videos = list(video.group_video_of_clusters(cluster_assignments, y_frames[back_to_single_time], run_cfg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-15T21:49:44.421729Z",
     "start_time": "2019-06-15T21:47:32.625Z"
    }
   },
   "outputs": [],
   "source": [
    "stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-15T21:49:44.422400Z",
     "start_time": "2019-06-15T21:47:32.634Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "#new_im.save('test.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-15T21:49:44.423110Z",
     "start_time": "2019-06-15T21:47:32.639Z"
    }
   },
   "outputs": [],
   "source": [
    "def reverse_pos_pipeline(x, normalisation_factors):\n",
    "    \"\"\"TODO This is again pretty shitty... ultra hidden global variable\"\"\"\n",
    "    return x + normalisation_factors[:x.shape[-1]]\n",
    "\n",
    "def video_prep_raw_data(data):\n",
    "    if run_config['use_time_series']:\n",
    "        return reverse_pos_pipeline(scaler.inverse_transform(data[:, -1, :]).reshape(-1, 15, 2))\n",
    "    else:\n",
    "        return reverse_pos_pipeline(scaler.inverse_transform(data.reshape(-1, 30)).reshape(-1, 15, 2))\n",
    "    \n",
    "def video_prep_recon_data(input_data):\n",
    "    return reverse_pos_pipeline(scaler.inverse_transform(model(input_data).numpy()).reshape(-1, 15, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-15T21:49:44.423821Z",
     "start_time": "2019-06-15T21:47:32.643Z"
    }
   },
   "outputs": [],
   "source": [
    "if run_config['data_type'] == _DATA_TYPE_2D_POS_:\n",
    "    _positional_data_ = [reverse_pos_pipeline(input_data, normalisation_factors=normalisation_factors), \n",
    "                         reverse_pos_pipeline(reconstructed_data, normalisation_factors=normalisation_factors)]\n",
    "else:\n",
    "    raise NotImplementedError('give me a break')\n",
    "    \n",
    "p = video.comparision_video_of_reconstruction(_positional_data_,\n",
    "                                              images_paths_for_experiments=images_paths_for_experiments, \n",
    "                                              n_train=len(data_train),\n",
    "                                              cluster_assignments=cluster_assignments,\n",
    "                                              as_frames=False,\n",
    "                                              exp_desc=exp_desc_short)\n",
    "\n",
    "display_video(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-15T21:49:44.424520Z",
     "start_time": "2019-06-15T21:47:32.648Z"
    }
   },
   "outputs": [],
   "source": [
    "# Super ugly... but necessary...\n",
    "# first there is the time offset due to the slicing\n",
    "# then there is the concatenation of the data...\n",
    "\n",
    "angle_data_pos_to_frame = []\n",
    "\n",
    "for exp_key, data in angle_data_raw: \n",
    "    _exp = SD._experiment_from_key_(exp_key)\n",
    "    \n",
    "    if len(angle_data_pos_to_frame) == 0:\n",
    "        _idx = np.arange(data.shape[0])[run_config['time_series_length'] - 1:]\n",
    "    else:\n",
    "        _idx = np.arange(data.shape[0])# + len(angle_data_pos_to_frame)\n",
    "        \n",
    "    angle_data_pos_to_frame += [(_exp, d) for d in _idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-15T21:49:44.425195Z",
     "start_time": "2019-06-15T21:47:32.657Z"
    }
   },
   "outputs": [],
   "source": [
    "        images_paths_for_experiments = settings.data.EXPERIMENTS.map(lambda x: (x, config.positional_data(x)))\\\n",
    "                                               .flat_map(lambda x: [(x[0], config.get_path_for_image(x[0], i)) for i in range(x[1].shape[1])])\\\n",
    "                                               .to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-15T21:49:44.427012Z",
     "start_time": "2019-06-15T21:47:32.661Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "    frames_idx_with_labels = preprocessing.get_frames_with_idx_and_labels(settings.data.LABELLED_DATA)\n",
    "    frames_of_interest = ~frames_idx_with_labels['label'].isin([settings.data._BehaviorLabel_.REST])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-15T21:49:44.427721Z",
     "start_time": "2019-06-15T21:47:32.666Z"
    }
   },
   "outputs": [],
   "source": [
    "images_paths_for_experiments = [(exp, config.get_path_for_image(exp, i)) for exp, i in angle_data_pos_to_frame]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-15T21:49:44.428396Z",
     "start_time": "2019-06-15T21:47:32.671Z"
    }
   },
   "outputs": [],
   "source": [
    "images_paths_for_experiments[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-15T21:49:44.429194Z",
     "start_time": "2019-06-15T21:47:32.681Z"
    }
   },
   "outputs": [],
   "source": [
    "reload(video)\n",
    "from collections import OrderedDict\n",
    "_N_CLUSTER_TO_VIZ_ = 10\n",
    "_t = [(misc.flatten(sequences), cluster_id) for cluster_id, sequences in video.group_by_cluster(cluster_assignments).items()]\n",
    "_t = sorted(_t, key=lambda x: len(x[0]), reverse=True)\n",
    "\n",
    "cluster_colors = sns.color_palette(n_colors=len(np.unique(cluster_assignments)))\n",
    "\n",
    "cluster_vids = OrderedDict((p[1], video.comparision_video_of_reconstruction(input_data,\n",
    "                                                                            cluster_assignments=cluster_assignments,\n",
    "                                                                            images_paths_for_experiments=images_paths_for_experiments,\n",
    "                                                                            n_train=data_train.shape[0],\n",
    "                                                                            cluster_colors=cluster_colors,\n",
    "                                                                            cluster_id_to_visualize=p[1], \n",
    "                                                                            exp_desc=exp_desc_short,\n",
    "                                                                            is_2d=False))\n",
    "                    for p in _t[:_N_CLUSTER_TO_VIZ_])\n",
    "\n",
    "print('cluster_vids: ', cluster_vids.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-15T21:49:44.429902Z",
     "start_time": "2019-06-15T21:47:32.687Z"
    }
   },
   "outputs": [],
   "source": [
    "! cat ./drosoph_vae/helpers/video.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-15T21:49:44.430746Z",
     "start_time": "2019-06-15T21:47:32.693Z"
    }
   },
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "_N_CLUSTER_TO_VIZ_ = 10\n",
    "_t = [(misc.flatten(sequences), cluster_id) for cluster_id, sequences in video.group_by_cluster(cluster_assignments).items()]\n",
    "_t = sorted(_t, key=lambda x: len(x[0]), reverse=True)\n",
    "\n",
    "cluster_colors = sns.color_palette(n_colors=len(np.unique(cluster_assignments)))\n",
    "\n",
    "cluster_vids = OrderedDict((p[1], video.comparision_video_of_reconstruction(_positional_data_,\n",
    "                                                                      cluster_assignments=cluster_assignments,\n",
    "                                                                      images_paths_for_experiments=images_paths_for_experiments,\n",
    "                                                                      n_train=data_train.shape[0],\n",
    "                                                                      cluster_colors=cluster_colors,\n",
    "                                                                      cluster_id_to_visualize=p[1], exp_desc=exp_desc_short))\n",
    "                    for p in _t[:_N_CLUSTER_TO_VIZ_])\n",
    "\n",
    "print('cluster_vids: ', cluster_vids.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-15T21:49:44.431298Z",
     "start_time": "2019-06-15T21:47:32.698Z"
    }
   },
   "outputs": [],
   "source": [
    "#c_idx = 0\n",
    "c_idx += 1\n",
    "display_video(list(cluster_vids.values())[c_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-15T21:49:44.432029Z",
     "start_time": "2019-06-15T21:47:32.711Z"
    }
   },
   "outputs": [],
   "source": [
    "c_idx = 0\n",
    "#c_idx += 1\n",
    "display_video(list(cluster_vids.values())[c_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-15T21:49:44.432713Z",
     "start_time": "2019-06-15T21:47:32.715Z"
    }
   },
   "outputs": [],
   "source": [
    "images_paths_for_experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-15T21:49:44.433727Z",
     "start_time": "2019-06-15T21:47:32.720Z"
    }
   },
   "outputs": [],
   "source": [
    "len(np.where(cluster_assignments == 11)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-15T21:49:44.434709Z",
     "start_time": "2019-06-15T21:47:32.726Z"
    }
   },
   "outputs": [],
   "source": [
    "np.array(images_paths_for_experiments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-15T21:49:44.435542Z",
     "start_time": "2019-06-15T21:47:32.730Z"
    }
   },
   "outputs": [],
   "source": [
    "for fs, c in _t:\n",
    "    print(f\"cluster {c} has {len(fs)} elements\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-15T21:49:44.436182Z",
     "start_time": "2019-06-15T21:47:32.736Z"
    }
   },
   "outputs": [],
   "source": [
    "reload(video)\n",
    "\n",
    "_t = [(misc.flatten(sequences), cluster_id) for cluster_id, sequences in video.group_by_cluster(cluster_assignments).items()]\n",
    "_t = sorted(_t, key=lambda x: len(x[0]), reverse=True)\n",
    "p = video.video_angle(cluster_assignments, images_paths_for_experiments, cluster_id_to_visualize=_t[3][1], exp_desc=exp_desc_short)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-15T21:49:44.436772Z",
     "start_time": "2019-06-15T21:47:32.740Z"
    }
   },
   "outputs": [],
   "source": [
    "display_video(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-15T21:49:44.437369Z",
     "start_time": "2019-06-15T21:47:32.749Z"
    }
   },
   "outputs": [],
   "source": [
    "stop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-29T08:18:01.614372Z",
     "start_time": "2019-05-29T08:18:01.610583Z"
    }
   },
   "source": [
    "# Convolution Clarification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are the results shown for a Conv1d for all padding options:\n",
    "\n",
    "- valid: only convolutions where the kernel fits inside the input are comptued\n",
    "- causal: input is shifted such that the kernel can only see itself and backwards in time\n",
    "- same: input is padded such that the convolution can also be applied to the border cases\n",
    "\n",
    "kernel sizes of 2 & 3, and dilation rates for 1 to 3.\n",
    "\n",
    "The result is that a valid convolution of kernel size 2 with a dilation factor of 1 compresses the input in a for us good way.\n",
    "The data goes from `[batch_size, n_time_steps, n_channels]` to `[batch_size, n_time_steps - 1, n_filters]` \n",
    "and crops the first time step only. Thus building features by only looking backwards in time,\n",
    "dropping the first-time step. Thus features are build over time and space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-15T21:49:44.438046Z",
     "start_time": "2019-06-15T21:47:32.755Z"
    }
   },
   "outputs": [],
   "source": [
    "example_data = np.zeros((1, 10, 5), dtype=np.float32)\n",
    "\n",
    "for row in range(example_data.shape[1]):\n",
    "    example_data[:, row, :] = row\n",
    "    \n",
    "example_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-15T21:49:44.438684Z",
     "start_time": "2019-06-15T21:47:32.761Z"
    }
   },
   "outputs": [],
   "source": [
    "def conv_clarification_kernel(kernel_size):\n",
    "    conv1d_kernel_no_time = np.zeros((kernel_size, example_data.shape[-1], 1), dtype=np.float32)\n",
    "    conv1d_kernel_no_time[0, :, :] = .5\n",
    "    conv1d_kernel_no_time[1, :, :] = 1.\n",
    "    \n",
    "    if kernel_size == 3:\n",
    "        conv1d_kernel_no_time[2, :, :] = 0.1\n",
    "    \n",
    "    return conv1d_kernel_no_time\n",
    "\n",
    "\n",
    "for kernel_size in range(2, 4):\n",
    "    print(f\"data\\n{example_data}\")\n",
    "    print(f\"kernel\\n{conv_clarification_kernel(kernel_size)}\")\n",
    "    for padding in ['valid', 'causal', 'same']:\n",
    "        for dilation in range(1, 4):\n",
    "            example_conv1d = tfkl.Conv1D(filters=1, \n",
    "                                         kernel_size=kernel_size,\n",
    "                                         use_bias=False, \n",
    "                                         padding=padding,\n",
    "                                         dilation_rate=dilation,\n",
    "                                         kernel_initializer=tf.constant_initializer(conv_clarification_kernel(kernel_size)))\n",
    "\n",
    "            conv_res = example_conv1d(example_data).numpy()\n",
    "            print(f\"padding: {padding}, dilation_rate: {dilation}, kernel_size: {kernel_size}, output shape: {conv_res.shape}\\n{conv_res}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-15T21:49:44.439278Z",
     "start_time": "2019-06-15T21:47:32.767Z"
    }
   },
   "outputs": [],
   "source": [
    "def conv_clarification_kernel(kernel_size):\n",
    "    conv1d_kernel_no_time = np.zeros((kernel_size, example_data.shape[-1], example_conv1d_n_filters), dtype=np.float32)\n",
    "    conv1d_kernel_no_time[0, :, :] = .5\n",
    "    conv1d_kernel_no_time[1, :, :] = 1.\n",
    "    \n",
    "    if kernel_size == 3:\n",
    "        conv1d_kernel_no_time[2, :, :] = 0.1\n",
    "    \n",
    "    return conv1d_kernel_no_time\n",
    "\n",
    "kernel_size = 2\n",
    "padding = 'valid'\n",
    "dilation_rate = 1\n",
    "example_conv1d_n_filters = 2\n",
    "\n",
    "print(f\"data\\n{example_data}\")\n",
    "print(f\"kernel\\n{conv_clarification_kernel(kernel_size)}\")\n",
    "example_conv1d = tfkl.Conv1D(filters=example_conv1d_n_filters, \n",
    "                             kernel_size=kernel_size,\n",
    "                             use_bias=False, \n",
    "                             padding=padding,\n",
    "                             dilation_rate=dilation_rate,\n",
    "                             kernel_initializer=tf.constant_initializer(conv_clarification_kernel(kernel_size)))\n",
    "\n",
    "example_max_pooling_layer = tfkl.MaxPool1D()\n",
    "example_dense = tfkl.Dense(2, use_bias=False, kernel_initializer='ones')\n",
    "\n",
    "conv_res = example_conv1d(example_data[:,:2,:]).numpy()\n",
    "#max_pool_res = example_max_pooling_layer(conv_res)\n",
    "#dense_res = example_dense(max_pool_res)\n",
    "print(f\"padding: {padding}, dilation_rate: {dilation_rate}, kernel_size: {kernel_size}, output shape: {conv_res.shape}\")\n",
    "print('conv\\n', conv_res)\n",
    "#print('max pool\\n', max_pool_res.numpy())\n",
    "#print('dense\\n', dense_res.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-15T21:49:44.439829Z",
     "start_time": "2019-06-15T21:47:32.772Z"
    }
   },
   "outputs": [],
   "source": [
    "conv_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-15T21:49:44.440447Z",
     "start_time": "2019-06-15T21:47:32.777Z"
    }
   },
   "outputs": [],
   "source": [
    "class Conv1D_Transpose(tfkl.Layer):\n",
    "    def __init__(self, n_filters, kernel_size, batch_size):\n",
    "        super(Conv1D_Transpose, self).__init__()        \n",
    "        self.n_filters = n_filters\n",
    "        self.batch_size = batch_size\n",
    "        self.conv2d_transpose = tfkl.Conv2DTranspose(filters=n_filters, kernel_size=kernel_size, strides=2, padding='valid', kernel_initializer='ones')\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        x = tf.reshape(inputs, [self.batch_size, 1, *inputs.shape[1:]])\n",
    "        print(x.shape)\n",
    "        x = self.conv2d_transpose(x)\n",
    "        #x = tf.reshape(x, [self.batch_size, -1, self.n_filters])\n",
    "        \n",
    "        return x\n",
    "\n",
    "example_deconv1d = Conv1D_Transpose(n_filters=2, kernel_size=2, batch_size=1)\n",
    "example_deconv1d(conv_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-15T21:49:44.441302Z",
     "start_time": "2019-06-15T21:47:32.783Z"
    }
   },
   "outputs": [],
   "source": [
    "_ted = example_deconv1d(conv_res)\n",
    "tf.reshape(_ted, _ted.shape[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-15T21:49:44.442047Z",
     "start_time": "2019-06-15T21:47:32.788Z"
    }
   },
   "outputs": [],
   "source": [
    "UpsamplingConv(2)(conv_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-15T21:49:44.442642Z",
     "start_time": "2019-06-15T21:47:32.793Z"
    }
   },
   "outputs": [],
   "source": [
    "tfkl.UpSampling1D(3)(conv_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-15T21:49:44.443311Z",
     "start_time": "2019-06-15T21:47:32.798Z"
    }
   },
   "outputs": [],
   "source": [
    "class UpsamplingConv(tfkl.Layer):\n",
    "    def __init__(self, n_filters, kernel_size=2):\n",
    "        super(UpsamplingConv, self).__init__()\n",
    "        \n",
    "        self.n_filters = n_filters\n",
    "        self.kernel_size = kernel_size\n",
    "    \n",
    "    def call(self, x): \n",
    "        x = tfkl.UpSampling1D(3)(x) # upscale with 3 so that we can again apply `valid` padding and \"reverse\" the encoder\n",
    "        print(x.shape)\n",
    "        # TODO maybe add some fancy flipping of the input\n",
    "        x = tfkl.Conv1D(self.n_filters, self.kernel_size, padding='valid')(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-15T21:49:44.443932Z",
     "start_time": "2019-06-15T21:47:32.803Z"
    }
   },
   "outputs": [],
   "source": [
    "conv_res.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-15T21:49:44.444616Z",
     "start_time": "2019-06-15T21:47:32.808Z"
    }
   },
   "outputs": [],
   "source": [
    "conv_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-29T17:55:51.831670Z",
     "start_time": "2019-05-29T17:55:51.802162Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-15T21:49:44.445232Z",
     "start_time": "2019-06-15T21:47:32.814Z"
    }
   },
   "outputs": [],
   "source": [
    "example_deconv = tfkl.Conv2DTranspose(1, 2, kernel_initializer='ones')\n",
    "example_deconv(conv_res.reshape(-1, 1, *conv_res.shape[1:])).numpy().reshape(-1, *conv_res.shape[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-15T21:49:44.445792Z",
     "start_time": "2019-06-15T21:47:32.819Z"
    }
   },
   "outputs": [],
   "source": [
    "conv_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-15T21:49:44.446421Z",
     "start_time": "2019-06-15T21:47:32.824Z"
    }
   },
   "outputs": [],
   "source": [
    "conv_res.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-15T21:49:44.446967Z",
     "start_time": "2019-06-15T21:47:32.830Z"
    }
   },
   "outputs": [],
   "source": [
    "conv_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-15T21:49:44.447508Z",
     "start_time": "2019-06-15T21:47:32.834Z"
    }
   },
   "outputs": [],
   "source": [
    "tf.rank(conv_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-15T21:49:44.448057Z",
     "start_time": "2019-06-15T21:47:32.840Z"
    }
   },
   "outputs": [],
   "source": [
    "paddings = [[r, 0] for r in range(3)]\n",
    "paddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-15T21:49:44.448616Z",
     "start_time": "2019-06-15T21:47:32.845Z"
    }
   },
   "outputs": [],
   "source": [
    "tf.pad(conv_res, [[0, 0], [0, 1], [0, 0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-15T21:49:44.449211Z",
     "start_time": "2019-06-15T21:47:32.851Z"
    }
   },
   "outputs": [],
   "source": [
    "tfc.nn.conv1d_transpose(input=conv_res, filters=np.ones((2, 2, 2), dtype=np.float32), output_shape=[1, 2, 2], strides=1, padding='VALID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-15T21:49:44.449942Z",
     "start_time": "2019-06-15T21:47:32.856Z"
    }
   },
   "outputs": [],
   "source": [
    "_pdc1dt = PaddedConv1dTransposed(n_filters=2)\n",
    "print(conv_res.shape)\n",
    "resc1 = _pdc1dt(conv_res)\n",
    "print(resc1.shape)\n",
    "resc1.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-15T21:49:44.450986Z",
     "start_time": "2019-06-15T21:47:32.861Z"
    }
   },
   "outputs": [],
   "source": [
    "_pdc1dt(_pdc1dt(resc1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-15T21:49:44.451754Z",
     "start_time": "2019-06-15T21:47:32.868Z"
    }
   },
   "outputs": [],
   "source": [
    "#_t_layer_sizes_generative=[4,6,8,10,12,14,16,18]\n",
    "#_t_layer_sizes_generative=[1] * 6\n",
    "#_t_upsampling_size = [4] * 6 #, 2, 2]\n",
    "#_t_strides = [2] * 6\n",
    "##_t_padding = ['valid', 'valid', 'same']\n",
    "##_t_layer_sizes_generative=[4, 8, 16]\n",
    "#_latent_dim = 2\n",
    "#_t_generative_net = tf.keras.Sequential([tfkl.InputLayer(input_shape=(_latent_dim,)),\n",
    "#                                           tfkl.Lambda(lambda x: tf.reshape(x, [1000, 1, _latent_dim])),\n",
    "#                                           *[TemporalUpsamplingConv(conv_n_filters=fs, \n",
    "#                                                                    upsampling_size=us,\n",
    "#                                                                    conv_strides=s,\n",
    "#                                                                    conv_padding='valid',\n",
    "#                                                                    name=f\"gen_conv_{i}\") for i, (fs, us, s) \n",
    "#                                             in enumerate(zip(_t_layer_sizes_generative,\n",
    "#                                                              _t_upsampling_size,\n",
    "#                                                              _t_strides,\n",
    "#                                                             ))]],\n",
    "#                                          name='generative_net')\n",
    "#\n",
    "#_t_generative_net.summary()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "default_view": {},
   "name": "cvae.ipynb",
   "private_outputs": true,
   "provenance": [
    {
     "file_id": "1eb0NOTQapkYs3X0v-zL1x5_LFKgDISnp",
     "timestamp": 1527173385672
    }
   ],
   "toc_visible": true,
   "version": "0.3.2",
   "views": {}
  },
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {},
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "threshold": 4,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
