{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0TD5ZrvEMbhZ"
   },
   "source": [
    "# VAE using the reparametrization trick"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "heading_collapsed": true,
    "id": "e1_Y75QXJS6h"
   },
   "source": [
    "## Imports and enabling of eager execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-09T15:38:44.321184Z",
     "start_time": "2019-06-09T15:38:42.342660Z"
    },
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "hidden": true,
    "id": "YfIk2es3hJEd"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from collections import namedtuple\n",
    "import itertools\n",
    "import warnings\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import PIL\n",
    "import imageio\n",
    "from IPython import display\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.manifold import TSNE\n",
    "from functional import seq\n",
    "from pathlib import Path\n",
    "from functools import reduce\n",
    "\n",
    "from importlib import reload # for debugging and developing, optional\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib as tfc\n",
    "import tensorflow.contrib.eager as tfe\n",
    "import tensorflow.keras as tfk\n",
    "import tensorflow.keras.layers as tfkl\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "# for the KL-loss explosion problem\n",
    "from tensorflow.python.eager.execution_callbacks import InfOrNanError\n",
    "from tensorflow.python.eager.core import _NotOkStatusException\n",
    "\n",
    "tf.enable_eager_execution()\n",
    "# we currently handle them ourselves. but with this, it will throw an error before we can apply the fix\n",
    "tfe.seterr(inf_or_nan='raise')\n",
    "\n",
    "# otherwise TF will print soooo many warnings\n",
    "warnings.filterwarnings('ignore', '.*FutureWarning.*np.complexfloating.*')\n",
    "\n",
    "from som_vae.helpers.tensorflow import _TF_DEFAULT_SESSION_CONFIG_\n",
    "import som_vae.helpers.tensorflow as tf_helpers\n",
    "sess = tf.InteractiveSession(config=_TF_DEFAULT_SESSION_CONFIG_)\n",
    "tf.keras.backend.set_session(sess)\n",
    "\n",
    "from som_vae import settings\n",
    "from som_vae import preprocessing\n",
    "from som_vae.helpers.misc import extract_args, chunks, foldl, if_last\n",
    "from som_vae.helpers.jupyter import fix_layout, display_video\n",
    "from som_vae.settings import config, skeleton\n",
    "from som_vae.settings import data as SD\n",
    "from som_vae.helpers import video, plots, misc, jupyter\n",
    "from som_vae import preprocessing\n",
    "from som_vae.helpers.logging import enable_logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-09T15:38:44.326321Z",
     "start_time": "2019-06-09T15:38:44.322736Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "jupyter.fix_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants (Settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-09T15:38:44.587663Z",
     "start_time": "2019-06-09T15:38:44.327674Z"
    }
   },
   "outputs": [],
   "source": [
    "# all those experiments and data will be used\n",
    "from som_vae.settings import config\n",
    "print(f\"this is the main experiment, study, and fly id: {config.full_experiment_id()}.\\n\\nloadable experiments. there is a blacklist below.\")\n",
    "!ls $config.__EXPERIMENT_ROOT__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-09T15:38:44.595039Z",
     "start_time": "2019-06-09T15:38:44.591842Z"
    }
   },
   "outputs": [],
   "source": [
    "# if you want to see the flys as well, or just more information\n",
    "# !tree -L 2 $config.__EXPERIMENT_ROOT__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-09T15:38:45.564734Z",
     "start_time": "2019-06-09T15:38:44.597658Z"
    }
   },
   "outputs": [],
   "source": [
    "config.positional_data(SD.EXPERIMENTS[0]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-09T17:04:26.978404Z",
     "start_time": "2019-06-09T17:04:26.962241Z"
    }
   },
   "outputs": [],
   "source": [
    "_EXPERIMENT_BLACK_LIST_ = ['181220_Rpr_R57C10_GC6s_tdTom'] # all other experiments are used\n",
    "_FLY_BLACK_LIST_ = ['180920_aDN_PR-Fly2-005_SG1', '180921_aDN_CsCh-Fly6-003_SG1'] # for those flys the angle conversion give odd results,\n",
    "                                                                                  # and their distributions seem way off compared to the others)\n",
    "\n",
    "\n",
    "_DATA_TYPE_3D_ANGLE_ = '3d_angle'\n",
    "_DATA_TYPE_2D_POS_ = '2d_pos'\n",
    "_SUPPORTED_DATA_TYPES_ = [_DATA_TYPE_3D_ANGLE_, _DATA_TYPE_2D_POS_]\n",
    "\n",
    "_MODEL_IMPL_TEMPORAL_CONV_ = 'temp_conv'\n",
    "_MODEL_IMPL_PADD_CONV_ = 'padd_conv'\n",
    "_MODEL_IMPL_SKIP_CON_ = 'skip_padd_conv' \n",
    "\n",
    "run_config = {\n",
    "    'debug': True,                 # general flag for debug mode, triggers all `d_.*`-options.\n",
    "    'd_zero_data': False,          # overwrite the data with zeroed out data, the overall shape is kept.\n",
    "    'd_sinoid_data': False, \n",
    "    'd_sinoid_cluster_data': True,\n",
    "    'd_no_compression': False,     # if true, the latent_space will be the same dimension as the input. allowing the model to learn the identity function.\n",
    "    'use_all_experiments': False,\n",
    "    'data_type': _DATA_TYPE_3D_ANGLE_,\n",
    "    'use_time_series': True,       # triggers time series application, without this the model is only dense layers\n",
    "    'time_series_length': 16,      # note that this is equal to the minimal wanted receptive field length\n",
    "    'conv_layer_kernel_size': 2,   # you can set either this or `n_conv_layers` to None, it will be automatically computed. see section `Doc` for an explanation.\n",
    "    'n_conv_layers': None,         # you can set either this or `conv_layer_kernel_size` to None, it will be automatically computed. see section `Doc` for an explanation.\n",
    "    'latent_dim': None,               # should be adapted given the input dim\n",
    "    'batch_size': 128,\n",
    "    'loss_weight_reconstruction': 1.0, # will be adjusted further down (currently)\n",
    "    'loss_weight_kl': 0.0,             # will be adjusted further down (currently)\n",
    "    'dropout_rate': 0.,\n",
    "    'with_batch_norm': True,\n",
    "    'model_impl': _MODEL_IMPL_SKIP_CON_\n",
    "}\n",
    "\n",
    "# And now the ... ugly parts begin. -> TODO put this in a class, \n",
    "\n",
    "if run_config['use_all_experiments']:\n",
    "    # takes way too long otherwise\n",
    "    run_config['batch_size'] = 1024\n",
    "\n",
    "if not(run_config['data_type'] in _SUPPORTED_DATA_TYPES_):\n",
    "    raise NotImplementedError(f\"This data type is not supported. Must be one of either {_SUPPORTED_DATA_TYPES_}\")\n",
    "    \n",
    "if run_config['n_conv_layers'] is None:\n",
    "    run_config['n_conv_layers'] = np.int(np.ceil(np.log2((run_config['time_series_length'] - 1) / (2 * (run_config['conv_layer_kernel_size'] - 1)) + 1)))\n",
    "\n",
    "if run_config['conv_layer_kernel_size'] is None:\n",
    "    raise NotImplementedError('ups')\n",
    "    \n",
    "if run_config['data_type'] == _DATA_TYPE_2D_POS_:\n",
    "    # goes from 15 * 2 = 30 -> 8\n",
    "    run_config['latent_dim'] = 8\n",
    "elif run_config['data_type'] == _DATA_TYPE_3D_ANGLE_:\n",
    "    # goes from 18 -> 4\n",
    "    run_config['latent_dim'] = 4\n",
    "else:\n",
    "    raise ValueError(f\"this data_type is not supported: {run_config['data_type']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iYn4MdZnKCey"
   },
   "source": [
    "## Loading of 2d positional data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-09T17:04:27.952057Z",
     "start_time": "2019-06-09T17:04:27.941299Z"
    }
   },
   "outputs": [],
   "source": [
    "def experiments_from_root(root=config.__EXPERIMENT_ROOT__):\n",
    "    return seq(Path(root).iterdir()).flat_map(lambda p: (c for c in p.iterdir() if c.is_dir()))\\\n",
    "                                    .flat_map(lambda p: (c for c in p.iterdir() if c.is_dir()))\\\n",
    "                                    .map(lambda p: reduce(lambda acc, el: ([*acc[0], acc[1].stem], acc[1].parent), range(3), ([], p)))\\\n",
    "                                    .map(lambda pair: list(reversed(pair[0])))\\\n",
    "                                    .map(lambda ls: SD.Experiment._make([*ls, SD._key_from_list_(ls)]))\\\n",
    "                                    .to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-09T19:13:45.610269Z",
     "start_time": "2019-06-09T19:13:45.605574Z"
    }
   },
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "\n",
    "Data = namedtuple('Data', 'joint_positions normalisation_factors image_paths labels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-09T19:11:34.731262Z",
     "start_time": "2019-06-09T19:11:34.725271Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "#def load_2d_pos(include_rest_data=False, use_single_fly=True):\n",
    "#    \"\"\"load_2d_pos loads the 2d-positional data\n",
    "#    \n",
    "#    Parameters\n",
    "#        include_rest_data (bool): set to true if you want to include `resting` fly behaviour as well,\n",
    "#            ignored when `use_all_experiments` == True\n",
    "#        use_single_fly (bool): if True only data from one fly, but multiple experiments are used\n",
    "#            if True then there will be no labels\n",
    "#    Returns:\n",
    "#        A `Data` object, which is a namedtuple\n",
    "#    \n",
    "#    \"\"\"\n",
    "#    if use_single_fly:\n",
    "#        joint_positions, normalisation_factors = preprocessing.get_data_and_normalization(settings.data.EXPERIMENTS, normalize_data=True)\n",
    "#        \n",
    "#        frames_idx_with_labels = preprocessing.get_frames_with_idx_and_labels(settings.data.LABELLED_DATA)\n",
    "#        labels = frames_idx_with_labels['label'].apply(lambda x: x.value)\n",
    "#        if use_single_fly:\n",
    "#            frames_of_interest = ~frames_idx_with_labels['label'].isin([settings.data._BehaviorLabel_.REST])\n",
    "#            labels = labels[frames_idx_with_labels]\n",
    "#            \n",
    "#        images_paths_for_experiments = settings.data.EXPERIMENTS.map(lambda x: (x, config.positional_data(x)))\\\n",
    "#                                               .flat_map(lambda x: [(x[0], config.get_path_for_image(x[0], i)) for i in range(x[1].shape[1])])\\\n",
    "#                                               .to_list()\n",
    "#        \n",
    "#        if len(frames_of_interest) != len(joint_positions):\n",
    "#            warnings.warn('There is a bug here. The number of images and number of data points to NOT align.')\n",
    "#            frames_of_interest = np.where(frames_of_interest[:len(joint_positions)])[0]\n",
    "#        \n",
    "#    else:\n",
    "#        all_experiments = [e for e in experiments_from_root() if e.study_id not in _EXPERIMENT_BLACK_LIST_ or config.get_experiment_id(e) in _FLY_BLACK_LIST_]\n",
    "#        joint_positions, normalisation_factors = preprocessing.get_data_and_normalization(all_experiments, normalize_data=True)\n",
    "#        labels = None\n",
    "#            \n",
    "#    if run_config['use_all_experiments']:\n",
    "#        all_experiments = [e for e in experiments_from_root() if e.study_id not in _EXPERIMENT_BLACK_LIST_ or config.get_experiment_id(e) in _FLY_BLACK_LIST_]\n",
    "#        joint_positions, normalisation_factors = preprocessing.get_data_and_normalization(all_experiments, normalize_data=True)\n",
    "#    else:\n",
    "#        joint_positions, normalisation_factors = preprocessing.get_data_and_normalization(settings.data.EXPERIMENTS, normalize_data=True)\n",
    "#\n",
    "#        images_paths_for_experiments = settings.data.EXPERIMENTS.map(lambda x: (x, config.positional_data(x)))\\\n",
    "#                                               .flat_map(lambda x: [(x[0], config.get_path_for_image(x[0], i)) for i in range(x[1].shape[1])])\\\n",
    "#                                               .to_list()\n",
    "#\n",
    "#        if len(frames_of_interest) != len(joint_positions):\n",
    "#            warnings.warn('There is a bug here. The number of images and number of data points to NOT align.')\n",
    "#            frames_of_interest = np.where(frames_of_interest[:len(joint_positions)])[0]\n",
    "#        \n",
    "#        joint_positions = joint_positions[frames_of_interest[:len(joint_positions)]]\n",
    "#        frames_idx_with_labels = frames_idx_with_labels.iloc[frames_of_interest]\n",
    "#        images_paths_for_experiments =  np.array(images_paths_for_experiments)[frames_of_interest].tolist()\n",
    "#            \n",
    "#    \n",
    "#    return Data(joint_positions, normalisation_factors, image_paths, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-09T17:04:28.144469Z",
     "start_time": "2019-06-09T17:04:28.128448Z"
    }
   },
   "outputs": [],
   "source": [
    "if not run_config['use_all_experiments']:\n",
    "    frames_idx_with_labels = preprocessing.get_frames_with_idx_and_labels(settings.data.LABELLED_DATA)\n",
    "    frames_of_interest = ~frames_idx_with_labels['label'].isin([settings.data._BehaviorLabel_.REST])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-09T17:04:28.791840Z",
     "start_time": "2019-06-09T17:04:28.778813Z"
    }
   },
   "outputs": [],
   "source": [
    "# TODO form a wrapper around the used data, experiments (the ids), data, normalisation factor, images, ... a namedtuple should do the trick\n",
    "if run_config['data_type'] == _DATA_TYPE_2D_POS_:\n",
    "    if run_config['use_all_experiments']:\n",
    "        all_experiments = [e for e in experiments_from_root() if e.study_id not in _EXPERIMENT_BLACK_LIST_ or config.get_experiment_id(e) in _FLY_BLACK_LIST_]\n",
    "        joint_positions, normalisation_factors = preprocessing.get_data_and_normalization(all_experiments, normalize_data=True)\n",
    "    else:\n",
    "        joint_positions, normalisation_factors = preprocessing.get_data_and_normalization(settings.data.EXPERIMENTS, normalize_data=True)\n",
    "\n",
    "        images_paths_for_experiments = settings.data.EXPERIMENTS.map(lambda x: (x, config.positional_data(x)))\\\n",
    "                                               .flat_map(lambda x: [(x[0], config.get_path_for_image(x[0], i)) for i in range(x[1].shape[1])])\\\n",
    "                                               .to_list()\n",
    "\n",
    "        if len(frames_of_interest) != len(joint_positions):\n",
    "            warnings.warn('There is a bug here. The number of images and number of data points to NOT align.')\n",
    "            frames_of_interest = np.where(frames_of_interest[:len(joint_positions)])[0]\n",
    "        \n",
    "        joint_positions = joint_positions[frames_of_interest[:len(joint_positions)]]\n",
    "        frames_idx_with_labels = frames_idx_with_labels.iloc[frames_of_interest]\n",
    "        images_paths_for_experiments =  np.array(images_paths_for_experiments)[frames_of_interest].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## loading of angle-data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-09T19:02:04.029556Z",
     "start_time": "2019-06-09T19:01:59.274031Z"
    }
   },
   "outputs": [],
   "source": [
    "if run_config['data_type'] == _DATA_TYPE_3D_ANGLE_ and not run_config['use_all_experiments']:\n",
    "    data_angle = np.vstack(seq(settings.data.EXPERIMENTS).map(lambda x: settings.config.positional_data(x, dimensions='3d')))\n",
    "\n",
    "    #\n",
    "    # Using man-made selection (from Semigh)\n",
    "    #\n",
    "    data_angle_raw = SD.convert_3d_to_angle(data_angle)\n",
    "    warnings.warn('There is a bug here. The number of images and number of data points to NOT align.')\n",
    "    frames_of_interest = frames_of_interest[:len(data_angle_raw)]\n",
    "    \n",
    "    selected_cols = [2,7,12, 19+2, 19+4, 19+12]\n",
    "    angled_data_columns = SD.get_3d_columns_names(selected_cols)\n",
    "    # for some odd reason numpy complains with I do data_angle_raw[frames_of_interest, selected_cols]\n",
    "    #plots.plot_angle_columns(data_angle_raw[:, selected_cols][frames_of_interest], angled_data_columns);\n",
    "    \n",
    "    # But not all of this data has information in it (measured by variance),\n",
    "    # so we use a different selection\n",
    "\n",
    "    #\n",
    "    # column selection \n",
    "    #\n",
    "    threshold = 0.\n",
    "    selected_cols = np.where(np.var(data_angle_raw, axis=0) > threshold)[0]\n",
    "    \n",
    "    angled_data_columns = SD.get_3d_columns_names(selected_cols)\n",
    "    #f = plots.plot_angle_columns(data_angle_raw[:, selected_cols][frames_of_interest], angled_data_columns)\n",
    "    #f.suptitle(f\"threshold: {threshold}, {len(selected_cols)} selected\");\n",
    "    #plt.subplots_adjust(top=0.97)\n",
    "\n",
    "    # TODO not so sure here, should we really normalize the data?\n",
    "    joint_positions, normalisation_factors = preprocessing.normalize(data_angle_raw[:, selected_cols][frames_of_interest])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-09T17:04:34.007694Z",
     "start_time": "2019-06-09T17:04:34.003770Z"
    }
   },
   "outputs": [],
   "source": [
    "if run_config['data_type'] == _DATA_TYPE_3D_ANGLE_ and run_config['use_all_experiments']:\n",
    "    all_experiments = [e for e in experiments_from_root() if (e.study_id not in _EXPERIMENT_BLACK_LIST_) and (e.key not in _FLY_BLACK_LIST_)]\n",
    "    # `per_experiment` is a shitty parameter name, the data is not normalised and return per experiment.\n",
    "    loading_kwargs = {'dimensions': '3d', 'return_with_experiment_id': True}\n",
    "    angle_data_raw = [(exp_id, SD.convert_3d_to_angle(d)) for exp_id, d in preprocessing.get_data_and_normalization(all_experiments, **loading_kwargs)]\n",
    "\n",
    "    # takes for ever to render, if you want to see this, please run it yourself\n",
    "    #plots.plot_distribution_of_angle_data(angle_data_raw, run_config=run_config);\n",
    "\n",
    "    exp_ids, angle_data  = zip(*angle_data_raw)\n",
    "    angle_data = np.vstack(angle_data)\n",
    "    selected_cols = np.where(np.var(angle_data, axis=0) > 0.0)[0]\n",
    "    joint_positions = angle_data[:, selected_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-09T17:04:34.021037Z",
     "start_time": "2019-06-09T17:04:34.009032Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# will generate a huge plot and take about 6min to run...\n",
    "#plots.plot_distribution_of_angle_data(angle_data_raw, run_config=run_config);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-09T17:04:34.037864Z",
     "start_time": "2019-06-09T17:04:34.022553Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "#all_experiments = experiments_from_root()\n",
    "#print(len(all_experiments))\n",
    "#\n",
    "#pos_data = preprocessing.get_data_and_normalization(all_experiments, per_experiment=True)\n",
    "#\n",
    "#norm_pos_data, norm_pos_data_params = zip(*[preprocessing.normalize(p) for p in pos_data])\n",
    "#experiment_lengths = [len(p) for p in norm_pos_data] # for applying the right normalization factors\n",
    "#norm_pos_data = np.vstack(norm_pos_data)\n",
    "#\n",
    "#print(f\"in total we have {len(all_experiments)} experiments, but only {len(experiment_lengths)} are usable right now\")\n",
    "#\n",
    "#norm_pos_data_embedded = TSNE(n_components=2, random_state=42).fit_transform(norm_pos_data[:, :, :2].reshape(norm_pos_data.shape[0], -1))\n",
    "#\n",
    "##_cs = sns.color_palette(n_colors=len(seen_labels))\n",
    "##\n",
    "##fig = plt.figure(figsize=(10, 10))\n",
    "##_all_frames_ = pd.concat((training_frames, testing_frames))\n",
    "##\n",
    "##behaviour_colours = dict(zip(seen_labels, _cs))\n",
    "##\n",
    "##for l, c in behaviour_colours.items():\n",
    "##    _d = X_embedded[_all_frames_['label'] == l]\n",
    "##    # c=[c] since matplotlib asks for it\n",
    "##    plt.scatter(_d[:, 0], _d[:,1], c=[c], label=l.name, marker='.')\n",
    "##    \n",
    "##plt.legend()\n",
    "##plt.title('simple t-SNE on latent space')\n",
    "##fig.savefig(f\"../neural_clustering_data/figures/{som_vae_config['ex_name']}_tsne.png\")\n",
    "#\n",
    "#_cs = sns.color_palette(n_colors=len(experiment_lengths))\n",
    "#\n",
    "#\n",
    "#used_until = 0\n",
    "#for i, l in enumerate(experiment_lengths):\n",
    "#    plt.scatter(norm_pos_data_embedded[used_until:used_until+l, 0], norm_pos_data_embedded[used_until:used_until+l, 1], c=[_cs[i]])\n",
    "#    used_until += l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-09T17:04:34.051717Z",
     "start_time": "2019-06-09T17:04:34.039966Z"
    }
   },
   "outputs": [],
   "source": [
    "joint_positions.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-09T17:15:21.409378Z",
     "start_time": "2019-06-09T17:15:21.394656Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def dummy_data_complex_sine_like(length):\n",
    "    DummyBehaviour = namedtuple('DummyBehaviour', 'type amplitude fraction frequency')\n",
    "    # make sure that the fractions add up to 1.\n",
    "    # cluster id, behaviour\n",
    "    _dummy_behaviours_ = [\n",
    "        (0, ('sinoid',  1.0, 0.1, 2)),\n",
    "        (1, ('flat',    0.0, 0.2, 0)),\n",
    "        (2, ('sinoid',  1.0, 0.2, 3)),\n",
    "        (3, ('sinoid',  1.0, 0.1, 5)),\n",
    "        (4, ('flat',    1.0, 0.2, 0)),\n",
    "        (2, ('sinoid',   .5,  .2, 3)),\n",
    "    ]\n",
    "    \n",
    "    \n",
    "    cur_idx = 0\n",
    "    nb_frames = length\n",
    "\n",
    "    _new_frames_ = np.zeros(nb_frames)\n",
    "    _cluster_assignments_ = np.zeros(nb_frames)\n",
    "\n",
    "    for l, db in _dummy_behaviours_:\n",
    "        db = DummyBehaviour(*db)\n",
    "        cur_idx_end = np.int(nb_frames * db.fraction + cur_idx)\n",
    "        idx = np.s_[cur_idx:cur_idx_end]\n",
    "        if db.type == 'sinoid':\n",
    "            _new_frames_[idx] = db.amplitude * np.sin(np.pi * np.linspace(0, 2, cur_idx_end - cur_idx) * db.frequency)\n",
    "        elif db.type == 'flat':\n",
    "            _new_frames_[idx] = db.amplitude\n",
    "            \n",
    "        _cluster_assignments_[idx] = l\n",
    "\n",
    "        cur_idx = cur_idx_end\n",
    "        \n",
    "    return _new_frames_, _cluster_assignments_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-09T17:11:55.987686Z",
     "start_time": "2019-06-09T17:11:55.978977Z"
    }
   },
   "outputs": [],
   "source": [
    "reload(misc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-09T17:19:00.541646Z",
     "start_time": "2019-06-09T17:19:00.406487Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# full preprocessing pipeline\n",
    "\n",
    "# scaling the data to be in [0, 1]\n",
    "# this is due to the sigmoid activation function in the reconstruction (and because ANN train better with normalised data) (which it is not...)\n",
    "#scaler = MinMaxScaler()\n",
    "scaler = StandardScaler()\n",
    "\n",
    "#\n",
    "# reshapping the data \n",
    "#\n",
    "\n",
    "# TODO bring this in order! (or in better order)\n",
    "\n",
    "if run_config['use_time_series']:\n",
    "    # it's the shitty logical combination of these values\n",
    "    # TODO the scaling should be learned on the training data only, but this is a bit tricky due to how we do the time-sequences\n",
    "    # TODO right now the training and testing data are just concatenated time-sequences, experiment overlapping. which is bad.\n",
    "    warnings.warn('this is not proper, fix the bugs here')\n",
    "    if run_config['data_type'] == _DATA_TYPE_2D_POS_:\n",
    "        reshaped_joint_position = scaler.fit_transform(misc.prep_2d_pos_data(joint_positions))\n",
    "    else:\n",
    "        reshaped_joint_position = scaler.fit_transform(joint_positions)\n",
    "        \n",
    "    reshaped_joint_position = misc.to_time_series_np(reshaped_joint_position, sequence_length=run_config['time_series_length'])\n",
    "else:\n",
    "    if run_config['data_type'] == _DATA_TYPE_2D_POS_:\n",
    "        # angle data is already flat\n",
    "        reshaped_joint_position = misc.prep_2d_pos_data(joint_positions)\n",
    "    else:\n",
    "        reshaped_joint_position = joint_positions\n",
    "\n",
    "#\n",
    "# debugging overwrite\n",
    "#\n",
    "    \n",
    "if run_config['debug']:\n",
    "    if run_config['d_zero_data']:\n",
    "        # resetting the scaler to make our life easier down below the pipeline\n",
    "        _dummy_data_ = np.zeros_like(joint_positions)\n",
    "    elif run_config['d_sinoid_data']:\n",
    "        if run_config['data_type'] == _DATA_TYPE_2D_POS_:\n",
    "            _dummy_data_ = np.zeros_like(joint_positions)\n",
    "            for frame in range(_dummy_data_.shape[0]):\n",
    "                for joint in range(_dummy_data_.shape[1]):\n",
    "                    _dummy_data_[frame, joint, :] = np.sin(2 * np.pi * frame/_dummy_data_.shape[0] + joint / _dummy_data_.shape[1])\n",
    "                \n",
    "        else:\n",
    "            _dummy_data_ = np.array([[np.sin(x) + (offset / joint_positions.shape[1]) \n",
    "                                      for x in range(len(joint_positions))] \n",
    "                                     for offset in range(joint_positions.shape[1])]).T.astype(joint_positions.dtype)\n",
    "    elif run_config['d_sinoid_cluster_data']:\n",
    "        if run_config['data_type'] == _DATA_TYPE_2D_POS_:\n",
    "            raise NotImplementedError\n",
    "        else:\n",
    "            _dummy_data_ = np.zeros_like(joint_positions)\n",
    "            _dummy_labels_ = np.zeros(joint_positions.shape[0])\n",
    "            for c in range(_dummy_data_.shape[1]):\n",
    "                _dummy_data_[:, c], _dummy_labels_ = dummy_data_complex_sine_like(_dummy_data_.shape[0])\n",
    "            \n",
    "    if run_config['data_type'] == _DATA_TYPE_2D_POS_:\n",
    "        _dummy_data_ = misc.prep_2d_pos_data(_dummy_data_)\n",
    "        \n",
    "    if run_config['use_time_series']:\n",
    "        reshaped_joint_position = scaler.fit_transform(_dummy_data_)\n",
    "        reshaped_joint_position = misc.to_time_series_np(reshaped_joint_position, sequence_length=run_config['time_series_length'])\n",
    "        labels = _dummy_labels_[run_config['time_series_length'] - 1:]\n",
    "    else:\n",
    "        reshaped_joint_position = _dummy_data_\n",
    "        labels = _dummy_labels_\n",
    "\n",
    "#\n",
    "# split and apply scaler\n",
    "#\n",
    "\n",
    "if reshaped_joint_position.shape[0] > 10**5:\n",
    "    n_of_data_points = int(reshaped_joint_position.shape[0] * 0.9)\n",
    "else:\n",
    "    n_of_data_points = int(reshaped_joint_position.shape[0] * 0.7)\n",
    "\n",
    "if run_config['use_time_series']:\n",
    "    data_train = reshaped_joint_position[:n_of_data_points]\n",
    "    data_test = reshaped_joint_position[n_of_data_points:]\n",
    "    labels_train = labels[:n_of_data_points]\n",
    "    labels_test = labels[n_of_data_points:]\n",
    "    print('train')\n",
    "    display.display(pd.DataFrame(data_train[:, -1, :]).describe())\n",
    "    print('test')\n",
    "    display.display(pd.DataFrame(data_test[:, -1, :]).describe())\n",
    "else:\n",
    "    data_train = scaler.fit_transform(reshaped_joint_position[:n_of_data_points])\n",
    "    data_test = scaler.transform(reshaped_joint_position[n_of_data_points:])\n",
    "    labels_train = labels[:n_of_data_points]\n",
    "    labels_test = labels[n_of_data_points:]\n",
    "    print('train')\n",
    "    display.display(pd.DataFrame(data_train).describe())\n",
    "    print('test')\n",
    "    display.display(pd.DataFrame(data_test).describe())\n",
    "    \n",
    "print(f\"shapes for train/test: {data_train.shape}, {data_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-09T17:19:05.520307Z",
     "start_time": "2019-06-09T17:19:03.971671Z"
    }
   },
   "outputs": [],
   "source": [
    "reload(plots)\n",
    "#\n",
    "# Making sure that the train/test distributions are not too different from each other\n",
    "#\n",
    "if run_config['use_time_series']:\n",
    "    _plt_data_idx_ = np.s_[:, -1, :]\n",
    "else:\n",
    "    _plt_data_idx_ = np.s_[:]\n",
    "    \n",
    "if run_config['data_type'] == _DATA_TYPE_3D_ANGLE_:\n",
    "    fig = plots.plot_3d_angle_data_distribution(data_train[_plt_data_idx_], data_test[_plt_data_idx_], selected_cols, exp_desc=config.config_description(run_config))\n",
    "else:\n",
    "    fig = plots.plot_2d_distribution(data_train[_plt_data_idx_], data_test[_plt_data_idx_], exp_desc=config.config_description(run_config))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PIGN6ouoQxt3"
   },
   "source": [
    "## Use *tf.data* to create batches and shuffle the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-09T17:19:27.409246Z",
     "start_time": "2019-06-09T17:19:27.397957Z"
    },
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "S4PIDhoDLbsZ"
   },
   "outputs": [],
   "source": [
    "def to_tf_data(X, y=None):\n",
    "    if y is None:\n",
    "        return tf.data.Dataset.from_tensor_slices(X).shuffle(len(X)).batch(run_config['batch_size'])\n",
    "    else:\n",
    "        return tf.data.Dataset.from_tensor_slices((X, y)).shuffle(len(X)).batch(run_config['batch_size'])\n",
    "\n",
    "train_dataset = to_tf_data(data_train)\n",
    "test_dataset = to_tf_data(data_test) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "THY-sZMiQ4UV"
   },
   "source": [
    "# model def"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Sources:\n",
    "\n",
    "- https://blog.keras.io/building-autoencoders-in-keras.html (keras autoencoder implementation)\n",
    "- https://medium.com/the-artificial-impostor/notes-understanding-tensorflow-part-3-7f6633fcc7c7 (temporal block)\n",
    "- https://stackoverflow.com/questions/46503816/keras-conv1d-layer-parameters-filters-and-kernel-size (refresher on conv layers)\n",
    "- https://towardsdatascience.com/types-of-convolutions-in-deep-learning-717013397f4d (refresher on conv layers)\n",
    "- https://jeddy92.github.io/JEddy92.github.io/ts_seq2seq_conv/ (for a good overview over diluted causal convolutions)\n",
    "- https://blog.goodaudience.com/introduction-to-1d-convolutional-neural-networks-in-keras-for-time-sequences-3a7ff801a2cf?gi=c5cb3c007035 (general reference)\n",
    "- https://medium.com/tensorflow/variational-autoencoders-with-tensorflow-probability-layers-d06c658931b7 (VAE with tensorflow probability)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-10T13:12:53.469656Z",
     "start_time": "2019-05-10T13:12:53.444967Z"
    },
    "hidden": true
   },
   "source": [
    "### Generative Network\n",
    "This defines the generative model which takes a latent encoding as input, and outputs the parameters for a conditional distribution of the observation, i.e. $p(x|z)$. Additionally, we use a unit Gaussian prior $p(z)$ for the latent variable.\n",
    "\n",
    "### Inference Network\n",
    "This defines an approximate posterior distribution $q(z|x)$, which takes as input an observation and outputs a set of parameters for the conditional distribution of the latent representation. In this example, we simply model this distribution as a diagonal Gaussian. In this case, the inference network outputs the mean and log-variance parameters of a factorized Gaussian (log-variance instead of the variance directly is for numerical stability).\n",
    "\n",
    "### Reparameterization Trick\n",
    "During optimization, we can sample from $q(z|x)$ by first sampling from a unit Gaussian, and then multiplying by the standard deviation and adding the mean. This ensures the gradients could pass through the sample to the inference network parameters.\n",
    "\n",
    "### Network architecture\n",
    "For the inference network, we use two convolutional layers followed by a fully-connected layer. In the generative network, we mirror this architecture by using a fully-connected layer followed by three convolution transpose layers (a.k.a. deconvolutional layers in some contexts). Note, it's common practice to avoid using batch normalization when training VAEs, since the additional stochasticity due to using mini-batches may aggravate instability on top of the stochasticity from sampling.\n",
    "\n",
    "The dilated convolution between signal $f$ and kernel $k$ and dilution factor $l$ is defined as:\n",
    "\n",
    "$$\\left(k \\ast_{l} f\\right)_t = \\sum_{\\tau=-\\infty}^{\\infty} k_\\tau \\cdot f_{t - l\\tau}$$\n",
    "\n",
    "![](./figures/diluted_convolution.png)\n",
    "![](./figures/WaveNet_gif.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "VAEs train by maximizing the evidence lower bound (ELBO) on the marginal log-likelihood:\n",
    "\n",
    "$$\\log p(x) \\ge \\text{ELBO} = \\mathbb{E}_{q(z|x)}\\left[\\log \\frac{p(x, z)}{q(z|x)}\\right].$$\n",
    "\n",
    "In practice, we optimize the single sample Monte Carlo estimate of this expectation:\n",
    "\n",
    "$$\\log p(x| z) + \\log p(z) - \\log q(z|x),$$\n",
    "where $z$ is sampled from $q(z|x)$.\n",
    "\n",
    "**Note**: we could also analytically compute the KL term, but here we incorporate all three terms in the Monte Carlo estimator for simplicity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-09T15:38:55.142941Z",
     "start_time": "2019-06-09T15:38:55.017927Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def _receptive_field_size_temporal_conv_net_(kernel_size, n_layers):\n",
    "    return 1 + 2 * (kernel_size - 1) * (2 ** n_layers - 1)\n",
    "\n",
    "for k in range(2, 5):\n",
    "    plt.plot([_receptive_field_size_temporal_conv_net_(kernel_size=k, n_layers=n) for n in range(10)], label=f\"kernel size: {k}\")\n",
    "plt.xlabel('number of layers')\n",
    "plt.ylabel('receptive field size')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-09T15:38:55.151692Z",
     "start_time": "2019-06-09T15:38:55.144234Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class TemporalBlock(tfkl.Layer):\n",
    "    def __init__(self, filter_size, kernel_size, dilation_rate, dropout=0.2, trainable=True, name=None, dtype=None, activity_regularizer=None, **kwargs):\n",
    "        \"\"\"\n",
    "        In the dilated convolution, the kernel only touches the signal at every lth entry \n",
    "        See https://www.inference.vc/dilated-convolutions-and-kronecker-factorisation/ (some bugs fixed and code adapted to our use case)\n",
    "        \"\"\"\n",
    "        super(TemporalBlock, self).__init__(\n",
    "            trainable=trainable, dtype=dtype,\n",
    "            activity_regularizer=activity_regularizer,\n",
    "            name=name, **kwargs)        \n",
    "        self.dropout = dropout\n",
    "        self.filter_size = filter_size\n",
    "        causal_conv_args = {\"padding\": \"causal\",\n",
    "                            \"dilation_rate\": dilation_rate, \n",
    "                            \"activation\": tf.nn.leaky_relu}\n",
    "        self.conv1 = tfkl.Conv1D(filter_size, kernel_size, **causal_conv_args, name=\"conv1\")\n",
    "        self.conv2 = tfkl.Conv1D(filter_size, kernel_size, **causal_conv_args, name=\"conv2\")\n",
    "        self.down_sample = None\n",
    "\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        channel_dim = -1\n",
    "        # SpatialDropout1D proved to be much better. Original paper: https://arxiv.org/abs/1411.4280\n",
    "        self.dropout1 = tfkl.SpatialDropout1D(self.dropout)\n",
    "        self.dropout2 = tfkl.SpatialDropout1D(self.dropout)\n",
    "        if input_shape[channel_dim] != self.filter_size:\n",
    "            # TODO why not a conv1d layer?\n",
    "            # self.down_sample = tf.layers.Conv1D(\n",
    "            #     self.filter_size, kernel_size=1, \n",
    "            #     activation=None, data_format=\"channels_last\", padding=\"valid\")\n",
    "            self.down_sample = tfkl.Dense(self.filter_size, activation=None)\n",
    "        self.built = True\n",
    "    \n",
    "    def call(self, inputs, training=True):\n",
    "        x = self.conv1(inputs)\n",
    "        x = self.dropout1(x, training=training)\n",
    "        x = self.conv2(x)\n",
    "        x = self.dropout2(x, training=training)\n",
    "        if self.down_sample is not None:\n",
    "            inputs = self.down_sample(inputs)\n",
    "        return tf.nn.relu(x + inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-09T15:38:55.163982Z",
     "start_time": "2019-06-09T15:38:55.152812Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def make_inference_net(model_to_wrap, input_shape, batch_size, activation=tf.nn.softplus):\n",
    "    \"\"\"This is basically a wrapper function. Define your model up to the split into μ and σ.\n",
    "    Which is what this function will do.\n",
    "    \n",
    "    Depending on your interpretation of how a VAE should do it (depends mostly on your reparametrisation trick),\n",
    "    you can provide an activation function.\n",
    "    \n",
    "    E.g. if you think σ should be the variance use `tf.nn.softplus` to force it to be positive.\n",
    "    if you think it should be the deviation, then provide `None`.\n",
    "    \"\"\"\n",
    "    \n",
    "    x = tfk.Input(shape=input_shape, batch_size=batch_size)\n",
    "    enc = model_to_wrap(x)\n",
    "    mean, var = tf.split(enc, num_or_size_splits=2, axis=-1)\n",
    "    \n",
    "    if activation is not None:\n",
    "        var = tfkl.Activation(activation)(var)\n",
    "    \n",
    "    return tfk.Model(x, [mean, var])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-09T15:38:55.182882Z",
     "start_time": "2019-06-09T15:38:55.165249Z"
    },
    "code_folding": [
     0,
     12
    ]
   },
   "outputs": [],
   "source": [
    "# DrosophVAE base class\n",
    "# build using:\n",
    "#   - https://github.com/tensorflow/tensorflow/blob/r1.13/tensorflow/contrib/eager/python/examples/generative_examples/cvae.ipynb \n",
    "#   - https://www.kaggle.com/hone5com/fraud-detection-with-variational-autoencoder\n",
    "\n",
    "def dense_layers(sizes, activation_fn=tf.nn.leaky_relu, name_prefix=None):\n",
    "    # no activation in the last layer\n",
    "    # both models inference and generative should be free (super important for the decoder)\n",
    "    # the encoder could be fixed, but we want the \"mean\" to be represent any value, \n",
    "    # a SoftPlus activation is applied to the \"variance\" in the `DrosophVAE.encode` method.\n",
    "    return [tfkl.Dense(size, activation=None if is_last else activation_fn, name=f\"{name_prefix}_dense_{idx}\") for idx, is_last, size in if_last(sizes)]\n",
    "\n",
    "def temporal_layers(filter_sizes, kernel_size=2, dropout=0.2):\n",
    "    return [TemporalBlock(filter_size, kernel_size, dilation_rate=2 ** i, dropout=dropout, name=f\"temporal_block_{i}\") for i, filter_size in enumerate(filter_sizes)]\n",
    "\n",
    "class DrosophVAE(tfk.Model):\n",
    "    def __init__(self, latent_dim, input_shape, batch_size, \n",
    "                 n_layers=3, dropout_rate_temporal=0.2, \n",
    "                 loss_weight_reconstruction=1.0, loss_weight_kl=1.0, \n",
    "                 filters_conv_layer=None, conv_layer_kernel_size=2,\n",
    "                 use_wavenet_temporal_layer=True):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "        -----\n",
    "        \n",
    "        latent_dim              int, dimension of latent space\n",
    "        input_shape             tuple, total input shape is: [batch_size, *input_shape]\n",
    "        batch_size              int\n",
    "        n_layers                int, number of dense layers. \n",
    "                                output shape of the dense layers is linearly scaled.\n",
    "        dropout_rate_temporal   float, in [0, 1). dropout rate for temporal blocks (conv layers).\n",
    "        filters_conv_layer      list[int]. filter sizes for conv layers\n",
    "        \"\"\"\n",
    "        super(DrosophVAE, self).__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self._input_shape = input_shape\n",
    "        self._batch_size = batch_size\n",
    "        self._loss_weight_reconstruction = loss_weight_reconstruction\n",
    "        self._loss_weight_kl = loss_weight_kl\n",
    "        self._layer_sizes_inference  = np.linspace(input_shape[-1], 2 * latent_dim, n_layers).astype(np.int)\n",
    "        # pseudo reverse as the inference network goes down to double the latent space, ask Semigh about this\n",
    "        # the 2 * n_layers is to keep compression speed roughly the same\n",
    "        self._layer_sizes_generative = np.linspace(latent_dim, input_shape[-1], 2 * n_layers).astype(np.int).tolist()\n",
    "        self._conv_layer_kernel_size = conv_layer_kernel_size\n",
    "        \n",
    "        if use_wavenet_temporal_layer:\n",
    "            # Remember that we do diluted convolutions -> The filter size can stay ~ constant. TODO discuss with Semigh\n",
    "            if filters_conv_layer is None:\n",
    "                # TODO this is probably not correct\n",
    "                self.filters_conv_layer = [input_shape[-1]] * 3\n",
    "            else:\n",
    "                self.filters_conv_layer = filters_conv_layer\n",
    "                \n",
    "            self.temporal_conv_net = tfk.Sequential([tfkl.InputLayer(input_shape=input_shape, name='input_temporal_conv_net'),\n",
    "                                                     *temporal_layers(kernel_size=self._conv_layer_kernel_size, \n",
    "                                                                      filter_sizes=self.filters_conv_layer,  \n",
    "                                                                      dropout=dropout_rate_temporal)],\n",
    "                                                     name='temporal_conv_net')\n",
    "            \n",
    "            inference_input_shape = input_shape\n",
    "            generative_input_shape = list(input_shape[:1]) + [latent_dim]\n",
    "        else:\n",
    "            self.temporal_conv_net = None\n",
    "            inference_input_shape = input_shape[-1]\n",
    "            generative_input_shape = (latent_dim, )\n",
    "            \n",
    "        self.inference_net = make_inference_net(tfk.Sequential([tfkl.InputLayer(input_shape=inference_input_shape, name='input_inference_net'),\n",
    "                                                 *dense_layers(self._layer_sizes_inference, name_prefix='inf')],\n",
    "                                                 name='inference_net'), inference_input_shape, batch_size)\n",
    "            \n",
    "        self.generative_net = tfk.Sequential([tfkl.InputLayer(input_shape=generative_input_shape, name='input_generative_net'),\n",
    "                                                  *dense_layers(self._layer_sizes_generative, name_prefix='gen')],\n",
    "                                                  name='generative_net')\n",
    "    \n",
    "    def sample(self, eps=None):\n",
    "        if eps is None:\n",
    "            if self._loss_weight_kl == 0.0:\n",
    "                warnings.warn('KL loss is 0.0. The latent space is not properly trained')\n",
    "            # The KL-loss is calculated against a normal distribution, \n",
    "            # thus it should resemble one and thus sampling should make sense.\n",
    "            #eps = tf.random_normal(shape=(self._batch_size, self.latent_dim))\n",
    "            eps = tf.random_normal(shape=[self._batch_size] + list(self.generative_net.input_shape[1:]))\n",
    "        return self.decode(eps, apply_sigmoid=False)\n",
    "    \n",
    "    def encode(self, x, training=False):\n",
    "        if self.temporal_conv_net:\n",
    "            # TODO combine them into one? max pooling or something\n",
    "            #x_tmp = tfkl.Lambda(lambda x: x[:, -1, :])(self.temporal_conv_net(x, training=training))\n",
    "            mean, var = self.inference_net(self.temporal_conv_net(x, training=training))\n",
    "            #mean, var = tf.split(self.inference_net(self.temporal_conv_net(x, training=training)), \n",
    "            #                        num_or_size_splits=2,\n",
    "            #                        axis=-1)\n",
    "        else:\n",
    "            mean, var = self.inference_net(x, training=training)\n",
    "            #mean, var = tf.split(self.inference_net(x),\n",
    "            #                        num_or_size_splits=2,\n",
    "            #                        axis=1)\n",
    "            \n",
    "        # the variance should be in [0, inf)\n",
    "        return mean, var\n",
    "  \n",
    "    def reparameterize(self, mean, var):\n",
    "        # TODO check: the params should be correct? check original paper\n",
    "        eps = tf.random_normal(shape=mean.shape)\n",
    "        #return eps * tf.exp(logvar * .5) + mean\n",
    "        # this is the truest form to the original paper https://arxiv.org/pdf/1312.6114v10.pdf\n",
    "        return eps * var + mean\n",
    "  \n",
    "    def decode(self, z, apply_sigmoid=False):\n",
    "        logits = self.generative_net(z)\n",
    "        if apply_sigmoid:\n",
    "            probs = tf.sigmoid(logits)\n",
    "            return probs\n",
    "  \n",
    "        return logits\n",
    "\n",
    "    def predict(self, x):\n",
    "        # https://github.com/LynnHo/VAE-Tensorflow/blob/master/train.py\n",
    "        # epsilon = tf.random_normal(tf.shape(z_mu))\n",
    "        # if is_training:\n",
    "        #     z = z_mu + tf.exp(0.5 * z_log_sigma_sq) * epsilon\n",
    "        # else:\n",
    "        #     z = z_mu\n",
    "        mean, logvar = self.encode(x)\n",
    "        z = model.reparameterize(mean, logvar)\n",
    "        return model.decode(z, apply_sigmoid=True)\n",
    "    \n",
    "    def call(self, x, training=False, apply_sigmoid=False):\n",
    "        return self.decode(self.reparameterize(*self.encode(x, training=training)), \n",
    "                           apply_sigmoid=apply_sigmoid)\n",
    "    \n",
    "    def _config_(self):\n",
    "        return {\n",
    "            \"latent_dim\": self.latent_dim,\n",
    "            \"input_shape\": self._input_shape,\n",
    "            \"batch_size\": self._batch_size,\n",
    "            \"layer_sizes_inference\": self._layer_sizes_inference,\n",
    "            \"layer_sizes_generative\": self._layer_sizes_generative,\n",
    "            \"loss_weight_reconstruction\": self._loss_weight_reconstruction,\n",
    "            \"loss_weight_kl\": self._loss_weight_kl,\n",
    "            \"model_impl\": self._name\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-09T15:38:55.195073Z",
     "start_time": "2019-06-09T15:38:55.185010Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class TemporalUpsamplingConv(tfkl.Layer):\n",
    "    \"\"\"\n",
    "    This layer requires good fine tuning of parameters. Use PaddedConv1dTransposed if you want an easier layer.\n",
    "    \n",
    "    For a general nice description of convolutions:\n",
    "        https://arxiv.org/pdf/1603.07285.pdf\n",
    "        \n",
    "        A guide to convolution arithmetic for deeplearning\n",
    "        Vincent Dumoulin and Francesco Visin\n",
    "    \n",
    "    For the artifacts:\n",
    "        Conditional generative adversarial nets for convolutional face generation\n",
    "        J. Gauthier.\n",
    "        Class Project for Stanford CS231N: Convolutional Neural Networks for Visual Recognition, Winter semester, Vol 2014. 2014. \n",
    "        http://www.foldl.me/uploads/papers/tr-cgans.pdf\n",
    "    \"\"\"\n",
    "    def __init__(self, conv_n_filters, upsampling_size=3, conv_kernel_size=2, conv_padding='valid', conv_strides=2, name=None):\n",
    "        super(TemporalUpsamplingConv, self).__init__(name=name)\n",
    "        \n",
    "        if conv_kernel_size % conv_strides != 0:\n",
    "            warnings.warn(f\"Using a kernel size not divisable by the stride will lead to artifacts.:\"\n",
    "                          f\" Given kernel_size: {conv_kernel_size}\"\n",
    "                          f\" stride: {conv_strides}\")\n",
    "        \n",
    "        self.upsampling_size = upsampling_size\n",
    "        self.conv_n_filters = conv_n_filters\n",
    "        self.conv_kernel_size = conv_kernel_size\n",
    "        self.conv_padding = conv_padding\n",
    "        self.conv_strides = conv_strides\n",
    "        \n",
    "        # upscale with 3 so that we can again apply `valid` padding and \"reverse\" the encoder\n",
    "        self.upsampling = tfkl.UpSampling1D(size=self.upsampling_size,\n",
    "                                            name=f\"{name}_upsampling\")\n",
    "        # TODO maybe add some fancy flipping of the input, right now it cuts again from the \"start\", ideally it should append there...\n",
    "        self.conv = tfkl.Conv1D(filters=self.conv_n_filters, \n",
    "                                kernel_size=self.conv_kernel_size, \n",
    "                                padding=self.conv_padding, \n",
    "                                strides=self.conv_strides,\n",
    "                                name=f\"{name}_conv\")\n",
    "    \n",
    "    def call(self, x): \n",
    "        return self.conv(self.upsampling(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-09T15:38:55.212462Z",
     "start_time": "2019-06-09T15:38:55.197077Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class PaddedConv1dTransposed(tfkl.Layer):\n",
    "    \"\"\" The most inefficient transpose version. The focus is to get a roughly equal decompression speed.\n",
    "    \n",
    "    Build on https://arxiv.org/pdf/1603.07285.pdf relationship 8, page 22\n",
    "    \n",
    "    Note that this will almost certainly lead to artifacts as the receptive fields overlap. \n",
    "    But... as we don't really care about it... (at least for now).\n",
    "    \n",
    "    See also https://distill.pub/2016/deconv-checkerboard/\n",
    "    \"\"\"\n",
    "    def __init__(self, n_filters, kernel_size=2, name=None, activation=tf.nn.leaky_relu, batch_norm=False, padding=None):\n",
    "        \"\"\"\n",
    "        Do NOT set the padding by yourself, the input will be padded in a causal way. If you set padding, this padding will be applied.\n",
    "        \"\"\"\n",
    "        \n",
    "        if batch_norm:\n",
    "            name += '_bn'\n",
    "        super(PaddedConv1dTransposed, self).__init__(name=name)\n",
    "        \n",
    "        if padding is None:\n",
    "            padding = 'valid'\n",
    "            self._padding_overwrite_ = False\n",
    "        else:\n",
    "            self._padding_overwrite_ = True\n",
    "        \n",
    "        self.n_filters = n_filters\n",
    "        self.kernel_size = kernel_size\n",
    "        \n",
    "        self.padding = [[0, 0], [1, 1], [0, 0]] # adds only a zero at the end of the time-dimension\n",
    "        self.conv = tfkl.Conv1D(filters=self.n_filters, kernel_size=self.kernel_size, activation=activation, padding=padding)\n",
    "        \n",
    "        if batch_norm:\n",
    "            self.batch_norm = tfkl.BatchNormalization()\n",
    "        else:\n",
    "            self.batch_norm = None\n",
    "    \n",
    "    def call(self, x): \n",
    "        if not self._padding_overwrite_:\n",
    "            x = tf.pad(x, self.padding)\n",
    "        x = self.conv(x)\n",
    "        \n",
    "        if self.batch_norm:\n",
    "            x = self.batch_norm(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-09T15:38:55.230787Z",
     "start_time": "2019-06-09T15:38:55.213884Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class DrosophVAEConv(DrosophVAE):\n",
    "    \"\"\"\n",
    "    About the Deconvolution: https://datascience.stackexchange.com/questions/6107/what-are-deconvolutional-layers\n",
    "    \"\"\"\n",
    "    def __init__(self, latent_dim, input_shape, batch_size, n_start_filters=None, dropout_rate_temporal=0.2, loss_weight_reconstruction=1.0, loss_weight_kl=1.0, with_batch_norm=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "        -----\n",
    "        \n",
    "        latent_dim              int, dimension of latent space\n",
    "        input_shape             tuple, total input shape is: [batch_size, *input_shape]\n",
    "        batch_size              int\n",
    "        n_layers                int, number of dense layers. \n",
    "                                output shape of the dense layers is linearly scaled.\n",
    "        dropout_rate_temporal   float, in [0, 1). dropout rate for temporal blocks (conv layers).\n",
    "        filters_conv_layer      list[int]. filter sizes for conv layers\n",
    "        \"\"\"\n",
    "        # just a dummy init, we are only interested in defining the two nets.\n",
    "        super(DrosophVAEConv, self).__init__(\n",
    "            latent_dim=latent_dim,\n",
    "            input_shape=input_shape,\n",
    "            batch_size=batch_size,\n",
    "            loss_weight_reconstruction=loss_weight_reconstruction,\n",
    "            loss_weight_kl=loss_weight_kl,\n",
    "            use_wavenet_temporal_layer=False,\n",
    "            n_layers=1)\n",
    "        \n",
    "        if n_start_filters is None:\n",
    "            n_start_filters = input_shape[-1]\n",
    "            \n",
    "        self.latent_dim = latent_dim\n",
    "        self._layer_sizes_inference  = np.linspace(n_start_filters, 2 * latent_dim, num=input_shape[-2] -1, dtype=np.int)\n",
    "        # pseudo reverse as the inference network goes down to double the latent space, ask Semigh about this\n",
    "        # the 2 * n_layers is to keep compression speed roughly the same\n",
    "        \n",
    "        # since the layers grow only one time-step per layer:\n",
    "        self._layer_sizes_generative = np.linspace(latent_dim, n_start_filters, num=input_shape[-2] - 1, dtype=np.int)\n",
    "        \n",
    "        # TODO add MaxPooling\n",
    "        self.inference_net = tfk.Sequential([tfkl.InputLayer(input_shape=input_shape, name='input_inference_net'),\n",
    "                                                  *[self._convolutional_layer_(idx=i,\n",
    "                                                                               filters=fs,\n",
    "                                                                               kernel_size=2,\n",
    "                                                                               padding='valid',\n",
    "                                                                               name=f\"inf_{i}\",\n",
    "                                                                               activation=tf.nn.leaky_relu) \n",
    "                                                    for i, fs in enumerate(self._layer_sizes_inference)],\n",
    "                                                  tfkl.Flatten(),\n",
    "                                                  tfkl.Dense(2 * self.latent_dim)],\n",
    "                                                 name='inference_net')\n",
    "        # This does not work...\n",
    "        #self.generative_net = tf.keras.Sequential([tfkl.InputLayer(input_shape=(latent_dim,)),\n",
    "        #                                           tfkl.Lambda(lambda x: tf.reshape(x, [batch_size, 1, latent_dim]), name='gen_reshaping'),\n",
    "        #                                           *[UpsamplingConv(n_filters=fs, name=f\"gen_conv_{i}\") for i, fs in enumerate(self._layer_sizes_generative)],\n",
    "        #                                           tfkl.Dense(input_shape[-1])],\n",
    "        #                                          name='generative_net')\n",
    "        \n",
    "        self.generative_net = tfk.Sequential([tfkl.InputLayer(input_shape=(self.latent_dim,), name='input_generative_net'),\n",
    "                                              tfkl.Lambda(lambda x: tf.reshape(x, [-1, 1, self.latent_dim])),\n",
    "                                              *[PaddedConv1dTransposed(n_filters=fs, batch_norm=with_batch_norm, name=f\"gen_{i}\") for i, fs\n",
    "                                                in enumerate(self._layer_sizes_generative)],\n",
    "                                              tfkl.TimeDistributed(tfkl.Dense(input_shape[-1], activation=tf.nn.leaky_relu, name=f\"gen_dense_0\")), \n",
    "                                              tfkl.TimeDistributed(tfkl.Dense(input_shape[-1], activation=tf.nn.leaky_relu, name=f\"gen_dense_1\")), \n",
    "                                              tfkl.TimeDistributed(tfkl.Dense(input_shape[-1], activation=None, name=f\"gen_dense_2\"))], \n",
    "                                                  name='generative_net')\n",
    "        \n",
    "    def _convolutional_layer_(self, idx, **kwargs):\n",
    "        return tfk.Sequential([tfkl.Conv1D(**{**kwargs, 'name': f\"{kwargs['name']}_block_{idx}_conv_0\"}), \n",
    "                               tfkl.Conv1D(**{**kwargs, 'name': f\"{kwargs['name']}_block_{idx}_conv_1\", 'padding': 'same'}), \n",
    "                                tfkl.BatchNormalization(name=f\"conv_block_{idx}_batch_norm\")], \n",
    "                              name=f\"conv_block_{idx}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-09T15:38:55.250157Z",
     "start_time": "2019-06-09T15:38:55.232048Z"
    },
    "code_folding": [
     0,
     67,
     73,
     120
    ]
   },
   "outputs": [],
   "source": [
    "class DrosophVAESkipConv(DrosophVAE):\n",
    "    \"\"\"\n",
    "    About the Deconvolution: https://datascience.stackexchange.com/questions/6107/what-are-deconvolutional-layers\n",
    "    \n",
    "    Based on https://arxiv.org/pdf/1807.04863.pdf to avoid latent space collapse\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, latent_dim, input_shape, batch_size, \n",
    "                 n_conv_layers=None, n_start_filters=None, dropout_rate_temporal=0.2, loss_weight_reconstruction=1.0, loss_weight_kl=1.0):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "        -----\n",
    "        \n",
    "        latent_dim              int, dimension of latent space\n",
    "        input_shape             tuple, total input shape is: [batch_size, *input_shape]\n",
    "        batch_size              int\n",
    "        n_layers                int, number of dense layers. \n",
    "                                output shape of the dense layers is linearly scaled.\n",
    "        dropout_rate_temporal   float, in [0, 1). dropout rate for temporal blocks (conv layers).\n",
    "        filters_conv_layer      list[int]. filter sizes for conv layers\n",
    "        \"\"\"\n",
    "        # just a dummy init, we are only interested in defining the two nets.\n",
    "        super(DrosophVAESkipConv, self).__init__(\n",
    "            latent_dim=latent_dim,\n",
    "            input_shape=input_shape,\n",
    "            batch_size=batch_size,\n",
    "            loss_weight_reconstruction=loss_weight_reconstruction,\n",
    "            loss_weight_kl=loss_weight_kl,\n",
    "            use_wavenet_temporal_layer=False,\n",
    "            n_layers=1,\n",
    "        )\n",
    "        \n",
    "        if n_start_filters is None:\n",
    "            n_start_filters = input_shape[-1]\n",
    "            \n",
    "        if n_conv_layers is None:\n",
    "            n_conv_layers = np.int(np.ceil((n_start_filters - latent_dim) / 2 + 1))\n",
    "            \n",
    "        self.latent_dim = latent_dim\n",
    "        self._layer_sizes_inference  = np.linspace(n_start_filters, 2 * latent_dim, num=input_shape[-2] -1, dtype=np.int)\n",
    "        # pseudo reverse as the inference network goes down to double the latent space, ask Semigh about this\n",
    "        # the 2 * n_layers is to keep compression speed roughly the same\n",
    "        \n",
    "        # since the layers grow only one time-step per layer:\n",
    "        self._layer_sizes_generative = np.linspace(latent_dim, n_start_filters, num=input_shape[-2] - 1, dtype=np.int)\n",
    "        \n",
    "        print(self._layer_sizes_inference)\n",
    "        print(self._layer_sizes_generative)\n",
    "    \n",
    "        # TODO add MaxPooling\n",
    "        self.inference_net = make_inference_net(tfk.Sequential([*[_convolutional_layer_(idx=i,\n",
    "                                                                  filters=fs,\n",
    "                                                                  kernel_size=2,\n",
    "                                                                  padding='valid',\n",
    "                                                                  name=f\"inf_{i}\",\n",
    "                                                                  activation=tf.nn.leaky_relu) \n",
    "                                            for i, fs in enumerate(self._layer_sizes_inference)],\n",
    "                                          tfkl.Flatten(),\n",
    "                                          tfkl.Dense(2 * self.latent_dim)],\n",
    "                                         name='inference_net'), input_shape, batch_size)\n",
    "        \n",
    "        self.generative_net = _skip_connection_model_(input_shape=self.latent_dim, \n",
    "                                                      layer_sizes=self._layer_sizes_generative,\n",
    "                                                      output_dim=input_shape[-1],\n",
    "                                                      name='generative_net')\n",
    "        \n",
    "        \n",
    "def _convolutional_layer_(idx, **kwargs):\n",
    "    return tfk.Sequential([tfkl.Conv1D(**{**kwargs, 'name': f\"{kwargs['name']}_block_{idx}_conv_0\"}), \n",
    "                           tfkl.Conv1D(**{**kwargs, 'name': f\"{kwargs['name']}_block_{idx}_conv_1\", 'padding': 'same'}), \n",
    "                            tfkl.BatchNormalization(name=f\"conv_block_{idx}_batch_norm\")], \n",
    "                          name=f\"conv_block_{idx}\")\n",
    "\n",
    "class SkipConnectionLayer(tfkl.Layer):\n",
    "    \"\"\"\n",
    "    Taken from https://arxiv.org/pdf/1512.03385.pdf, Deep Residual Learning for Image Recognition\n",
    "    \n",
    "    The batch normalisation prior to a convolution and before activation follows:\n",
    "    S. Ioffe and C. Szegedy. Batch normalization:  Accelerating deepnetwork training by reducing internal covariate shift. InICML, 2015.\n",
    "        \n",
    "    This class only exists because I can't read too much output. Tensorflow, yeah! (or not)\n",
    "    \n",
    "    Roughtly equivalent (as it was this some time ago) \"functional\"-style:\n",
    "    x = tfkl.BatchNormalization()(x)\n",
    "    x = PaddedConv1dTransposed(n_filters=fs, activation=None, batch_norm=False)(x)\n",
    "    x_skip = tf.reshape(tfkl.Dense(fs, activation=None)(input_layer), [-1, 1, x.shape[-1]])\n",
    "    x = x + x_skip\n",
    "    x = tfkl.BatchNormalization()(x)\n",
    "    x = tfkl.Activation(tf.nn.leaky_relu)(x)\n",
    "    \"\"\"\n",
    "    def __init__(self, n_filters_weight_layer, activation=tf.nn.leaky_relu, name=None):\n",
    "        super(SkipConnectionLayer, self).__init__(name=name)\n",
    "        self.bn_in = tfkl.BatchNormalization()\n",
    "        self.weight_layer_0 = PaddedConv1dTransposed(n_filters=n_filters_weight_layer, activation=None, batch_norm=False)\n",
    "        self.act_0 = tfkl.Activation(activation)\n",
    "        self.weight_layer_1 = PaddedConv1dTransposed(n_filters=n_filters_weight_layer, activation=None, batch_norm=False, padding='same')\n",
    "                                          \n",
    "        self.bn_sk = tfkl.BatchNormalization()\n",
    "        self.identity_layer = tfkl.Dense(n_filters_weight_layer, activation=None)\n",
    "        self.reshape_layer = tfkl.Lambda(lambda x: tf.reshape(x, [-1, 1, n_filters_weight_layer]))\n",
    "        self.act_1 = tfkl.Activation(activation)\n",
    "        \n",
    "    def call(self, x_and_skipped):\n",
    "        x, x_skipped = x_and_skipped\n",
    "        x = self.bn_in(x)\n",
    "        x_skipped = self.bn_sk(x_skipped) # This is from another paper, which I forgot to bookmark, seemed sensible\n",
    "        x_skipped = self.identity_layer(x_skipped)\n",
    "        \n",
    "        x = self.weight_layer_0(x)\n",
    "        x = self.act_0(x)\n",
    "        \n",
    "        x = self.weight_layer_1(x)\n",
    "        \n",
    "        x = x + self.reshape_layer(x_skipped)\n",
    "        \n",
    "        x = self.act_1(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "        \n",
    "def _skip_connection_model_(input_shape, layer_sizes, output_dim, name):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    input_layer = tfkl.Input(shape=(input_shape,))\n",
    "    x = tfkl.Lambda(lambda x: tf.reshape(x, [-1, 1, input_shape]))(input_layer)\n",
    "\n",
    "    for i, fs in enumerate(layer_sizes):\n",
    "        x = SkipConnectionLayer(fs)([x, input_layer])\n",
    "        \n",
    "    x = tfkl.TimeDistributed(tfkl.Dense(output_dim, activation=None))(x)\n",
    "\n",
    "    return tfk.Model(inputs=[input_layer], outputs=[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-09T15:38:55.264108Z",
     "start_time": "2019-06-09T15:38:55.251229Z"
    }
   },
   "outputs": [],
   "source": [
    "if run_config['use_time_series']:\n",
    "    assert len(data_train.shape) == 3, 'run all the necessary code, shape does not align with config'\n",
    "else:\n",
    "    assert len(data_train.shape) == 2, 'run all the necessary code, shape does not align with config'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-09T15:38:55.281812Z",
     "start_time": "2019-06-09T15:38:55.265244Z"
    }
   },
   "outputs": [],
   "source": [
    "data_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-09T15:38:55.298213Z",
     "start_time": "2019-06-09T15:38:55.283584Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# LOSS FUNCTION\n",
    "#\n",
    "# https://github.com/pytorch/examples/issues/399\n",
    "#   Argues that since we are using a normal distribution we should not use any activation function in the last layer\n",
    "#   and the loss should be MSE.\n",
    "# https://stats.stackexchange.com/questions/332179/how-to-weight-kld-loss-vs-reconstruction-loss-in-variational-auto-encoder?rq=1\n",
    "#   Some general discussion about KL vs recon-loss\n",
    "# https://stats.stackexchange.com/questions/368001/is-the-output-of-a-variational-autoencoder-meant-to-be-a-distribution-that-can-b\n",
    "\n",
    "def compute_loss(model, x, detailed=False, kl_nan_offset=1e-18):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "    \n",
    "        model          the model\n",
    "        x              the data\n",
    "        detailed       set to true if you want the separate losses to be returned as well, basically a debug mode\n",
    "        kl_nan_offset  the kicker, can lead to NaN errors otherwise (don't ask me how long it took to find this)\n",
    "                       value was found empirically \n",
    "    \"\"\"\n",
    "    mean, var = model.encode(x)\n",
    "    z = model.reparameterize(mean, var)\n",
    "    x_logit = model.decode(z)\n",
    "    \n",
    "    #if run_config['use_time_series']:\n",
    "    #    # Note, the model is trained to reconstruct only the last, most current time step (by taking the last entry in the timeseries)\n",
    "    #    # this works on classification data (or binary data)\n",
    "    #    #cross_ent = tf.nn.sigmoid_cross_entropy_with_logits(logits=x_logit, labels=x[:, -1, :])\n",
    "    #    recon_loss = tf.losses.mean_squared_error(predictions=x_logit, labels=x[:, -1, :])\n",
    "    #else:\n",
    "    #    recon_loss = tf.losses.mean_squared_error(predictions=x_logit, labels=x)\n",
    "    \n",
    "    # Putting more weight on the most current time epochs\n",
    "    recon_loss = tf.losses.mean_squared_error(predictions=x_logit,\n",
    "                                          labels=x, \n",
    "                                          weights=np.exp(np.linspace(0, 1, num=run_config['time_series_length']))\\\n",
    "                                                    .reshape((1, run_config['time_series_length'], 1)))\n",
    "    \n",
    "    # Checkout https://en.wikipedia.org/wiki/Jensen%E2%80%93Shannon_divergence\n",
    "    # https://arxiv.org/pdf/1606.00704.pdf\n",
    "    #  Adversarially Learned Inference\n",
    "    #  Vincent Dumoulin, Ishmael Belghazi, Ben Poole, Olivier Mastropietro1,Alex Lamb1,Martin Arjovsky3Aaron Courville1\n",
    "    \n",
    "    # This small constant offset prevents Nan errors\n",
    "    p = tfp.distributions.Normal(loc=tf.zeros_like(mean) + tf.constant(kl_nan_offset), scale=tf.ones_like(var) + tf.constant(kl_nan_offset))\n",
    "    q = tfp.distributions.Normal(loc=mean + tf.constant(kl_nan_offset), scale=var + tf.constant(kl_nan_offset))\n",
    "    try:\n",
    "        # the KL loss can explode easily, this is to prevent overflow errors\n",
    "        kl = tf.reduce_mean(tf.clip_by_value(tfp.distributions.kl_divergence(p, q, allow_nan_stats=True), 0., 1e32))\n",
    "    except (_NotOkStatusException, InfOrNanError) as e:\n",
    "        print('Error with KL-loss: ', e, tf.reduce_mean(var))\n",
    "        kl = 1.\n",
    "    \n",
    "    if not detailed:\n",
    "        kl = tf.clip_by_value(kl, 0., 1.)\n",
    "    \n",
    "    if model._loss_weight_kl == 0.:\n",
    "        loss = model._loss_weight_reconstruction*recon_loss \n",
    "    else:\n",
    "        # KL loss can be NaN for some data. This is inherit to KL-loss (but the data is probably more to blame)\n",
    "        loss = model._loss_weight_reconstruction*recon_loss + model._loss_weight_kl*kl\n",
    "    \n",
    "    if detailed:\n",
    "        return loss, recon_loss, kl\n",
    "    else:\n",
    "        return loss\n",
    "\n",
    "def compute_gradients(model, x): \n",
    "    with tf.GradientTape() as tape: \n",
    "        loss = compute_loss(model, x) \n",
    "        return tape.gradient(loss, model.trainable_variables), loss\n",
    "\n",
    "def apply_gradients(optimizer, gradients, variables, global_step=None):\n",
    "    # TODO try out gradient clipping\n",
    "    #gradients, _ = tf.clip_by_global_norm(gradients, 5.0)\n",
    "    optimizer.apply_gradients(zip(gradients, variables), global_step=global_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-09T15:38:58.594326Z",
     "start_time": "2019-06-09T15:38:55.299643Z"
    },
    "code_folding": [
     0
    ],
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "NS2GWywBbAWo"
   },
   "outputs": [],
   "source": [
    "# This is the init cell. The model and all related objects are created here.\n",
    "if run_config['debug'] and run_config['d_no_compression']:\n",
    "    run_config['latent_dim'] = data_train.shape[-1]\n",
    "    \n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# This is a bit shitty... but I like to run the training loop multiple times...\n",
    "test_reports = np.array([]) \n",
    "train_reports = np.array([]) \n",
    "_cur_train_reports = []\n",
    "_cur_test_reports  = []\n",
    "cur_min_val_idx = 0\n",
    "run_config['loss_weight_reconstruction'] = 1.0\n",
    "run_config['loss_weight_kl'] = 0.0 # 1e-3\n",
    "\n",
    "print(f\"Using model: {run_config['model_impl']}\")\n",
    "      \n",
    "model_config = {'latent_dim': run_config['latent_dim'], \n",
    "                'input_shape': data_train.shape[1:], \n",
    "                'batch_size': run_config['batch_size'], \n",
    "                'loss_weight_reconstruction': run_config['loss_weight_reconstruction'],\n",
    "                'loss_weight_kl': run_config['loss_weight_kl']}\n",
    "      \n",
    "if run_config['model_impl'] == _MODEL_IMPL_TEMPORAL_CONV_:\n",
    "    model = DrosophVAE(run_config['latent_dim'], \n",
    "                       input_shape=data_train.shape[1:], \n",
    "                       batch_size=run_config['batch_size'], \n",
    "                       n_layers=run_config['n_conv_layers'], \n",
    "                       dropout_rate_temporal=run_config['dropout_rate'],\n",
    "                       loss_weight_reconstruction=run_config['loss_weight_reconstruction'],\n",
    "                       loss_weight_kl=run_config['loss_weight_kl'], \n",
    "                       use_wavenet_temporal_layer=run_config['use_time_series'])\n",
    "\n",
    "    if run_config['use_time_series']:\n",
    "        model.temporal_conv_net.summary()\n",
    "elif run_config['model_impl'] == _MODEL_IMPL_PADD_CONV_:\n",
    "    model = DrosophVAEConv(**{**model_config, 'with_batch_norm': run_config['with_batch_norm']})\n",
    "elif run_config['model_impl'] == _MODEL_IMPL_SKIP_CON_:\n",
    "    model = DrosophVAESkipConv(**model_config)\n",
    "else:\n",
    "    raise ValueError('not such model')\n",
    "\n",
    "model.inference_net.summary(line_length=100)\n",
    "model.generative_net.summary(line_length=100)\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(1e-4)\n",
    "#optimizer = tf.train.AdadeltaOptimizer(1e-4)\n",
    "run_config['optimizer'] = optimizer._name\n",
    "_config_hash_ = config.get_config_hash(run_config)\n",
    "_base_path_ = f\"{settings.config.__DATA_ROOT__}/tvae_logs/{config.config_description(run_config, short=True)}_{_config_hash_}\"\n",
    "_model_checkpoints_path_ = f\"{settings.config.__DATA_ROOT__}/models/{config.config_description(run_config, short=True)}_{_config_hash_}/checkpoint\" \n",
    "train_summary_writer = tfc.summary.create_file_writer(_base_path_ + '/train')\n",
    "test_summary_writer = tfc.summary.create_file_writer(_base_path_ + '/test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-09T16:13:21.387765Z",
     "start_time": "2019-06-09T16:13:14.456331Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# This is the run cell. Designed to be able to train the model for an arbitrary amount of epochs.\n",
    "def _compute_loss_for_data_(model, data):\n",
    "    loss = tfe.metrics.Mean()\n",
    "    recon = tfe.metrics.Mean()\n",
    "    kl = tfe.metrics.Mean()\n",
    "    for batch in data:\n",
    "        loss_b, recon_b, kl_b  = compute_loss(model, batch, detailed=True)\n",
    "        loss(loss_b)\n",
    "        recon(recon_b)\n",
    "        kl(kl_b)\n",
    "        \n",
    "    total_loss = loss.result()\n",
    "    total_recon = recon.result()\n",
    "    total_kl = kl.result()\n",
    "    \n",
    "    return total_loss, total_recon, total_kl\n",
    "\n",
    "def _progress_str_(epoch, _cur_train_reports, _cur_test_reports, time=None, stopped=False):\n",
    "    progress_str = f\"Epoch: {epoch:0>4}, train/test loss: {_cur_train_reports[-1][0]:0.3f}\\t {_cur_test_reports[-1][0]:0.3f}\"\n",
    "    if time:\n",
    "        progress_str += f\" took {time:0.3f} sec\"\n",
    "        \n",
    "    if stopped:\n",
    "        progress_str = \"Stopped training during \" + progress_str\n",
    "        \n",
    "    return progress_str\n",
    "\n",
    "\n",
    "print(f\"will train model {model._config_()}, with global params: {run_config}, hash: {_config_hash_}\")\n",
    "print(f\"will train for ever...\")\n",
    "epoch = len(train_reports)\n",
    "\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    # pesky tensorflow again\n",
    "    warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "    while True:\n",
    "        try:\n",
    "            start_time = time.time()\n",
    "            for train_x in train_dataset:\n",
    "                gradients, loss = compute_gradients(model, train_x)\n",
    "                apply_gradients(optimizer, gradients, model.trainable_variables)\n",
    "            end_time = time.time()\n",
    "\n",
    "            _cur_train_reports += [_compute_loss_for_data_(model, train_dataset)]\n",
    "            _cur_test_reports += [_compute_loss_for_data_(model, test_dataset)]\n",
    "            \n",
    "            _recorded_scalars_ =  ['loss', 'recon', 'kl']\n",
    "            tf_helpers.tf_write_scalars(train_summary_writer, zip(_recorded_scalars_, _cur_train_reports[-1]), step=epoch)\n",
    "            tf_helpers.tf_write_scalars(test_summary_writer,  zip(_recorded_scalars_, _cur_test_reports[-1]),  step=epoch)\n",
    "\n",
    "            with train_summary_writer.as_default(), tfc.summary.always_record_summaries():\n",
    "                for g, var_name in zip(gradients, [tf_helpers.tf_clean_variable_name(v.name) for v in model.trainable_variables]):\n",
    "                    tfc.summary.histogram(f'gradient_{var_name}', g, step=epoch)\n",
    "\n",
    "            if epoch % 10 == 0:\n",
    "                print(_progress_str_(epoch, _cur_train_reports, _cur_test_reports, time=end_time - start_time))\n",
    "                tfc.summary.flush()\n",
    "            else:\n",
    "                # simple \"loading bar\"\n",
    "                print('=' * (epoch % 10) + '.' * (10 - (epoch % 10)), end='\\r')\n",
    "                \n",
    "            if epoch > 10 and _cur_test_reports[-1][0] < _cur_test_reports[cur_min_val_idx][0]:\n",
    "                cur_min_val_idx = epoch\n",
    "                model.save_weights(_model_checkpoints_path_)\n",
    "                \n",
    "            epoch += 1\n",
    "\n",
    "            if np.argmin(np.array(_cur_test_reports)[:, 1]) < (len(_cur_test_reports) - 10):\n",
    "                # if there was no improvement in the last 10 epochs, stop it\n",
    "                print('early stopping')\n",
    "                break\n",
    "        except KeyboardInterrupt:\n",
    "            tfc.summary.flush()\n",
    "            print(_progress_str_(epoch, _cur_train_reports, _cur_test_reports, stopped=True))\n",
    "            break\n",
    "        \n",
    "        \n",
    "tfc.summary.flush()\n",
    "train_reports = np.array(_cur_train_reports)\n",
    "test_reports =  np.array(_cur_test_reports)\n",
    "\n",
    "train_losses = train_reports[:, 0]\n",
    "test_losses = test_reports[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-09T16:04:25.082170Z",
     "start_time": "2019-06-09T16:04:25.060020Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# triplet, semi-supervised learning block init cell\n",
    "triplet_epoch = 0\n",
    "\n",
    "triplet_test_losses = np.array([]) \n",
    "triplet_train_losses = np.array([]) \n",
    "triplet_cur_train_reports = []\n",
    "triplet_cur_test_reports  = []\n",
    "triplet_cur_min_val_idx = 0\n",
    "run_config['loss_weight_reconstruction'] = 1.0\n",
    "run_config['loss_weight_kl'] = 0.0 # 1e-3\n",
    "\n",
    "triplet_optimizer = tf.train.AdamOptimizer(1e-4)\n",
    "#optimizer = tf.train.AdadeltaOptimizer(1e-4)\n",
    "run_config['optimizer_triplet'] = optimizer._name\n",
    "_model_checkpoints_encoder_path_ = f\"{settings.config.__DATA_ROOT__}/models/{config.config_description(run_config, short=True)}_{_config_hash_}_encoder/checkpoint\" \n",
    "\n",
    "labels_as_int = frames_idx_with_labels['label'].apply(lambda x: x.value).values\n",
    "triplet_train_dataset = to_tf_data(data_train, labels_as_int[run_config['time_series_length'] - 1:len(data_train)+run_config['time_series_length'] - 1])\n",
    "triplet_test_dataset = to_tf_data(data_test, labels_as_int[len(data_train) + run_config['time_series_length'] - 1:]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-09T16:04:25.605568Z",
     "start_time": "2019-06-09T16:04:25.580916Z"
    },
    "code_folding": [
     0,
     71
    ]
   },
   "outputs": [],
   "source": [
    "# triplet loss\n",
    "\"\"\"Define functions to create the triplet loss with online triplet mining.\"\"\"\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "def _pairwise_distances(embeddings, squared=False):\n",
    "    \"\"\"Compute the 2D matrix of distances between all the embeddings.\n",
    "\n",
    "    Args:\n",
    "        embeddings: tensor of shape (batch_size, embed_dim)\n",
    "        squared: Boolean. If true, output is the pairwise squared euclidean distance matrix.\n",
    "                 If false, output is the pairwise euclidean distance matrix.\n",
    "\n",
    "    Returns:\n",
    "        pairwise_distances: tensor of shape (batch_size, batch_size)\n",
    "    \"\"\"\n",
    "    # Get the dot product between all embeddings\n",
    "    # shape (batch_size, batch_size)\n",
    "    dot_product = tf.matmul(embeddings, tf.transpose(embeddings))\n",
    "\n",
    "    # Get squared L2 norm for each embedding. We can just take the diagonal of `dot_product`.\n",
    "    # This also provides more numerical stability (the diagonal of the result will be exactly 0).\n",
    "    # shape (batch_size,)\n",
    "    square_norm = tf.diag_part(dot_product)\n",
    "\n",
    "    # Compute the pairwise distance matrix as we have:\n",
    "    # ||a - b||^2 = ||a||^2  - 2 <a, b> + ||b||^2\n",
    "    # shape (batch_size, batch_size)\n",
    "    distances = tf.expand_dims(square_norm, 1) - 2.0 * dot_product + tf.expand_dims(square_norm, 0)\n",
    "\n",
    "    # Because of computation errors, some distances might be negative so we put everything >= 0.0\n",
    "    distances = tf.maximum(distances, 0.0)\n",
    "\n",
    "    if not squared:\n",
    "        # Because the gradient of sqrt is infinite when distances == 0.0 (ex: on the diagonal)\n",
    "        # we need to add a small epsilon where distances == 0.0\n",
    "        mask = tf.cast(tf.equal(distances, 0.0), tf.float32)\n",
    "        distances = distances + mask * 1e-16\n",
    "\n",
    "        distances = tf.sqrt(distances)\n",
    "\n",
    "        # Correct the epsilon added: set the distances on the mask to be exactly 0.0\n",
    "        distances = distances * (1.0 - mask)\n",
    "\n",
    "    return distances\n",
    "\n",
    "\n",
    "def _get_anchor_positive_triplet_mask(labels):\n",
    "    \"\"\"Return a 2D mask where mask[a, p] is True iff a and p are distinct and have same label.\n",
    "\n",
    "    Args:\n",
    "        labels: tf.int32 `Tensor` with shape [batch_size]\n",
    "\n",
    "    Returns:\n",
    "        mask: tf.bool `Tensor` with shape [batch_size, batch_size]\n",
    "    \"\"\"\n",
    "    # Check that i and j are distinct\n",
    "    indices_equal = tf.cast(tf.eye(tf.shape(labels)[0]), tf.bool)\n",
    "    indices_not_equal = tf.logical_not(indices_equal)\n",
    "\n",
    "    # Check if labels[i] == labels[j]\n",
    "    # Uses broadcasting where the 1st argument has shape (1, batch_size) and the 2nd (batch_size, 1)\n",
    "    labels_equal = tf.equal(tf.expand_dims(labels, 0), tf.expand_dims(labels, 1))\n",
    "\n",
    "    # Combine the two masks\n",
    "    mask = tf.logical_and(indices_not_equal, labels_equal)\n",
    "\n",
    "    return mask\n",
    "\n",
    "\n",
    "def _get_anchor_negative_triplet_mask(labels):\n",
    "    \"\"\"Return a 2D mask where mask[a, n] is True iff a and n have distinct labels.\n",
    "\n",
    "    Args:\n",
    "        labels: tf.int32 `Tensor` with shape [batch_size]\n",
    "\n",
    "    Returns:\n",
    "        mask: tf.bool `Tensor` with shape [batch_size, batch_size]\n",
    "    \"\"\"\n",
    "    # Check if labels[i] != labels[k]\n",
    "    # Uses broadcasting where the 1st argument has shape (1, batch_size) and the 2nd (batch_size, 1)\n",
    "    labels_equal = tf.equal(tf.expand_dims(labels, 0), tf.expand_dims(labels, 1))\n",
    "\n",
    "    mask = tf.logical_not(labels_equal)\n",
    "\n",
    "    return mask\n",
    "\n",
    "\n",
    "def _get_triplet_mask(labels):\n",
    "    \"\"\"Return a 3D mask where mask[a, p, n] is True iff the triplet (a, p, n) is valid.\n",
    "\n",
    "    A triplet (i, j, k) is valid if:\n",
    "        - i, j, k are distinct\n",
    "        - labels[i] == labels[j] and labels[i] != labels[k]\n",
    "\n",
    "    Args:\n",
    "        labels: tf.int32 `Tensor` with shape [batch_size]\n",
    "    \"\"\"\n",
    "    # Check that i, j and k are distinct\n",
    "    indices_equal = tf.cast(tf.eye(tf.shape(labels)[0]), tf.bool)\n",
    "    indices_not_equal = tf.logical_not(indices_equal)\n",
    "    i_not_equal_j = tf.expand_dims(indices_not_equal, 2)\n",
    "    i_not_equal_k = tf.expand_dims(indices_not_equal, 1)\n",
    "    j_not_equal_k = tf.expand_dims(indices_not_equal, 0)\n",
    "\n",
    "    distinct_indices = tf.logical_and(tf.logical_and(i_not_equal_j, i_not_equal_k), j_not_equal_k)\n",
    "\n",
    "\n",
    "    # Check if labels[i] == labels[j] and labels[i] != labels[k]\n",
    "    label_equal = tf.equal(tf.expand_dims(labels, 0), tf.expand_dims(labels, 1))\n",
    "    i_equal_j = tf.expand_dims(label_equal, 2)\n",
    "    i_equal_k = tf.expand_dims(label_equal, 1)\n",
    "\n",
    "    valid_labels = tf.logical_and(i_equal_j, tf.logical_not(i_equal_k))\n",
    "\n",
    "    # Combine the two masks\n",
    "    mask = tf.logical_and(distinct_indices, valid_labels)\n",
    "\n",
    "    return mask\n",
    "\n",
    "\n",
    "def batch_all_triplet_loss(labels, embeddings, margin, squared=False):\n",
    "    \"\"\"Build the triplet loss over a batch of embeddings.\n",
    "\n",
    "    We generate all the valid triplets and average the loss over the positive ones.\n",
    "\n",
    "    Args:\n",
    "        labels: labels of the batch, of size (batch_size,)\n",
    "        embeddings: tensor of shape (batch_size, embed_dim)\n",
    "        margin: margin for triplet loss\n",
    "        squared: Boolean. If true, output is the pairwise squared euclidean distance matrix.\n",
    "                 If false, output is the pairwise euclidean distance matrix.\n",
    "\n",
    "    Returns:\n",
    "        triplet_loss: scalar tensor containing the triplet loss\n",
    "    \"\"\"\n",
    "    # Get the pairwise distance matrix\n",
    "    pairwise_dist = _pairwise_distances(embeddings, squared=squared)\n",
    "\n",
    "    # shape (batch_size, batch_size, 1)\n",
    "    anchor_positive_dist = tf.expand_dims(pairwise_dist, 2)\n",
    "    assert anchor_positive_dist.shape[2] == 1, \"{}\".format(anchor_positive_dist.shape)\n",
    "    # shape (batch_size, 1, batch_size)\n",
    "    anchor_negative_dist = tf.expand_dims(pairwise_dist, 1)\n",
    "    assert anchor_negative_dist.shape[1] == 1, \"{}\".format(anchor_negative_dist.shape)\n",
    "\n",
    "    # Compute a 3D tensor of size (batch_size, batch_size, batch_size)\n",
    "    # triplet_loss[i, j, k] will contain the triplet loss of anchor=i, positive=j, negative=k\n",
    "    # Uses broadcasting where the 1st argument has shape (batch_size, batch_size, 1)\n",
    "    # and the 2nd (batch_size, 1, batch_size)\n",
    "    triplet_loss = anchor_positive_dist - anchor_negative_dist + margin\n",
    "\n",
    "    # Put to zero the invalid triplets\n",
    "    # (where label(a) != label(p) or label(n) == label(a) or a == p)\n",
    "    mask = _get_triplet_mask(labels)\n",
    "    mask = tf.cast(mask, tf.float32)\n",
    "    triplet_loss = tf.multiply(mask, triplet_loss)\n",
    "\n",
    "    # Remove negative losses (i.e. the easy triplets)\n",
    "    triplet_loss = tf.maximum(triplet_loss, 0.0)\n",
    "\n",
    "    # Count number of positive triplets (where triplet_loss > 0)\n",
    "    valid_triplets = tf.cast(tf.greater(triplet_loss, 1e-16), tf.float32)\n",
    "    num_positive_triplets = tf.reduce_sum(valid_triplets)\n",
    "    num_valid_triplets = tf.reduce_sum(mask)\n",
    "    fraction_positive_triplets = num_positive_triplets / (num_valid_triplets + 1e-16)\n",
    "\n",
    "    # Get final mean triplet loss over the positive valid triplets\n",
    "    triplet_loss = tf.reduce_sum(triplet_loss) / (num_positive_triplets + 1e-16)\n",
    "\n",
    "    return triplet_loss, fraction_positive_triplets\n",
    "\n",
    "\n",
    "def batch_hard_triplet_loss(labels, embeddings, margin, squared=False):\n",
    "    \"\"\"Build the triplet loss over a batch of embeddings.\n",
    "\n",
    "    For each anchor, we get the hardest positive and hardest negative to form a triplet.\n",
    "\n",
    "    Args:\n",
    "        labels: labels of the batch, of size (batch_size,)\n",
    "        embeddings: tensor of shape (batch_size, embed_dim)\n",
    "        margin: margin for triplet loss\n",
    "        squared: Boolean. If true, output is the pairwise squared euclidean distance matrix.\n",
    "                 If false, output is the pairwise euclidean distance matrix.\n",
    "\n",
    "    Returns:\n",
    "        triplet_loss: scalar tensor containing the triplet loss\n",
    "    \"\"\"\n",
    "    # Get the pairwise distance matrix\n",
    "    pairwise_dist = _pairwise_distances(embeddings, squared=squared)\n",
    "\n",
    "    # For each anchor, get the hardest positive\n",
    "    # First, we need to get a mask for every valid positive (they should have same label)\n",
    "    mask_anchor_positive = _get_anchor_positive_triplet_mask(labels)\n",
    "    mask_anchor_positive = tf.cast(mask_anchor_positive, tf.float32)\n",
    "\n",
    "    # We put to 0 any element where (a, p) is not valid (valid if a != p and label(a) == label(p))\n",
    "    anchor_positive_dist = tf.multiply(mask_anchor_positive, pairwise_dist)\n",
    "\n",
    "    # shape (batch_size, 1)\n",
    "    hardest_positive_dist = tf.reduce_max(anchor_positive_dist, axis=1, keepdims=True)\n",
    "    tf.summary.scalar(\"hardest_positive_dist\", tf.reduce_mean(hardest_positive_dist))\n",
    "\n",
    "    # For each anchor, get the hardest negative\n",
    "    # First, we need to get a mask for every valid negative (they should have different labels)\n",
    "    mask_anchor_negative = _get_anchor_negative_triplet_mask(labels)\n",
    "    mask_anchor_negative = tf.cast(mask_anchor_negative, tf.float32)\n",
    "\n",
    "    # We add the maximum value in each row to the invalid negatives (label(a) == label(n))\n",
    "    max_anchor_negative_dist = tf.reduce_max(pairwise_dist, axis=1, keepdims=True)\n",
    "    anchor_negative_dist = pairwise_dist + max_anchor_negative_dist * (1.0 - mask_anchor_negative)\n",
    "\n",
    "    # shape (batch_size,)\n",
    "    hardest_negative_dist = tf.reduce_min(anchor_negative_dist, axis=1, keepdims=True)\n",
    "    tf.summary.scalar(\"hardest_negative_dist\", tf.reduce_mean(hardest_negative_dist))\n",
    "\n",
    "    # Combine biggest d(a, p) and smallest d(a, n) into final triplet loss\n",
    "    triplet_loss = tf.maximum(hardest_positive_dist - hardest_negative_dist + margin, 0.0)\n",
    "\n",
    "    # Get final mean triplet loss\n",
    "    triplet_loss = tf.reduce_mean(triplet_loss)\n",
    "\n",
    "    return triplet_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-09T16:04:25.927929Z",
     "start_time": "2019-06-09T16:04:25.921790Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# LOSS LABEL FUNCTION\n",
    "\n",
    "def compute_loss_labels(x, labels):\n",
    "    \"\"\"About triplet loss: https://omoindrot.github.io/triplet-loss\n",
    "    \n",
    "    \"\"\"\n",
    "    #triplet_loss = tfc.losses.metric_learning.triplet_semihard_loss(labels, np.hstack((mean, var)))\n",
    "    #loss = triplet_loss.batch_all_triplet_loss(labels, np.hstack((mean, var)), 0.1)\n",
    "    loss, loss_fraction_pos = batch_all_triplet_loss(labels, x, 1., squared=True)\n",
    "    #triplet_loss = tfc.losses.metric_learning.cluster_loss(labels, np.hstack((mean, var)), 0.1)\n",
    "    \n",
    "    return loss\n",
    "\n",
    "\n",
    "def triplet_compute_gradients(model, x, y): \n",
    "    with tf.GradientTape() as tape: \n",
    "        #mean, var = model(x)\n",
    "        #encoded = tf.nn.l2_normalize(((mean, var)))\n",
    "        loss = compute_loss_labels(tf.concat(model(x), axis=1), y) \n",
    "        return tape.gradient(loss, model.trainable_variables), loss\n",
    "    \n",
    "    \n",
    "def _compute_loss_for_data_triplet_(model, data):\n",
    "    loss = tfe.metrics.Mean()\n",
    "    for x, y in data:\n",
    "        #mean, var = model(batch_x)\n",
    "        #encoded = tf.nn.l2_normalize(((mean, var)))\n",
    "        #loss_b = compute_loss_labels(mean, batch_y) \n",
    "        loss_b = compute_loss_labels(tf.concat(model(x), axis=1), y) \n",
    "        #loss_b = compute_loss_labels(model, batch_x, batch_y)\n",
    "        loss(loss_b)\n",
    "        \n",
    "    return loss.result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-09T16:04:30.191035Z",
     "start_time": "2019-06-09T16:04:30.187110Z"
    }
   },
   "outputs": [],
   "source": [
    "#grad, loss = triplet_compute_gradients(model.inference_net, train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-09T15:53:21.041333Z",
     "start_time": "2019-06-09T15:53:21.027520Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# debug\n",
    "#from functools import partial\n",
    "#\n",
    "#def wrapped_cll(x, y):\n",
    "#    return\n",
    "#\n",
    "#grad_fn = tfe.gradients_function()\n",
    "#\n",
    "#tf.expand_dims(tf.concat(_m(train_x), axis=1), axis=1).numpy().shape\n",
    "#\n",
    "#grad_y, grad_x = grad_fn(train_y, tf.concat(_m(train_x), axis=1))\n",
    "#\n",
    "#grad_x\n",
    "#\n",
    "#with tf.GradientTape() as tape: \n",
    "#    tape.watch(train_x)\n",
    "#    loss = compute_loss_labels(_m, train_x, train_y) \n",
    "#    #print(model.inference_net.trainable_variables)\n",
    "#    grad = tape.gradient(loss, _m.trainable_variables)\n",
    "#\n",
    "#loss, len(grad), len(_m.trainable_variables), grad[:3]\n",
    "#\n",
    "#with tf.GradientTape() as tape: \n",
    "#    loss = compute_loss(model, train_x) \n",
    "#    grad = tape.gradient(loss, model.trainable_variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-09T15:53:21.053925Z",
     "start_time": "2019-06-09T15:53:21.043929Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def _progress_str_triplet_(epoch, _cur_train_reports, _cur_test_reports, time=None, stopped=False):\n",
    "    progress_str = f\"Epoch: {epoch:0>4}, train/test loss: {_cur_train_reports[-1]:0.3f}\\t {_cur_test_reports[-1]:0.3f}\"\n",
    "    if time:\n",
    "        progress_str += f\" took {time:0.3f} sec\"\n",
    "        \n",
    "    if stopped:\n",
    "        progress_str = \"Stopped training during \" + progress_str\n",
    "        \n",
    "    return progress_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-09T16:26:16.433695Z",
     "start_time": "2019-06-09T16:26:04.733019Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# This is the run cell. Designed to be able to train the model.inference_net for an arbitrary amount of epochs.\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "print(f\"will train for ever...\")\n",
    "triplet_epoch = len(triplet_train_losses)\n",
    "\n",
    "# todo wrap below code using this\n",
    "#@tf.function\n",
    "#def train(model, dataset, optimizer):\n",
    "#    for x, y in dataset:\n",
    "#        with tf.GradientTape() as tape:\n",
    "#            prediction = model(x)\n",
    "#            loss = loss_fn(prediction, y)\n",
    "#        gradients = tape.gradient(loss, model.trainable_variables)\n",
    "#        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "#loss_fn = partial(batch_hard_triplet_loss, margin=1.)\n",
    "#grad_fn = tfe.gradients_function(partial(compute_loss_labels, model=model.inference_net))\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    # pesky tensorflow again\n",
    "    warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "    while True:\n",
    "        try:\n",
    "            start_time = time.time()\n",
    "            for train_x, train_y in triplet_train_dataset:\n",
    "                gradients, loss = triplet_compute_gradients(model.inference_net, train_x, train_y)\n",
    "                #loss = compute_loss_labels(model.inference_net, train_x, train_y) \n",
    "                #grad_y, gradients = grad_fn(train_y, train_x)\n",
    "                apply_gradients(triplet_optimizer, gradients, model.inference_net.trainable_variables)\n",
    "            end_time = time.time()\n",
    "\n",
    "            triplet_cur_train_reports += [_compute_loss_for_data_triplet_(model.inference_net, triplet_train_dataset)]\n",
    "            triplet_cur_test_reports += [_compute_loss_for_data_triplet_(model.inference_net, triplet_test_dataset)]\n",
    "            \n",
    "            _triplet_recorded_scalars_ =  ['triplet_loss']\n",
    "            tf_helpers.tf_write_scalars(train_summary_writer, zip(_triplet_recorded_scalars_, [triplet_cur_train_reports[-1]]), step=triplet_epoch)\n",
    "            tf_helpers.tf_write_scalars(test_summary_writer,  zip(_triplet_recorded_scalars_, [triplet_cur_test_reports[-1]]),  step=triplet_epoch)\n",
    "\n",
    "            with train_summary_writer.as_default(), tfc.summary.always_record_summaries():\n",
    "                for g, var_name in zip(gradients, [tf_helpers.tf_clean_variable_name(v.name) for v in model.inference_net.trainable_variables]):\n",
    "                    tfc.summary.histogram(f'gradient_{var_name}', g, step=triplet_epoch)\n",
    "\n",
    "            if triplet_epoch % 10 == 0:\n",
    "                print(_progress_str_triplet_(triplet_epoch, triplet_cur_train_reports, triplet_cur_test_reports, time=end_time - start_time))\n",
    "                tfc.summary.flush()\n",
    "            else:\n",
    "                # simple \"loading bar\"\n",
    "                print('=' * (triplet_epoch % 10) + '.' * (10 - (triplet_epoch % 10)), end='\\r')\n",
    "                \n",
    "            #if triplet_epoch > 10 and triplet_cur_test_reports[-1][0] < triplet_cur_test_reports[cur_min_val_idx][0]:\n",
    "            #    cur_min_val_idx = triplet_epoch\n",
    "            #    model.inference_net.save_weights(_model_checkpoints_path_)\n",
    "                \n",
    "            triplet_epoch += 1\n",
    "\n",
    "            #if np.argmin(np.array(triplet_cur_test_reports)[:, 1]) < (len(triplet_cur_test_reports) - 10):\n",
    "            #    # if there was no improvement in the last 10 epochs, stop it\n",
    "            #    print('early stopping')\n",
    "            #    break\n",
    "        except KeyboardInterrupt:\n",
    "            tfc.summary.flush()\n",
    "            print(_progress_str_triplet_(triplet_epoch, triplet_cur_train_reports, triplet_cur_test_reports, stopped=True))\n",
    "            break\n",
    "        \n",
    "        \n",
    "tfc.summary.flush()\n",
    "triplet_train_losses = np.array(triplet_cur_train_reports)\n",
    "triplet_test_losses =  np.array(triplet_cur_test_reports)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-21T16:37:31.785780Z",
     "start_time": "2019-05-21T16:37:31.782033Z"
    }
   },
   "source": [
    "# evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-09T16:26:22.024039Z",
     "start_time": "2019-06-09T16:26:22.018115Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def _reverse_to_original_shape_(X):\n",
    "    rescaled = scaler.inverse_transform(X)\n",
    "    if run_config['data_type'] == _DATA_TYPE_2D_POS_:\n",
    "        return rescaled.reshape(-1, 15, 2)\n",
    "    else:\n",
    "        return rescaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-09T16:26:23.582085Z",
     "start_time": "2019-06-09T16:26:22.370834Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# data pipeline for evaluation\n",
    "\n",
    "exp_desc = config.exp_desc(run_config, {**model._config_(), 'epochs': len(train_losses)})\n",
    "exp_desc_short = config.exp_desc(run_config, {**model._config_(), 'epochs': len(train_losses)}, short=True)\n",
    "input_data_raw = np.vstack((data_train, data_test))\n",
    "\n",
    "if run_config['use_time_series']:\n",
    "    back_to_single_time = np.s_[:, -1, :]\n",
    "else:\n",
    "    back_to_single_time = np.s_[:]\n",
    "    \n",
    "\n",
    "input_data = _reverse_to_original_shape_(input_data_raw[back_to_single_time])\n",
    "reconstructed_data = _reverse_to_original_shape_(model(input_data_raw, apply_sigmoid=False).numpy()[back_to_single_time])\n",
    "    \n",
    "_min_nb_batches_for_sample_length_ = int(np.ceil(len(input_data_raw) / run_config['batch_size']))\n",
    "generated_data = _reverse_to_original_shape_(np.vstack([model.sample().numpy() for _ in range(_min_nb_batches_for_sample_length_)])[back_to_single_time])[:len(reconstructed_data)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-09T16:26:23.841981Z",
     "start_time": "2019-06-09T16:26:23.583550Z"
    }
   },
   "outputs": [],
   "source": [
    "# losses\n",
    "for a, n in zip(range(train_reports.shape[1]), _recorded_scalars_):\n",
    "    plt.subplot(train_reports.shape[1] + 1, 1, a + 1)\n",
    "    plt.plot(train_reports[:, a], label=f\"train_{n}\")\n",
    "    plt.plot(test_reports[:, a], label=f\"test_{n}\")\n",
    "    plt.title(n)\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-09T16:26:30.063120Z",
     "start_time": "2019-06-09T16:26:29.955368Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(triplet_train_losses)\n",
    "plt.plot(triplet_test_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-09T15:53:21.289700Z",
     "start_time": "2019-06-09T15:38:42.787Z"
    }
   },
   "outputs": [],
   "source": [
    "#plots.plot_losses(train_losses, test_losses, exp_desc=exp_desc);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-09T16:26:37.834745Z",
     "start_time": "2019-06-09T16:26:35.796293Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if run_config['data_type'] == _DATA_TYPE_2D_POS_:\n",
    "    fig = plots.plot_comparing_joint_position_with_reconstructed(input_data,\n",
    "                                                                 reconstructed_data,\n",
    "                                                                 generated_data,\n",
    "                                                                 validation_cut_off=len(data_train),\n",
    "                                                                 exp_desc=exp_desc_short);\n",
    "else:\n",
    "    # ncols is an ugly hack... it works on the basis that we have three working angles for each leg\n",
    "    if run_config['use_all_experiments']:\n",
    "        start = 100\n",
    "        end = 1000\n",
    "    else:\n",
    "        start = 0\n",
    "        end = len(input_data)\n",
    "    xticks = np.arange(start, end)\n",
    "    if run_config['debug']:\n",
    "        _input_data = input_data_raw[:, :, 0]\n",
    "        _recon = model(input_data_raw, apply_sigmoid=False).numpy()[:, :, 0]\n",
    "        fig, axs = plt.subplots(nrows=_input_data.shape[-1], ncols=1, figsize=(20, 30), sharex=True, sharey=True)\n",
    "        for i in range(_input_data.shape[-1]):\n",
    "            _idx_ = np.s_[start:end, i]\n",
    "            axs[i].plot(xticks, _input_data[_idx_], label='input')\n",
    "            axs[i].plot(xticks, _recon[_idx_], label='reconstructed')\n",
    "            #axs[i].plot(xticks, generated_data[_idx_], label='generated')\n",
    "\n",
    "            #axs[i].set_title(cn)\n",
    "\n",
    "            #for a in axs[i]:\n",
    "            #    a.axvline(len(data_train), label='validation cut off', linestyle='--')\n",
    "    else:\n",
    "        fig, axs = plt.subplots(nrows=input_data.shape[1], ncols=1, figsize=(20, 30), sharex=True, sharey=True)\n",
    "        for i, cn in enumerate(SD.get_3d_columns_names(selected_cols)):\n",
    "            _idx_ = np.s_[start:end, i]\n",
    "            axs[i].plot(xticks, input_data[_idx_], label='input')\n",
    "            axs[i].plot(xticks, reconstructed_data[_idx_], label='reconstructed')\n",
    "            #axs[i].plot(xticks, generated_data[_idx_], label='generated')\n",
    "\n",
    "            axs[i].set_title(cn)\n",
    "\n",
    "            #for a in axs[i]:\n",
    "            #    a.axvline(len(data_train), label='validation cut off', linestyle='--')\n",
    "\n",
    "    axs[-1].set_xlabel('time step')\n",
    "    axs[0].legend(loc='upper left')\n",
    "    \n",
    "    #plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "    plt.suptitle(f\"Comparision of selection of data\\n({exp_desc})\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(top=0.94)\n",
    "    plt.savefig(f\"./figures/{exp_desc_short}_input_gen_recon_comparision.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Latent space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-09T16:01:48.318870Z",
     "start_time": "2019-06-09T16:01:48.314653Z"
    }
   },
   "outputs": [],
   "source": [
    "from hdbscan import HDBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-09T16:05:47.014500Z",
     "start_time": "2019-06-09T16:05:41.760880Z"
    }
   },
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "LatentSpaceEncoding = namedtuple('LatentSpaceEncoding', 'mean var')\n",
    "\n",
    "if run_config['use_all_experiments']:\n",
    "    warnings.warn('should use all data `input_data`')\n",
    "    if model._name in ['drosoph_vae_conv', 'drosoph_vae_skip_conv']:\n",
    "        X_latent = LatentSpaceEncoding(*map(lambda x: x.numpy(), model.encode(input_data_raw[np.random.choice(len(input_data), 10000)])))\n",
    "    else:\n",
    "        X_latent = LatentSpaceEncoding(*map(lambda x: x.numpy()[back_to_single_time], model.encode(input_data_raw[np.random.choice(len(input_data), 10000)])))\n",
    "else:\n",
    "    if model._name in ['drosoph_vae_conv', 'drosoph_vae_skip_conv']:\n",
    "        X_latent = LatentSpaceEncoding(*map(lambda x: x.numpy(), model.encode(input_data_raw)))\n",
    "    else:\n",
    "        X_latent = LatentSpaceEncoding(*map(lambda x: x.numpy()[back_to_single_time], model.encode(input_data_raw)))\n",
    "    \n",
    "X_latent_mean_tsne_proj = TSNE(n_components=2, random_state=42).fit_transform(np.hstack((X_latent.mean, X_latent.var)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-09T16:05:47.048704Z",
     "start_time": "2019-06-09T16:05:47.015967Z"
    }
   },
   "outputs": [],
   "source": [
    "cluster_assignments = HDBSCAN(min_cluster_size=8).fit_predict(np.hstack((X_latent.mean, X_latent.var)))\n",
    "cluster_colors = sns.color_palette(n_colors=len(np.unique(cluster_assignments)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-09T16:05:50.389812Z",
     "start_time": "2019-06-09T16:05:50.377487Z"
    }
   },
   "outputs": [],
   "source": [
    "from matplotlib.collections import LineCollection\n",
    "\n",
    "def plot_debug(input_data, cluster_assignments, cluster_colors=None):\n",
    "    _clusters = np.unique(cluster_assignments)\n",
    "    _colors = sns.color_palette(n_colors=len(_clusters))\n",
    "    if cluster_colors is None:\n",
    "        cluster_colors = dict(zip(_clusters, _colors))\n",
    "        \n",
    "    lines, colors = zip(*[([(x, input_data[x, 0]) for x in segment], cluster_colors[cluster_id])\n",
    "                           for cluster_id, segments in video.group_by_cluster(cluster_assignments).items() \n",
    "                           for segment in segments])\n",
    "\n",
    "\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    coll = LineCollection(lines, colors=colors)\n",
    "    #coll.set_array(np.random.random(xy.shape[0]))\n",
    "\n",
    "    ax.add_collection(coll)\n",
    "    ax.autoscale_view()\n",
    "\n",
    "    plt.title('Input data and cluster assigment using debug data');\n",
    "    \n",
    "if run_config['debug']:\n",
    "    plot_debug(input_data, cluster_assignments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-09T16:05:52.007445Z",
     "start_time": "2019-06-09T16:05:52.002193Z"
    }
   },
   "outputs": [],
   "source": [
    "exp_desc_short"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-09T16:01:54.648652Z",
     "start_time": "2019-06-09T16:01:54.635892Z"
    }
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "# use this to add a different shape to the scatter plot\n",
    "# frames_idx_with_labels[:len(frames_of_interest)][frames_of_interest][run_config['time_series_length'] - 1:]['label'].apply(lambda x: x.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-09T16:05:58.933015Z",
     "start_time": "2019-06-09T16:05:57.499114Z"
    }
   },
   "outputs": [],
   "source": [
    "from matplotlib import gridspec\n",
    "\n",
    "labels = frames_idx_with_labels['label'].apply(lambda x: x.name)\n",
    "\n",
    "fig = plt.figure(figsize=(20, 18))\n",
    "gs = gridspec.GridSpec(3, 2, figure=fig)\n",
    "ax1 = plt.subplot(gs[:2, :])\n",
    "ax2 = plt.subplot(gs[-1:, :1])\n",
    "ax3 = plt.subplot(gs[-1:, 1:])\n",
    "\n",
    "#plt.figure(figsize=(20, 12))\n",
    "#fig, axs = plt.subplots(nrows=3, ncols=1, figsize=(20, 30))\n",
    "for cluster in np.unique(cluster_assignments):\n",
    "    c_idx = cluster_assignments == cluster\n",
    "    if run_config['use_all_experiments']:\n",
    "        c_idx = c_idx & (np.random.random(len(c_idx)) > 0.7) # don't show all of them, takes for ever otherwise\n",
    "    sns.scatterplot(X_latent_mean_tsne_proj[c_idx, 0], \n",
    "                    X_latent_mean_tsne_proj[c_idx, 1], \n",
    "                    label=cluster, \n",
    "                    ax=ax1,\n",
    "                    color=cluster_colors[cluster], \n",
    "                    style=labels[run_config['time_series_length'] - 1:][c_idx],\n",
    "                    legend='brief')\n",
    "    sns.scatterplot(X_latent.mean[c_idx, 0], X_latent.mean[c_idx, 1], label=cluster, ax=ax2)\n",
    "    sns.scatterplot(X_latent.var[c_idx, 0], X_latent.var[c_idx, 1], label=cluster, ax=ax3)\n",
    "    \n",
    "ax1.set_title('T-SNE proejection of latent space (mean & var stacked)')\n",
    "ax2.set_title('mean')\n",
    "ax3.set_title('var');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-09T16:02:02.874723Z",
     "start_time": "2019-06-09T16:02:02.864815Z"
    }
   },
   "outputs": [],
   "source": [
    "def reverse_pos_pipeline(x, normalisation_factors):\n",
    "    \"\"\"TODO This is again pretty shitty... ultra hidden global variable\"\"\"\n",
    "    return x + normalisation_factors[:x.shape[-1]]\n",
    "\n",
    "def video_prep_raw_data(data):\n",
    "    if run_config['use_time_series']:\n",
    "        return reverse_pos_pipeline(scaler.inverse_transform(data[:, -1, :]).reshape(-1, 15, 2))\n",
    "    else:\n",
    "        return reverse_pos_pipeline(scaler.inverse_transform(data.reshape(-1, 30)).reshape(-1, 15, 2))\n",
    "    \n",
    "def video_prep_recon_data(input_data):\n",
    "    return reverse_pos_pipeline(scaler.inverse_transform(model(input_data).numpy()).reshape(-1, 15, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-09T16:06:49.819913Z",
     "start_time": "2019-06-09T16:06:33.218070Z"
    }
   },
   "outputs": [],
   "source": [
    "if run_config['data_type'] == _DATA_TYPE_2D_POS_:\n",
    "    _positional_data_ = [reverse_pos_pipeline(input_data, normalisation_factors=normalisation_factors), \n",
    "                         reverse_pos_pipeline(reconstructed_data, normalisation_factors=normalisation_factors)]\n",
    "else:\n",
    "    raise NotImplementedError('give me a break')\n",
    "    \n",
    "p = video.comparision_video_of_reconstruction(_positional_data_,\n",
    "                                              images_paths_for_experiments=images_paths_for_experiments, \n",
    "                                              n_train=len(data_train),\n",
    "                                              cluster_assignments=cluster_assignments,\n",
    "                                              as_frames=False,\n",
    "                                              exp_desc=exp_desc_short)\n",
    "\n",
    "display_video(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-09T16:02:23.306505Z",
     "start_time": "2019-06-09T16:02:03.163035Z"
    }
   },
   "outputs": [],
   "source": [
    "if run_config['data_type'] == _DATA_TYPE_2D_POS_:\n",
    "    _positional_data_ = [reverse_pos_pipeline(input_data, normalisation_factors=normalisation_factors), \n",
    "                         reverse_pos_pipeline(reconstructed_data, normalisation_factors=normalisation_factors)]\n",
    "else:\n",
    "    raise NotImplementedError('give me a break')\n",
    "    \n",
    "p = video.comparision_video_of_reconstruction(_positional_data_,\n",
    "                                              images_paths_for_experiments=images_paths_for_experiments, \n",
    "                                              n_train=len(data_train),\n",
    "                                              cluster_assignments=cluster_assignments,\n",
    "                                              as_frames=False,\n",
    "                                              exp_desc=exp_desc_short)\n",
    "\n",
    "display_video(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-09T16:02:23.341267Z",
     "start_time": "2019-06-09T16:02:23.309098Z"
    }
   },
   "outputs": [],
   "source": [
    "# Super ugly... but necessary...\n",
    "# first there is the time offset due to the slicing\n",
    "# then there is the concatenation of the data...\n",
    "\n",
    "angle_data_pos_to_frame = []\n",
    "\n",
    "for exp_key, data in angle_data_raw: \n",
    "    _exp = SD._experiment_from_key_(exp_key)\n",
    "    \n",
    "    if len(angle_data_pos_to_frame) == 0:\n",
    "        _idx = np.arange(data.shape[0])[run_config['time_series_length'] - 1:]\n",
    "    else:\n",
    "        _idx = np.arange(data.shape[0])# + len(angle_data_pos_to_frame)\n",
    "        \n",
    "    angle_data_pos_to_frame += [(_exp, d) for d in _idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-09T15:53:21.295773Z",
     "start_time": "2019-06-09T15:38:43.008Z"
    }
   },
   "outputs": [],
   "source": [
    "        images_paths_for_experiments = settings.data.EXPERIMENTS.map(lambda x: (x, config.positional_data(x)))\\\n",
    "                                               .flat_map(lambda x: [(x[0], config.get_path_for_image(x[0], i)) for i in range(x[1].shape[1])])\\\n",
    "                                               .to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-09T15:53:21.296304Z",
     "start_time": "2019-06-09T15:38:43.013Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "    frames_idx_with_labels = preprocessing.get_frames_with_idx_and_labels(settings.data.LABELLED_DATA)\n",
    "    frames_of_interest = ~frames_idx_with_labels['label'].isin([settings.data._BehaviorLabel_.REST])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-09T15:53:21.296834Z",
     "start_time": "2019-06-09T15:38:43.017Z"
    }
   },
   "outputs": [],
   "source": [
    "images_paths_for_experiments = [(exp, config.get_path_for_image(exp, i)) for exp, i in angle_data_pos_to_frame]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-09T15:53:21.297336Z",
     "start_time": "2019-06-09T15:38:43.021Z"
    }
   },
   "outputs": [],
   "source": [
    "images_paths_for_experiments[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-09T16:03:12.250644Z",
     "start_time": "2019-06-09T16:03:12.211648Z"
    }
   },
   "outputs": [],
   "source": [
    "reload(video)\n",
    "from collections import OrderedDict\n",
    "_N_CLUSTER_TO_VIZ_ = 10\n",
    "_t = [(misc.flatten(sequences), cluster_id) for cluster_id, sequences in video.group_by_cluster(cluster_assignments).items()]\n",
    "_t = sorted(_t, key=lambda x: len(x[0]), reverse=True)\n",
    "\n",
    "cluster_colors = sns.color_palette(n_colors=len(np.unique(cluster_assignments)))\n",
    "\n",
    "cluster_vids = OrderedDict((p[1], video.comparision_video_of_reconstruction(input_data,\n",
    "                                                                            cluster_assignments=cluster_assignments,\n",
    "                                                                            images_paths_for_experiments=images_paths_for_experiments,\n",
    "                                                                            n_train=data_train.shape[0],\n",
    "                                                                            cluster_colors=cluster_colors,\n",
    "                                                                            cluster_id_to_visualize=p[1], \n",
    "                                                                            exp_desc=exp_desc_short,\n",
    "                                                                            is_2d=False))\n",
    "                    for p in _t[:_N_CLUSTER_TO_VIZ_])\n",
    "\n",
    "print('cluster_vids: ', cluster_vids.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-09T15:53:21.298372Z",
     "start_time": "2019-06-09T15:38:43.032Z"
    }
   },
   "outputs": [],
   "source": [
    "! cat ./som_vae/helpers/video.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-09T16:08:36.099471Z",
     "start_time": "2019-06-09T16:08:17.290929Z"
    }
   },
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "_N_CLUSTER_TO_VIZ_ = 10\n",
    "_t = [(misc.flatten(sequences), cluster_id) for cluster_id, sequences in video.group_by_cluster(cluster_assignments).items()]\n",
    "_t = sorted(_t, key=lambda x: len(x[0]), reverse=True)\n",
    "\n",
    "cluster_colors = sns.color_palette(n_colors=len(np.unique(cluster_assignments)))\n",
    "\n",
    "cluster_vids = OrderedDict((p[1], video.comparision_video_of_reconstruction(_positional_data_,\n",
    "                                                                      cluster_assignments=cluster_assignments,\n",
    "                                                                      images_paths_for_experiments=images_paths_for_experiments,\n",
    "                                                                      n_train=data_train.shape[0],\n",
    "                                                                      cluster_colors=cluster_colors,\n",
    "                                                                      cluster_id_to_visualize=p[1], exp_desc=exp_desc_short))\n",
    "                    for p in _t[:_N_CLUSTER_TO_VIZ_])\n",
    "\n",
    "print('cluster_vids: ', cluster_vids.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-09T16:09:48.011886Z",
     "start_time": "2019-06-09T16:09:47.997966Z"
    }
   },
   "outputs": [],
   "source": [
    "#c_idx = 0\n",
    "c_idx += 1\n",
    "display_video(list(cluster_vids.values())[c_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-09T16:03:34.942637Z",
     "start_time": "2019-06-09T16:03:34.871223Z"
    }
   },
   "outputs": [],
   "source": [
    "c_idx = 0\n",
    "#c_idx += 1\n",
    "display_video(list(cluster_vids.values())[c_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-09T15:53:21.299858Z",
     "start_time": "2019-06-09T15:38:43.052Z"
    }
   },
   "outputs": [],
   "source": [
    "images_paths_for_experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-09T15:53:21.300348Z",
     "start_time": "2019-06-09T15:38:43.058Z"
    }
   },
   "outputs": [],
   "source": [
    "len(np.where(cluster_assignments == 11)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-09T15:53:21.300840Z",
     "start_time": "2019-06-09T15:38:43.070Z"
    }
   },
   "outputs": [],
   "source": [
    "np.array(images_paths_for_experiments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-09T15:53:21.301373Z",
     "start_time": "2019-06-09T15:38:43.075Z"
    }
   },
   "outputs": [],
   "source": [
    "for fs, c in _t:\n",
    "    print(f\"cluster {c} has {len(fs)} elements\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-09T15:53:21.301883Z",
     "start_time": "2019-06-09T15:38:43.080Z"
    }
   },
   "outputs": [],
   "source": [
    "reload(video)\n",
    "\n",
    "_t = [(misc.flatten(sequences), cluster_id) for cluster_id, sequences in video.group_by_cluster(cluster_assignments).items()]\n",
    "_t = sorted(_t, key=lambda x: len(x[0]), reverse=True)\n",
    "p = video.video_angle(cluster_assignments, images_paths_for_experiments, cluster_id_to_visualize=_t[3][1], exp_desc=exp_desc_short)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-09T15:53:21.302344Z",
     "start_time": "2019-06-09T15:38:43.085Z"
    }
   },
   "outputs": [],
   "source": [
    "display_video(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-09T15:53:21.302799Z",
     "start_time": "2019-06-09T15:38:43.096Z"
    }
   },
   "outputs": [],
   "source": [
    "stop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-29T08:18:01.614372Z",
     "start_time": "2019-05-29T08:18:01.610583Z"
    }
   },
   "source": [
    "# Convolution Clarification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are the results shown for a Conv1d for all padding options:\n",
    "\n",
    "- valid: only convolutions where the kernel fits inside the input are comptued\n",
    "- causal: input is shifted such that the kernel can only see itself and backwards in time\n",
    "- same: input is padded such that the convolution can also be applied to the border cases\n",
    "\n",
    "kernel sizes of 2 & 3, and dilation rates for 1 to 3.\n",
    "\n",
    "The result is that a valid convolution of kernel size 2 with a dilation factor of 1 compresses the input in a for us good way.\n",
    "The data goes from `[batch_size, n_time_steps, n_channels]` to `[batch_size, n_time_steps - 1, n_filters]` \n",
    "and crops the first time step only. Thus building features by only looking backwards in time,\n",
    "dropping the first-time step. Thus features are build over time and space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-09T15:53:21.303256Z",
     "start_time": "2019-06-09T15:38:43.103Z"
    }
   },
   "outputs": [],
   "source": [
    "example_data = np.zeros((1, 10, 5), dtype=np.float32)\n",
    "\n",
    "for row in range(example_data.shape[1]):\n",
    "    example_data[:, row, :] = row\n",
    "    \n",
    "example_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-09T15:53:21.303721Z",
     "start_time": "2019-06-09T15:38:43.108Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def conv_clarification_kernel(kernel_size):\n",
    "    conv1d_kernel_no_time = np.zeros((kernel_size, example_data.shape[-1], 1), dtype=np.float32)\n",
    "    conv1d_kernel_no_time[0, :, :] = .5\n",
    "    conv1d_kernel_no_time[1, :, :] = 1.\n",
    "    \n",
    "    if kernel_size == 3:\n",
    "        conv1d_kernel_no_time[2, :, :] = 0.1\n",
    "    \n",
    "    return conv1d_kernel_no_time\n",
    "\n",
    "\n",
    "for kernel_size in range(2, 4):\n",
    "    print(f\"data\\n{example_data}\")\n",
    "    print(f\"kernel\\n{conv_clarification_kernel(kernel_size)}\")\n",
    "    for padding in ['valid', 'causal', 'same']:\n",
    "        for dilation in range(1, 4):\n",
    "            example_conv1d = tfkl.Conv1D(filters=1, \n",
    "                                         kernel_size=kernel_size,\n",
    "                                         use_bias=False, \n",
    "                                         padding=padding,\n",
    "                                         dilation_rate=dilation,\n",
    "                                         kernel_initializer=tf.constant_initializer(conv_clarification_kernel(kernel_size)))\n",
    "\n",
    "            conv_res = example_conv1d(example_data).numpy()\n",
    "            print(f\"padding: {padding}, dilation_rate: {dilation}, kernel_size: {kernel_size}, output shape: {conv_res.shape}\\n{conv_res}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-09T15:53:21.304242Z",
     "start_time": "2019-06-09T15:38:43.114Z"
    }
   },
   "outputs": [],
   "source": [
    "def conv_clarification_kernel(kernel_size):\n",
    "    conv1d_kernel_no_time = np.zeros((kernel_size, example_data.shape[-1], example_conv1d_n_filters), dtype=np.float32)\n",
    "    conv1d_kernel_no_time[0, :, :] = .5\n",
    "    conv1d_kernel_no_time[1, :, :] = 1.\n",
    "    \n",
    "    if kernel_size == 3:\n",
    "        conv1d_kernel_no_time[2, :, :] = 0.1\n",
    "    \n",
    "    return conv1d_kernel_no_time\n",
    "\n",
    "kernel_size = 2\n",
    "padding = 'valid'\n",
    "dilation_rate = 1\n",
    "example_conv1d_n_filters = 2\n",
    "\n",
    "print(f\"data\\n{example_data}\")\n",
    "print(f\"kernel\\n{conv_clarification_kernel(kernel_size)}\")\n",
    "example_conv1d = tfkl.Conv1D(filters=example_conv1d_n_filters, \n",
    "                             kernel_size=kernel_size,\n",
    "                             use_bias=False, \n",
    "                             padding=padding,\n",
    "                             dilation_rate=dilation_rate,\n",
    "                             kernel_initializer=tf.constant_initializer(conv_clarification_kernel(kernel_size)))\n",
    "\n",
    "example_max_pooling_layer = tfkl.MaxPool1D()\n",
    "example_dense = tfkl.Dense(2, use_bias=False, kernel_initializer='ones')\n",
    "\n",
    "conv_res = example_conv1d(example_data[:,:2,:]).numpy()\n",
    "#max_pool_res = example_max_pooling_layer(conv_res)\n",
    "#dense_res = example_dense(max_pool_res)\n",
    "print(f\"padding: {padding}, dilation_rate: {dilation_rate}, kernel_size: {kernel_size}, output shape: {conv_res.shape}\")\n",
    "print('conv\\n', conv_res)\n",
    "#print('max pool\\n', max_pool_res.numpy())\n",
    "#print('dense\\n', dense_res.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-09T15:53:21.304769Z",
     "start_time": "2019-06-09T15:38:43.118Z"
    }
   },
   "outputs": [],
   "source": [
    "conv_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-09T15:53:21.305272Z",
     "start_time": "2019-06-09T15:38:43.123Z"
    }
   },
   "outputs": [],
   "source": [
    "class Conv1D_Transpose(tfkl.Layer):\n",
    "    def __init__(self, n_filters, kernel_size, batch_size):\n",
    "        super(Conv1D_Transpose, self).__init__()        \n",
    "        self.n_filters = n_filters\n",
    "        self.batch_size = batch_size\n",
    "        self.conv2d_transpose = tfkl.Conv2DTranspose(filters=n_filters, kernel_size=kernel_size, strides=2, padding='valid', kernel_initializer='ones')\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        x = tf.reshape(inputs, [self.batch_size, 1, *inputs.shape[1:]])\n",
    "        print(x.shape)\n",
    "        x = self.conv2d_transpose(x)\n",
    "        #x = tf.reshape(x, [self.batch_size, -1, self.n_filters])\n",
    "        \n",
    "        return x\n",
    "\n",
    "example_deconv1d = Conv1D_Transpose(n_filters=2, kernel_size=2, batch_size=1)\n",
    "example_deconv1d(conv_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-09T15:53:21.305787Z",
     "start_time": "2019-06-09T15:38:43.128Z"
    }
   },
   "outputs": [],
   "source": [
    "_ted = example_deconv1d(conv_res)\n",
    "tf.reshape(_ted, _ted.shape[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-09T15:53:21.306307Z",
     "start_time": "2019-06-09T15:38:43.132Z"
    }
   },
   "outputs": [],
   "source": [
    "UpsamplingConv(2)(conv_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-09T15:53:21.306794Z",
     "start_time": "2019-06-09T15:38:43.137Z"
    }
   },
   "outputs": [],
   "source": [
    "tfkl.UpSampling1D(3)(conv_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-09T15:53:21.307323Z",
     "start_time": "2019-06-09T15:38:43.143Z"
    }
   },
   "outputs": [],
   "source": [
    "class UpsamplingConv(tfkl.Layer):\n",
    "    def __init__(self, n_filters, kernel_size=2):\n",
    "        super(UpsamplingConv, self).__init__()\n",
    "        \n",
    "        self.n_filters = n_filters\n",
    "        self.kernel_size = kernel_size\n",
    "    \n",
    "    def call(self, x): \n",
    "        x = tfkl.UpSampling1D(3)(x) # upscale with 3 so that we can again apply `valid` padding and \"reverse\" the encoder\n",
    "        print(x.shape)\n",
    "        # TODO maybe add some fancy flipping of the input\n",
    "        x = tfkl.Conv1D(self.n_filters, self.kernel_size, padding='valid')(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-09T15:53:21.307776Z",
     "start_time": "2019-06-09T15:38:43.150Z"
    }
   },
   "outputs": [],
   "source": [
    "conv_res.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-09T15:53:21.308266Z",
     "start_time": "2019-06-09T15:38:43.154Z"
    }
   },
   "outputs": [],
   "source": [
    "conv_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-29T17:55:51.831670Z",
     "start_time": "2019-05-29T17:55:51.802162Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-09T15:53:21.308777Z",
     "start_time": "2019-06-09T15:38:43.161Z"
    }
   },
   "outputs": [],
   "source": [
    "example_deconv = tfkl.Conv2DTranspose(1, 2, kernel_initializer='ones')\n",
    "example_deconv(conv_res.reshape(-1, 1, *conv_res.shape[1:])).numpy().reshape(-1, *conv_res.shape[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-09T15:53:21.309301Z",
     "start_time": "2019-06-09T15:38:43.166Z"
    }
   },
   "outputs": [],
   "source": [
    "conv_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-09T15:53:21.309783Z",
     "start_time": "2019-06-09T15:38:43.170Z"
    }
   },
   "outputs": [],
   "source": [
    "conv_res.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-09T15:53:21.310311Z",
     "start_time": "2019-06-09T15:38:43.174Z"
    }
   },
   "outputs": [],
   "source": [
    "conv_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-09T15:53:21.310803Z",
     "start_time": "2019-06-09T15:38:43.179Z"
    }
   },
   "outputs": [],
   "source": [
    "tf.rank(conv_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-09T15:53:21.311308Z",
     "start_time": "2019-06-09T15:38:43.184Z"
    }
   },
   "outputs": [],
   "source": [
    "paddings = [[r, 0] for r in range(3)]\n",
    "paddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-09T15:53:21.311829Z",
     "start_time": "2019-06-09T15:38:43.188Z"
    }
   },
   "outputs": [],
   "source": [
    "tf.pad(conv_res, [[0, 0], [0, 1], [0, 0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-09T15:53:21.312321Z",
     "start_time": "2019-06-09T15:38:43.193Z"
    }
   },
   "outputs": [],
   "source": [
    "tfc.nn.conv1d_transpose(input=conv_res, filters=np.ones((2, 2, 2), dtype=np.float32), output_shape=[1, 2, 2], strides=1, padding='VALID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-09T15:53:21.312828Z",
     "start_time": "2019-06-09T15:38:43.197Z"
    }
   },
   "outputs": [],
   "source": [
    "_pdc1dt = PaddedConv1dTransposed(n_filters=2)\n",
    "print(conv_res.shape)\n",
    "resc1 = _pdc1dt(conv_res)\n",
    "print(resc1.shape)\n",
    "resc1.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-09T15:53:21.313357Z",
     "start_time": "2019-06-09T15:38:43.202Z"
    }
   },
   "outputs": [],
   "source": [
    "_pdc1dt(_pdc1dt(resc1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-09T15:53:21.313824Z",
     "start_time": "2019-06-09T15:38:43.208Z"
    }
   },
   "outputs": [],
   "source": [
    "#_t_layer_sizes_generative=[4,6,8,10,12,14,16,18]\n",
    "#_t_layer_sizes_generative=[1] * 6\n",
    "#_t_upsampling_size = [4] * 6 #, 2, 2]\n",
    "#_t_strides = [2] * 6\n",
    "##_t_padding = ['valid', 'valid', 'same']\n",
    "##_t_layer_sizes_generative=[4, 8, 16]\n",
    "#_latent_dim = 2\n",
    "#_t_generative_net = tf.keras.Sequential([tfkl.InputLayer(input_shape=(_latent_dim,)),\n",
    "#                                           tfkl.Lambda(lambda x: tf.reshape(x, [1000, 1, _latent_dim])),\n",
    "#                                           *[TemporalUpsamplingConv(conv_n_filters=fs, \n",
    "#                                                                    upsampling_size=us,\n",
    "#                                                                    conv_strides=s,\n",
    "#                                                                    conv_padding='valid',\n",
    "#                                                                    name=f\"gen_conv_{i}\") for i, (fs, us, s) \n",
    "#                                             in enumerate(zip(_t_layer_sizes_generative,\n",
    "#                                                              _t_upsampling_size,\n",
    "#                                                              _t_strides,\n",
    "#                                                             ))]],\n",
    "#                                          name='generative_net')\n",
    "#\n",
    "#_t_generative_net.summary()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "default_view": {},
   "name": "cvae.ipynb",
   "private_outputs": true,
   "provenance": [
    {
     "file_id": "1eb0NOTQapkYs3X0v-zL1x5_LFKgDISnp",
     "timestamp": 1527173385672
    }
   ],
   "toc_visible": true,
   "version": "0.3.2",
   "views": {}
  },
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
